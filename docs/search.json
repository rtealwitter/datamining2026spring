[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "CSCI 145: Syllabus",
    "section": "",
    "text": "Course Description: Data mining is the process of discovering patterns in large data sets using techniques from mathematics, computer science and statistics with applications ranging from biology and neuroscience to history and economics. The goal of the course is to teach students fundamental data mining techniques that are commonly used in practice. Students will learn advanced data mining techniques (including linear classifiers, clustering, dimension reduction, transductive learning and topic modeling).\nPrerequisites: Linear Algebra (MATH 60 or CSCI 48 or equivalent), Data Structures and Advanced Programming (CSCI 62 or equivalent).\nStructure: We will meet on Tuesdays and Thursdays from 4:15 to 5:30pm in Kravis 164.\nResources: Most of the material we cover comes from either Chris Musco’s phenomenal machine learning course, or Chinmay Hegde’s fantastic deep learning course. While we do not have a textbook, we do have typed notes; I highly recommend you read the notes before each class.\nElectronic Devices: Phones and computers are distracting to you and your peers. Please do not use them during class.\nCommunication: Please post all your course related questions on discord, either in the appropriate channel or as a direct message to me.\n\nGrading\nYour grade in the class will be based on the number of points \\(P\\) that you earn. You will receive an A if \\(P \\geq 93\\), an A- if \\(93 &gt; P \\geq 90\\), a B+ if \\(90 &gt; P \\geq 87\\), and so on. You may earn points through the following assignments:\n\nProblem Sets (10 Points): Learning requires practice. Your main opportunity to practice the concepts we cover in this class will be on the problem sets. Your grade will be based on turning in solutions to each problem and, so that you engage with the solutions, a self grade of your own work. Because I do not want to incentivize the use of LLMs, I will not grade your solutions for correctness; instead, your problem set grade is based on completion and the accuracy of your own self grade.\nQuizzes (20 Points): In lieu of grading for correctness on the problem sets, I will give short quizzes at the beginning of randomly selected classes. These quizzes will be based on the problem sets and will test your understanding of the concepts we cover in class. The quizzes will be short (5 minutes) and will be graded for correctness.\nExams (50 Points): The two exams will be in-person, and cover the material from the first and second halves of the course, respectively. You may bring a double-sided cheat sheet, but you will not be allowed to use any electronic devices. The exams will be graded for correctness.\nProject (20 Points): The final project will be a chance for you to apply the concepts we have covered in class to a real-world problem. You will select a topic we cover in class and implement an algorithm we discussed on a data set of your choosing. You will write a report describing your results and what you learned. You will also give a presentation showcasing your results to the class. Except in special circumstances, you will complete your project as an individual.\nExtra Credit: My typed notes are work in progress, and I would love your help improving them! If you find an issue in the notes on the day of the lecture or later, please open an issue on the repo. I will give extra credit to the first person to correct each typo (worth 1/4 point), and mistake (worth 1/2 point).\n\nLate Policy: Most assignments will have a no-questions-asked late policy of 24 hours (refer to Gradescope for details on each specific assignment). I will not accept assignments more than 24 hours late.\n\n\nHonor Code\nAcademic integrity is an important part of your learning experience. You are welcome to use online material and discuss problems with others but you must explicitly acknowledge the outside resources (website, person, or LLM) on the work you submit.\nLarge Language Models: LLMs are a powerful tool. However, while they are very good at producing human-like text, they have no inherent sense of ‘correctness’. You may use LLMs (as detailed below) but you are wholly responsible for the material you submit.\nYou may use LLMs for:\n\nImplementing short blocks of code that you can easily check.\nAnswering simple questions whose answers you can easily verify.\n\nDo not use LLMS for:\n\nImplementing extensive blocks of code or code that you don’t understand.\nAnswering complicated questions (like those on the problem sets) that you cannot easily verify.\n\nUltimately, the point of the assignments is for you to practice the concepts. If you use an LLM in lieu of practice, then you deny yourself the chance to learn.\n\n\nAcademic Accommodations\nIf you have a Letter of Accommodation, please contact me as early in the semester as possible. If you do not have a Letter of Accommodation and you believe you are eligible, please reach out to Accessibility Services at accessibilityservices@cmc.edu."
  },
  {
    "objectID": "notes/PageRank.html",
    "href": "notes/PageRank.html",
    "title": "PageRank (The Power Method)",
    "section": "",
    "text": "There is a rich mathematical foundation behind machine learning. Today, we will see how the eigenvalue decomposition of a matrix can be used to solve a real-world data science problem: finding the most important pages on the internet. This algorithm is known as PageRank, and it was the key technology that powered Google in its early days.\nThere are lots of webpages on the internet, and they are all interconnected by hyperlinks. Some pages, like wikipedia.org (website A) and mayoclinic.org (website B), are clearly important and authoritative. Other pages, like tealscats.com (website C), are less important. When we search for a topic, we’d like to return the most relevant pages, sorted by their importance.\nOf course, we could manually order the pages, but that would be tedious, and way less fun. Instead, we will use the structure of the internet to determine which pages are most important. Consider a graph representation of the internet, where each page is a node and each hyperlink is a directed edge from one node to another.\n\n\n\n\nPageRank\nWe roughly expect that important pages will have many incoming links (e.g., lots of people link to Wikipedia). Not only that, but we also expect that these links come from other important pages (e.g., even authoritative pages like Mayo Clinic link to Wikipedia).\nWith these ideas in mind, let’s define an iterative process for refining the importance of each page. At first, we will assign each page the same importance, say \\(1/n\\) where \\(n\\) is the number of pages. Let \\(\\mathbf{p}^{(0)} \\in \\mathbb{R}^n\\) be the initial importance vector, where \\(p^{(0)}_i = 1/n\\) for each page \\(i\\). We will then iteratively update the importance vector so that the new importance of each page depends on how many pages link to it, and their importance in turn. We will use the following update rule: \\[\np_i^{(t+1)} = \\sum_{j=1}^n \\mathbb{1}[j \\text{ links to } i] \\frac{p_j^{(t)}}{d_j},\n\\] where \\(d_j\\) is the out-degree of page \\(j\\), i.e., the number of pages that page \\(j\\) links to. Why do we divide by \\(d_j\\)? If we didn’t, then a page with many outgoing links would dominate the importance of the pages it links to.\nNote: We will only consider pages with outgoing links, i.e., \\(d_j &gt; 0\\) for all \\(j\\). Otherwise, the importance of pages with no outgoing links would be undefined.\nObserve that the new importance is a linear combination of the previous importances. In the language of linear algebra, we are taking the dot product between the previous importance vector \\(\\mathbf{p}^{(t)}\\) and a vector that only depends on the structure of the graph. For page \\(i\\), call this vector \\([\\mathbf{A}]_{i,} \\in \\mathbb{R}^n\\) where \\[\n[\\mathbf{A}]_{i,j} = \\begin{cases}\n\\frac{1}{d_j} & \\text{if } j \\text{ links to } i \\\\\n0 & \\text{otherwise.}\n\\end{cases}\n\\] When we consider the entire importance vector, we can write the update rule as \\[\n\\mathbf{p}^{(t+1)} = \\mathbf{A} \\mathbf{p}^{(t)},\n\\] where \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\) is the matrix whose \\(i\\)th row is \\(\\mathbf{A}_i\\). One useful observation is that each column of \\(\\mathbf{A}\\) sums to 1, i.e., \\(\\sum_{i=1}^n [\\mathbf{A}]_{i,j} = \\sum_{i=1}^n \\mathbb{1}[j \\text{ links to } i ] \\frac1{d_j} = \\frac{d_j}{d_j} = 1\\) for each \\(j\\). This means that \\(\\mathbf{p}^{(t+1)}\\) will have the same sum as \\(\\mathbf{p}^{(t)}\\) i.e., \\[\n\\sum_{i=1}^n p_i^{(t+1)} = \\sum_{i=1}^n \\left( \\sum_{j=1}^n [\\mathbf{A}]_{i,j} p_j^{(t)} \\right) = \\sum_{j=1}^n p_j^{(t)} \\sum_{i=1}^n [\\mathbf{A}]_{i,j} = \\sum_{j=1}^n p_j^{(t)}.\n\\] Since we initialize \\(\\mathbf{p}^{(0)}\\) to have sum 1, we can see that \\(\\mathbf{p}^{(t)}\\) will always have sum 1. This enables us to interpret the importance vector \\(\\mathbf{p}^{(t)}\\) as a probability distribution over the pages. In particular, \\(p_i^{(t)}\\) is the probability that a random user will land on page \\(i\\) after \\(t\\) iterations of following links, when starting from a uniform distribution over all the pages.\nBut now, we could have graphs with the following undesirable properties:\n\nReducible: Some groups of pages may form isolated clusters with no links in or out. For example, suppose pages X and Y link to each other, but no other pages link to either X or Y. Then, any probability that flows into this cluster will be trapped there, and the importance scores of all other pages will go to zero.\nPeriodic: Some pages may be part of a cycle, causing the importance scores to oscillate. For example, Page X links only to Page Y, and Page Y links only to Page X. In this case, we would bounce back and forth between these two pages, never settling on a stable importance score.\n\nTo avoid this, we will add a small amount of probability to each page at every iteration, which effectively simulates a user who jumps to a random page with some small probability. Let \\(\\alpha \\in (0,1)\\) be close to 1, and define the new importance update rule as \\[\n\\mathbf{p}^{(t+1)} = \\alpha \\mathbf{A} \\mathbf{p}^{(t)} + (1-\\alpha) \\mathbf{1} \\frac{1}{n}.\n\\] The resulting updates are:\n\nIrreducible: With probability \\((1-\\alpha)\\), the user jumps to a random page, ensuring that every page can be reached from any other page over time. This prevents isolated clusters from trapping probability.\nAperiodic: Since there’s always a chance of jumping randomly, cycles are “broken”, and the chain no longer gets stuck in oscillations.\n\n\n\nPower Method\nOur final update rule is slightly inelegant because it has two terms: one that depends on the previous importance vector and one that does not. But, we can rewrite it as a single matrix multiplication. Let \\(\\mathbf{1} \\in \\mathbb{R}^n\\) be the vector of all ones. Recall that \\(\\sum_{i=1}^n p_i^{(t)} = 1 = \\mathbf{1}^\\top \\mathbf{p}^{(t)}\\). Then, we can rewrite the update rule as \\[\n\\mathbf{p}^{(t+1)} = \\alpha \\mathbf{A} \\mathbf{p}^{(t)} + (1-\\alpha) \\frac{1}{n} \\mathbf{1} \\mathbf{1}^\\top \\mathbf{p}^{(t)} = \\left( \\alpha \\mathbf{A} + (1-\\alpha) \\frac{1}{n} \\mathbf{1} \\mathbf{1}^\\top \\right) \\mathbf{p}^{(t)}.\n\\]\nDefine the matrix \\(\\mathbf{M} = \\alpha \\mathbf{A} + (1-\\alpha) \\frac{1}{n} \\mathbf{1} \\mathbf{1}^\\top\\). The update rule can now be written as \\[\n\\mathbf{p}^{(t+1)} = \\mathbf{M} \\mathbf{p}^{(t)} = \\mathbf{M}^2 \\mathbf{p}^{(t-1)}\n= \\mathbf{M}^t \\mathbf{p}^{(0)}.\n\\]\nSuppose that \\(\\mathbf{M}\\) is diagonalizable. However, because \\(\\mathbf{M}\\) is not necessarily symmetric, the left eigenvectors may be different from the right eigenvectors. Let \\[\n\\mathbf{M} = \\sum_{i=1}^r  \\mathbf{v}_i \\lambda_i \\mathbf{w}_i^\\top\n\\] be the eigenvalue decomposition of \\(\\mathbf{M}\\), where \\(\\lambda_1 \\geq \\ldots \\geq \\lambda_r\\) are the eigenvalues, \\(\\mathbf{w}_1, \\ldots, \\mathbf{w}_r\\) are the corresponding left eigenvectors, and \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_r\\) are the corresponding right eigenvectors. The right eigenvectors satisfy \\[\n\\mathbf{M} \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i\n\\] while the left eigenvectors satisfy \\[\n\\mathbf{w}_i^\\top \\mathbf{M} = \\lambda_i \\mathbf{w}_i^\\top.\n\\] The left and right eigenvectors are orthonormal, i.e., \\(\\mathbf{w}_i^\\top \\mathbf{w}_j = 0 = \\mathbf{v}_i^\\top \\mathbf{v}_j\\) for \\(i \\neq j\\) and \\(1\\) if \\(i=j\\). In addition, they satisfy a biorthonormal property, i.e., \\(\\mathbf{w}_i^\\top \\mathbf{v}_j = 0\\) for \\(i \\neq j\\). To see this, observe that \\[\n\\mathbf{M} \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i\n\\Leftrightarrow\n\\mathbf{w}_j^\\top \\mathbf{M} \\mathbf{v}_i = \\lambda_i \\mathbf{w}_j^\\top \\mathbf{v}_i\n\\Leftrightarrow\n(\\mathbf{w}_j^\\top \\lambda_j - \\lambda_i \\mathbf{w}_j^\\top) \\mathbf{v}_i = 0\n\\Leftrightarrow\n(\\lambda_j - \\lambda_i) \\mathbf{w}_j^\\top \\mathbf{v}_i = 0.\n\\] So, if \\(\\lambda_j \\neq \\lambda_i\\), then \\(\\mathbf{w}_j^\\top \\mathbf{v}_i = 0\\).\nWhen we repeatedly multiply by \\(\\mathbf{M}\\), the biorthonormal property gives us a surprisingly simple result: \\[\n\\mathbf{M}^2 = \\sum_{i=1}^r \\mathbf{v}_i \\lambda_i \\mathbf{w}_i^\\top\n\\sum_{j=1}^r \\mathbf{v}_j \\lambda_j \\mathbf{w}_j^\\top\n= \\sum_{i=1}^r \\mathbf{v}_i \\lambda_i \\mathbf{w}_i^\\top \\mathbf{v}_i \\lambda_i \\mathbf{w}_i^\\top = \\sum_{i=1}^r \\lambda_i^2 \\mathbf{v}_i \\mathbf{w}_i^\\top.\n\\] In general, \\(\\mathbf{M}^t = \\sum_{i=1}^r \\lambda_i^t \\mathbf{v}_i \\mathbf{w}_i^\\top\\).\nReturning to the probability vector, we can write \\[\n\\mathbf{p}^{(t)}\n= \\mathbf{M}^t \\mathbf{p}^{(0)} = \\sum_{i=1}^r \\lambda_i^t \\mathbf{v}_i \\mathbf{w}_i^\\top \\mathbf{p}^{(0)}\n= \\lambda_1^t \\sum_{i=1}^r \\left( \\frac{\\lambda_i}{\\lambda_1} \\right)^t \\mathbf{v}_i \\mathbf{w}_i^\\top \\mathbf{p}^{(0)}.\n\\] When \\(\\lambda_1 &gt; \\lambda_2\\), the term \\(\\left( \\frac{\\lambda_i}{\\lambda_1} \\right)^t\\) will go to 0 for all \\(i \\geq 2\\) as \\(t\\) increases. As long as \\(\\mathbf{v}_1^\\top \\mathbf{p}^{(0)}\\) is non-zero, the first term will dominate the sum, and \\[\n\\lim_{t \\to \\infty} \\mathbf{p}^{(t)} = \\lambda_1^t \\mathbf{v}_1 \\left(\\mathbf{w}_1^\\top \\mathbf{p}^{(0)}\\right).\n\\] Since every entry of \\(\\mathbf{M}\\) is at most \\(1\\) and \\(\\mathbf{1}^\\top \\mathbf{M} = \\mathbf{1}^\\top\\), it’s possible to show that \\(\\lambda_1 = 1\\).\nIn summary, the vector that Google uses to measure the importance of a page is simply a multiple of the first eigenvector of the matrix \\(\\mathbf{M}\\). Repeatedly multiplying matrices is known as the power method, and results in the dominant eigenvector(s) of a matrix."
  },
  {
    "objectID": "notes/PolynomialRegression.html",
    "href": "notes/PolynomialRegression.html",
    "title": "Polynomial Regression",
    "section": "",
    "text": "We have so far focused on linear regression. In the last lecture, we saw how to more efficiently train linear models using gradient descent. However, we still have the issue that linear models ultimately fit linear functions, which may not be expressive enough to capture the underlying patterns in the data. In this lecture, we will see how to make linear models more expressive by adding additional features."
  },
  {
    "objectID": "notes/PolynomialRegression.html#polynomial-regression",
    "href": "notes/PolynomialRegression.html#polynomial-regression",
    "title": "Polynomial Regression",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\nA linear model learns weights for each feature i.e.,\n\\[\nf(\\mathbf{x}) = w_0 + w_1 x_1 + w_2 x_2 + \\ldots + w_d x_d.\n\\]\nIn polynomial regression, we add additional features that are polynomial functions of the original features, e.g., \\[\nf(\\mathbf{x}) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + w_4 x_1^2 + w_5 x_2^2\n\\]\nIn terms of matrix multiplication view of linear regression, we can think of adding additional columns to the feature matrix \\(\\mathbf{X}\\) by multiplying existing columns together, or raising them to a power.\n\n\n\nNotice that the model is still linear in the parameters, so we can still use the same techniques to fit the model; the only change is that we have more features. Do these additional features help?\nClaim: The fit of the regression model can only be improved by adding additional features. Formally,\n\\[\n\\min_{\\mathbf{w} \\in \\mathbb{R}^{d_\\text{aug}}} \\|\\mathbf{y} - \\mathbf{X}_{\\text{aug}}\\mathbf{w}\\|^2_2 \\leq \\min_{\\mathbf{w} \\in \\mathbb{R}^{d}} \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^2_2\n\\]\nwhere \\(\\mathbf{X}\\) is the original data matrix with \\(d\\) features, and \\(\\mathbf{X}_{\\text{aug}}\\) is the augmented data matrix with additional polynomial features, for a total of \\(d_{\\text{aug}} &gt; d\\) features.\nTo see this, note that any weight vector \\(\\mathbf{w}\\) for the original data matrix \\(\\mathbf{X}\\) can be extended to a weight vector \\(\\mathbf{w}'\\) for the augmented data matrix \\(\\mathbf{X}_{\\text{aug}}\\) by setting the weights corresponding to the additional features to zero.\nLet’s see an example of polynomial regression in action. Consider a quadratic function with some noise defined on \\(x \\in [-1, 1]\\). We will fit polynomial regression models of varying degrees to this data.\n\n\n\nThe degree-1 polynomial (linear regression) does not fit the data well, the degree-2 polynomial gives a much better fit, and the degree-10 polynomial fits the data perfectly. However, the degree-10 polynomial appears to be fitting even the noise in the data, to the point where it appears to be oscillating wildly between data points. This phenomenon is called overfitting, where the model effectively memorizes the training data.\nIn the picture, we can tell that the degree-2 polynomial fits the pattern well, without overfitting to the noise. But how can we identify the right fit in general for higher dimensional and large datasets?\n\nGeneralization Error\nIf a model memorizes the training data to the point of overfitting, it will not generalize well to new data. We can measure how well it will generalize by splitting the data into a training set and a validation set. We train the model on the training set, and measure the performance on both the training set and the validation set.\nBecause the validation set is a random sample from the true data distribution, it gives us an unbiased estimate of the generalization error. We can use the validation loss as an approximation for the generalization error when we choose the best model and hyperparameters.\nThe most common way to split the data is to use 80% of the data for training and 10% for validation and 10% for testing. However, this is not ideal because it reduces the amount of data available for training and for approximating the generalization error. An alternative is to use cross-validation, where we split the non-test data into \\(k\\) folds, and use each fold as the validation set while training on the remaining \\(k-1\\) folds. This gives a better estimate of the generalization error, and allows us to use more data for training. However, the cost is the increased computational cost of training the model \\(k\\) times. On the problem set, we saw how to circumvent this cost for linear regression by exactly computing leave-one-out predictions.\nIn practice, we often introduce a bias by using the validation set to tune the model. When we train and evaluate a model multiple times on the same validation set to update the model (e.g., change hyperparameters, architecture, training method), we are effectively using the validation set as part of the training process. Eventually, the model can achieve better performance on the validation data, even though we never used the validation data to train the model. More broadly, this phenomenon is related to the more general problem of \\(p\\)-hacking in scientific research, where researchers try multiple analyses and only report the ones that give significant results. Our solution is to use a separate test set that is only used once at the very end to report the final performance of the model.\n\n\nRegularization\nSo what model achieves lowest generalization error? According to Occam’s razor, the simplest model that explains the data is often the best choice. However, we’ll need to define what we mean by “simple”.\nIn polynomial regression, we can think of the degree of the polynomial as a measure of complexity. However, this is not a very general measure of complexity, and it ignores how each degree is used.\nIn the example above, we fit the data with a degree-1, degree-2, and degree-10 polynomial. Let’s inspect the weights learned by each model. The histogram (below) shows that the degree-1 and degree-2 polynomials have small weights, while the degree-10 polynomial has some very large weights. This suggests that the degree-10 polynomial is a far more complex model, as it can achieve large changes in the output for small changes in the input.\n\n\n\nOne way of encouraging models to be simpler is to add a term to the loss function that penalizes large weights, e.g., we could use the following regularized loss function: \\[\n\\begin{align*}\n\\mathcal{L}(\\mathbf{w}) + \\lambda \\|\\mathbf{w}\\|^2_2,\n\\end{align*}\n\\] where \\(\\mathcal{L}(\\mathbf{w})\\) is the original loss function (e.g., mean squared error for regression), and \\(\\|\\mathbf{w}\\|^2_2\\) is the penalty term with hyperparameter \\(\\lambda &gt; 0\\). Increasing the value of \\(\\lambda\\) increases the strength of the penalty, which encourages the model to learn smaller weights and thus simplifies the model.\nWhen we use the \\(\\ell_2\\) norm for regularization, this is called ridge regression. We can also use the \\(\\ell_1\\) norm, this is called lasso regression, and it has the effect of encouraging sparsity in the model, i.e., some weights will be exactly zero. For some intuition on why \\(\\ell_1\\) regularization encourages sparsity, check out these notes. Lasso regression is particularly useful when we have many features, but we believe only a few of them are actually relevant to the prediction.\n\n\nDouble Descent\nHistorically, people believed there is a tradeoff between performance on the training data and performance on the test data: If we train a model for too long, or use a model that is too complex, it will overfit to the training data and perform poorly on the test data. Instead, the story went, we should monitor both the training and validation loss, and stop training when the validation loss starts to increase.\nOne of the key insights of modern machine learning is that this story is incomplete. If we keep training a model that appears to be overfitting, we can often see the validation loss start to decrease again.\n\n\n\nThis phenomenon is called double descent, because the validation loss curve descends first as the model fits the training data better, then ascends as the model overfits, and finally descends again. One explanation for the second descent is that the model is implicitly regularized by the training process. (Check out these notes for a proof of implicit regularization in linear regression.)\nThere is also another form of double descent that occurs as we increase the number of parameters in the model. When the number of parameters is greater than the number of training examples, the model can perfectly fit the training data, but it may not generalize well to new data. However, as we increase the number of parameters even further, it appears empirically that models implicitly regularize themselves and generalize better."
  },
  {
    "objectID": "notes/Autoencoders.html",
    "href": "notes/Autoencoders.html",
    "title": "Autoencoders",
    "section": "",
    "text": "We have so far assumed that our data is labeled (e.g., we know an image depicts a dog). There is a wealth of unlabeled data available, and we would like to use it to improve our models. For example, think of the many images on the web, or millions of documents of text. Today, we will explore how to leverage this unlabeled data using unsupervised learning techniques.\n\nIntroduction\nThere are many simple but clever ideas that we will see in unsupervised learning. The first is an autoencoder, which allows us to learn a meaningful, latent representation of the input data. The idea of autoencoders is quite natural: when we don’t have labels for our data, let’s use the data itself as a label.\nLet \\(d\\) be the dimension of the input data. We will map the input data to a compressed representation in \\(k\\) dimensions. Let \\(f_0: \\mathbb{R}^d \\to \\mathbb{R}^k\\) be the encoder, often a neural network, that maps the input data \\(\\boldsymbol{x}\\) to the latent representation \\(\\boldsymbol{z} = f_0(\\boldsymbol{x})\\). We will then map the latent representation back to the original space with a decoder \\(f_1: \\mathbb{R}^k \\to \\mathbb{R}^d\\), also a neural network, where \\(\\tilde{\\boldsymbol{x}} = f_1(\\boldsymbol{z})\\) is the reconstruction of the original input \\(\\boldsymbol{x}\\).\nOur goal is to minimize the reconstruction error, which is typically measured as the mean squared error (MSE) between the original input \\(\\boldsymbol{x}\\) and the reconstructed input \\(\\tilde{\\boldsymbol{x}}\\):\n\\[\n\\mathcal{L}_{\\text{recon}} = \\frac1{n} \\sum_{i=1}^n \\| \\boldsymbol{x}^{(i)} - \\tilde{\\boldsymbol{x}}^{(i)} \\|^2_2.\n\\]\n\n\n\nNotice that this problem is trivial when \\(k \\geq d\\), since we can simply represent \\(\\boldsymbol{z} = \\boldsymbol{x}\\) in the latent space without any loss of information. Crucially, our architecture must have a bottleneck, meaning that we need to enforce \\(k &lt; d\\) in order to learn a non-trivial representation.\nThere are many applications of autoencoders:\n\ndata compression (for example, JPEG is a form of lossy compression that can be interpreted as an autoencoder),\ndenoising (removing noise from images),\ninpainting (filling in missing parts of images),\nrepresentation learning (discovering useful features that can be used in downstream applications).\n\nWhen we map from \\(\\mathbb{R}^d\\) to a smaller dimension \\(\\mathbb{R}^k\\), we are necessarily losing information. However, much of this information isn’t useful. With images, for example, most \\(d\\)-dimensional vectors are not natural images that we would recognize. Instead, there is a manifold of \\(d\\)-dimensional natural images. Learning this manifold directly is challenging, and autoencoders provide a powerful tool for mapping to a lower-dimensional space and back.\n\n\n\nOnce we represent the data in the latent space, we can use it for various tasks such as classification, clustering, and generation. Instead of working in the pixel-space, for example, we can work in a lower dimension that captures the essential features of the data. Since all of our machine learning algorithms run in time dependent on the dimensionality of the input space, reducing the dimensionality can lead to significant speedups. Such latent representations form the backbone of state-of-the-art models in various domains, including computer vision, natural language processing, and speech recognition.\n\n\nVariational Autoencoders\nThe natural manifold of our data in \\(\\mathbb{R}^d\\) is generally complicated and non-convex. For example, we may have two images of the same cat, but the linear combination of these two images at the pixel-level does not lie on the manifold of cat images.\nWhen we build the autoencoder, we have a chance to make the latent space more structured. One method is called Variational Autoencoders, which impose a probabilistic structure on the latent space. Instead of the latent \\(\\boldsymbol{z} = f_0(\\boldsymbol{x})\\) being a deterministic function of the input, we model it as a distribution. Because of the many elegant properties of Gaussian distributions, we often choose to model the latent space as a multivariate Gaussian distribution. Then, the latent is drawn from a Gaussian distribution i.e., \\(\\boldsymbol{z} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_\\boldsymbol{x}, \\boldsymbol{\\Sigma}_\\boldsymbol{x})\\) is a random variable, where \\(\\boldsymbol{\\mu}_\\boldsymbol{x}\\) is the input-dependent mean and \\(\\boldsymbol{\\Sigma}_\\boldsymbol{x}\\) is the input-dependent covariance.\nIn variational autoencoders, the loss consists of the normal reconstruction error and a measure of the distance between the learned distribution and a well-behaved prior distribution, typically the standard Gaussian \\(\\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{I})\\): \\[\n\\mathcal{L}_\\text{VAE} = \\alpha \\mathcal{L}_{\\text{recon}} + (1-\\alpha) D_{\\text{KL}}(\\mathcal{N}(\\boldsymbol{\\mu}_\\boldsymbol{x}, \\boldsymbol{\\Sigma}_\\boldsymbol{x}) || \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{I})),\n\\] where \\(\\alpha \\in (0,1)\\) is a hyperparameter that balances the two terms, and \\(D_{KL}\\) is the Kullback-Leibler divergence. Formally, \\[\nD_{\\text{KL}}(P, Q) =\n\\mathbb{E}_{\\boldsymbol{z} \\sim P} \\left[ \\log \\frac{P(\\boldsymbol{z})}{Q(\\boldsymbol{z})} \\right],\n\\] where \\(P\\) is the learned distribution and \\(Q\\) is the prior distribution. The two losses will push the variational autoencoder in different directions: The reconstruction loss encourages the model to generate realistic samples, while the KL divergence encourages the latent space to always resemble the standard Gaussian distribution with mean \\(\\boldsymbol{0}\\) and covariance \\(\\boldsymbol{I}\\). These two goals are naturally at odds: if the encoder always outputted the mean \\(\\boldsymbol{\\mu}_\\boldsymbol{x}=\\boldsymbol{0}\\), and the covariance \\(\\boldsymbol{\\Sigma}_\\boldsymbol{x}=\\boldsymbol{I}\\), the KL divergence would be 0 but there would be no information for the decoder to reconstruct the input. However, by simultaneously training to achieve both goals, we can learn a meaningful latent space that captures the essential features of the data while also being structured.\n\n\n\nWe will use gradient descent to train the parameters of the encoder \\(f_0\\) and the decoder \\(f_1\\). However, when we use KL divergence as the loss, it’s not immediately clear how to backpropagate through a random sample \\(\\boldsymbol{z} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_\\boldsymbol{x}, \\boldsymbol{\\Sigma}_\\boldsymbol{x})\\). Instead, we will use the encoder to generate \\(\\boldsymbol{\\mu}_\\boldsymbol{x}\\) and \\(\\boldsymbol{\\Sigma}_\\boldsymbol{x}\\), and set \\(\\boldsymbol{z} = \\boldsymbol{\\mu}_\\boldsymbol{x} + \\boldsymbol{\\Sigma}_\\boldsymbol{x}^{1/2} \\boldsymbol{\\epsilon}\\), where \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{I})\\) is a standard normal variable.\n\n\nPrincipal Component Analysis\nPrincipal component analysis (PCA) is the “linear regression” of unsupervised learning, offering a simple but powerful technique for dimensional reduction. PCA is the simplest kind of autoencoder, where the encoder and decoder are each a fully connected linear layer without activation functions.\nLet \\(\\mathbf{W}_0 \\in \\mathbb{R}^{k \\times d}\\) be the encoder weight matrix, and \\(\\mathbf{W}_1 \\in \\mathbb{R}^{d \\times k}\\) be the decoder weight matrix. The latent for input \\(\\mathbf{x}\\) is \\(\\mathbf{z}^\\top = \\mathbf{x}^\\top \\mathbf{W}_0\\), and the reconstruction is \\(\\tilde{\\mathbf{x}}^\\top = \\mathbf{z}^\\top \\mathbf{W}_1  = \\mathbf{x}^\\top \\mathbf{W}_0 \\mathbf{W}_1\\).\n\n\n\nWe train the encoder and decoder on \\(n\\) (unlabelled) data points \\(\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(n)}\\). Let \\(\\mathbf{X} \\in \\mathbf{R}^{n \\times d}\\) be the matrix of input data, where the \\(i\\)th row corresponds to data point \\({\\mathbf{x}^{(i)}}^\\top\\).\n\n\n\nThe goal is for the input matrix \\(\\mathbf{X}\\) to be well approximated by the reconstruction \\(\\tilde{\\mathbf{X}} = \\mathbf{X} \\mathbf{W}_0 \\mathbf{W}_1\\). In particular, \\[\n\\mathcal{L}( \\mathbf{W}_0, \\mathbf{W}_1)\n= \\sum_{i=1}^n \\| \\mathbf{x}^{(i)} - \\tilde{\\mathbf{x}}^{(i)} \\|^2\n= \\sum_{i=1}^n \\sum_{j=1}^d (x_j^{(i)} - \\tilde{x}_j^{(i)})^2\n= \\| \\mathbf{X} - \\tilde{\\mathbf{X}} \\|_\\text{F}^2\n\\] where the Frobenius norm \\(\\| \\mathbf{A} \\|_\\text{F}^2\\) is the sum of squared entries of \\(\\mathbf{A}\\), and the last equality follows because \\([\\mathbf{X}]_{i,j} = x_j^{(i)}\\) is the \\(j\\)th feature of the \\(i\\)th data point.\nWe could apply gradient descent to minimize this loss with respect to the weights \\(\\mathbf{W}_0\\) and \\(\\mathbf{W}_1\\). But, like with linear regression, we can actually derive the optimal solution. Let’s revisit some linear algebra to see how we can do this derivation.\nWhile only some matrices have eigendecompositions, all matrices have singular value decompositions (SVDs). In particular, let \\(r\\) be the rank of \\(\\mathbf{X}\\), then we can write \\[\n\\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top\n\\] where \\(\\mathbf{U} \\in \\mathbb{R}^{n \\times r}\\) and \\(\\mathbf{V} \\in \\mathbb{R}^{d \\times r}\\) have orthonormal columns, and \\(\\mathbf{\\Sigma} \\in \\mathbb{R}^{r \\times r}\\) is a diagonal matrix with non-negative entries.\n\n\n\nBecause the columns of \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are orthonormal, we have \\[\n\\mathbf{U}^\\top \\mathbf{U} = \\mathbf{I}_r = \\mathbf{V}^\\top \\mathbf{V}.\n\\] Using our outerproduct perspective on matrix multiplication, we can write \\[\n\\mathbf{X} = \\sum_{i=1}^r \\mathbf{u}_i \\sigma_i \\mathbf{v}_i^\\top\n\\] where \\(\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_r \\geq 0\\) are the singular values of \\(\\mathbf{X}\\), and \\(\\mathbf{u}_i \\in \\mathbb{R}^{n \\times 1}\\) and \\(\\mathbf{v}_i \\in \\mathbb{R}^{d \\times 1}\\) are the left and right singular vectors, respectively. Let \\(\\mathbf{X}_k\\) be the rank-\\(k\\) approximation \\[\n\\mathbf{X}_k = \\sum_{i=1}^k \\mathbf{u}_i \\sigma_i \\mathbf{v}_i^\\top.\n\\] With this notation, we are ready to state a useful linear algebra result.\nEckart-Young-Mirsky Theorem: The best rank-\\(k\\) approximation of a matrix \\(\\mathbf{X}\\) in the Frobenius norm is given by its top \\(k\\) singular values and corresponding singular vectors i.e., \\[\n\\mathbf{X}_k = \\arg \\min_{\\text{rank}-k \\text{ } \\tilde{\\mathbf{X}}} \\| \\mathbf{X} - \\tilde{\\mathbf{X}} \\|_\\text{F}^2.\n\\]\n\n\n\nProof of Eckart-Young-Mirsky Theorem\n\nWe’ll prove the theorem using several properties of the trace. For a square matrix \\(\\mathbf{M}\\), the trace \\(\\text{tr}(\\mathbf{M})\\) is the sum of the diagonal entries, and it has the following three properties:\nFirst, \\(\\| \\mathbf{X} \\|_\\text{F}^2 = \\text{tr}(\\mathbf{X}^\\top \\mathbf{X})\\) for any \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\). To see why, observe that \\[\\| \\mathbf{X} \\|_\\text{F}^2 = \\sum_{i=1}^n \\sum_{j=1}^d [\\mathbf{X}]_{i,j}^2 = \\sum_{i=1}^n \\| [\\mathbf{X}]_{i,} \\|_2^2 = \\text{tr}(\\mathbf{X}^\\top \\mathbf{X}).\\]\nSecond, \\(\\text{tr}(\\mathbf{A} \\mathbf{B}) = \\text{tr}(\\mathbf{B} \\mathbf{A})\\) for any matrices \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times d}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{d \\times n}\\). To see why, observe that \\[\n\\text{tr}(\\mathbf{A} \\mathbf{B}) = \\sum_{i=1}^n [\\mathbf{A B}]_{i,i}\n= \\sum_{i=1}^n \\sum_{j=1}^d [\\mathbf{A}]_{i,j} [\\mathbf{B}]_{j,i}\n= \\sum_{j=1}^d \\sum_{i=1}^n [\\mathbf{B}]_{j,i} [\\mathbf{A}]_{i,j}\n= \\sum_{j=1}^d [\\mathbf{B A }]_{j,j}\n= \\text{tr}(\\mathbf{B} \\mathbf{A}).\n\\]\nThird, \\(\\| \\mathbf{XV} \\|_\\text{F}^2 = \\| \\mathbf{X} \\|_\\text{F}^2\\), where the columns of \\(\\mathbf{V} \\in \\mathbb{R}^{d \\times r}\\) are orthonormal. (The analogous result holds for multiplying \\(\\mathbf{X}\\) on the left by \\(\\mathbf{U}^\\top\\), where the columns of \\(\\mathbf{U} \\in \\mathbb{R}^{n \\times r}\\) are orthonormal.) To see why, observe that \\[\n\\| \\mathbf{XV} \\|_\\text{F}^2\n= \\text{tr}((\\mathbf{XV})^\\top (\\mathbf{XV}))\n= \\text{tr}(\\mathbf{V}^\\top \\mathbf{X}^\\top \\mathbf{X} \\mathbf{V})\n= \\text{tr}(\\mathbf{X}^\\top \\mathbf{X} \\mathbf{V} \\mathbf{V}^\\top)\n= \\text{tr}(\\mathbf{X}^\\top \\mathbf{X})\n= \\| \\mathbf{X} \\|_\\text{F}^2,\n\\] where we used the first property in the first and last equality, the second property in the third equality, and the orthonormality of the columns of \\(\\mathbf{V}\\) in the fourth equality.\nWith these properties, we are finally ready to prove the theorem. We have \\[\n\\begin{align}\n\\| \\mathbf{X} - \\tilde{\\mathbf{X}} \\|_\\text{F}^2\n= \\| \\mathbf{U \\Sigma V}^\\top - \\tilde{\\mathbf{X}} \\|_\\text{F}^2\n= \\| \\mathbf{U}^\\top \\mathbf{U \\Sigma V}^\\top \\mathbf{V} - \\mathbf{U}^\\top \\tilde{\\mathbf{X}} \\mathbf{V} \\|_\\text{F}^2\n= \\| \\mathbf{\\Sigma} - \\mathbf{C} \\|_\\text{F}^2,\n\\end{align}\n\\] where we use \\(\\mathbf{C} = \\mathbf{U}^\\top \\tilde{\\mathbf{X}} \\mathbf{V}\\) for notational convenience. Continuing, \\[\n\\| \\mathbf{\\Sigma} - \\mathbf{C} \\|_\\text{F}^2\n= \\sum_{i=1}^r (\\sigma_i - [\\mathbf{C}]_{i,i})^2\n+ \\sum_{i\\neq j} [\\mathbf{C}]_{i,j}^2.\n\\] When we choose \\(\\mathbf{C}\\), we can minimize this expression by setting \\([\\mathbf{C}]_{i,j} = 0\\) for all \\(i \\neq j\\). This means that the optimal \\(\\mathbf{C}\\) is diagonal, and we can choose up to \\(k\\) non-zero entries. Setting \\([\\mathbf{C}]_{i,i} = \\sigma_i\\) for \\(i \\leq k\\) and \\(0\\) otherwise gives us the best rank-\\(k\\) approximation. If we chose some \\(i' &gt; k\\) instead of \\(i \\leq k\\), we would be choosing a smaller singular value, which would increase the overall error. With the optimal choice \\(\\mathbf{C} = \\boldsymbol{\\Sigma}_k\\), we have \\(\\tilde{\\mathbf{X}} = \\mathbf{U} \\boldsymbol{\\Sigma}_k \\mathbf{V}^\\top = \\mathbf{X}_k\\), and error \\[\n\\| \\mathbf{X} - \\tilde{\\mathbf{X}} \\|_\\text{F}^2\n= \\sum_{i=k+1}^r \\sigma_i^2.\n\\]\n\n\nThe Eckart-Young-Mirsky theorem tells us that the best rank-\\(k\\) approximation of a matrix is given by its top \\(k\\) singular values and corresponding singular vectors. So we’d like to choose \\(\\mathbf{W}_0\\) and \\(\\mathbf{W}_1\\) so that \\(\\tilde{\\mathbf{X}} = \\mathbf{X} \\mathbf{W}_0 \\mathbf{W}_1\\). Setting \\(\\mathbf{W}_0 = \\mathbf{V}_k\\) and \\(\\mathbf{W}_1 = \\mathbf{V}_k^\\top\\) gives \\[\n\\tilde{\\mathbf{X}}\n= \\mathbf{X} \\mathbf{V}_k \\mathbf{V}_k^\\top\n= \\sum_{i=1}^r \\mathbf{u}_i \\sigma_i  \\mathbf{v}_i^\\top \\sum_{j=1}^k \\mathbf{v}_j \\mathbf{v}_j^\\top\n= \\sum_{i=1}^k \\mathbf{u}_i \\sigma_i \\mathbf{v}_i^\\top = \\mathbf{X}_k,\n\\] where the second equality follows because \\(\\mathbf{v}_i^\\top \\mathbf{v}_j=1\\) if \\(i=j\\) and \\(0\\) otherwise.\nWe call \\(\\mathbf{X} \\mathbf{V}_k = \\mathbf{U}_k \\boldsymbol{\\Sigma}_k\\) the latent representation, and the columns of \\(\\mathbf{V}_k\\) are the principal components. When we perform PCA, we are effectively running a singular value decomposition on \\(\\mathbf{X}\\). In practice, we generally mean-center and normalize \\(\\mathbf{X}\\) first; in particular, we subtract the mean from each column, and divide each column by its \\(\\ell_2\\)-norm.\nWe could compute the singular value decomposition directly using e.g., the scipy library: Returning the full decomposition requires \\(O(nd^2)\\) time, while computing a rank-\\(k\\) approximation can be done in \\(O(ndk)\\) time. We could have also computed the eigendecomposition of \\(\\mathbf{X}^\\top \\mathbf{X} = \\mathbf{V} \\boldsymbol{\\Sigma}^2 \\mathbf{V}^\\top\\), and \\(\\mathbf{X} \\mathbf{X}^\\top = \\mathbf{U} \\boldsymbol{\\Sigma}^2 \\mathbf{U}^\\top\\). But, this approach is generally less efficient since it requires computing the full eigendecomposition of a \\(d \\times d\\) matrix and a \\(n\\times n\\), which is roughly \\(O(d^3 + n^3)\\).\nIn practice, PCA gives us a low-dimensional representation of the data that captures the most important variance.\n\n\n\nAs we increase the rank of the approximation, the reconstruction error decreases. Recall from the proof of the Eckart-Young-Mirsky theorem that the error of the rank-\\(k\\) approximation is given by \\[\n\\sum_{i=k+1}^r \\sigma_i^2.\n\\] When the singular values are all similar, the reconstruction error decreases more slowly as we add more components. However, if the singular values decay quickly, we can achieve a significant reduction in error by adding just a few components.\n\n\n\nThe reconstruction error is also smaller when the rank is lower. So if we have columns that are linear related, then the error tends to be lower. For example, in a housing dataset, maybe we have features like square footage, lot size, and yard size. Since these features are all related, we can capture their relationships more easily with fewer dimensions. Other kinds of data with low-rank structure include image data, where pixels in a small region are often correlated, and text data, where word usage patterns can be captured with a small number of topics.\nConsider an individual data point \\(\\mathbf{x}\\). The latent vector \\(\\mathbf{z}\\) is obtained by projecting \\(\\mathbf{x}\\) onto the principal components: \\[\n\\mathbf{z} = \\sum_{i=1}^k \\langle \\mathbf{x}, \\mathbf{v}_i \\rangle.\n\\] Then the reconstruction is given by \\[\n\\tilde{\\mathbf{x}} = \\sum_{i=1}^k z_i \\mathbf{v}_i = \\sum_{i=1}^k \\langle \\mathbf{x}, \\mathbf{v}_i \\rangle \\mathbf{v}_i.\n\\]\n\n\n\nThe magnitude of the reconstruction is the same as the magnitude of the latent representation. To see why, observe that \\[\n\\| \\tilde{\\mathbf{x}} \\|_2^2\n= \\left \\| \\sum_{i=1}^k \\langle \\mathbf{x}, \\mathbf{v}_i \\rangle \\mathbf{v}_i \\right \\|_2^2\n= \\langle \\sum_{i=1}^k \\langle \\mathbf{x}, \\mathbf{v}_i \\rangle \\mathbf{v}_i, \\sum_{j=1}^k \\langle \\mathbf{x}, \\mathbf{v}_j \\rangle \\mathbf{v}_j \\rangle\n= \\sum_{i=1}^k \\langle \\mathbf{x}, \\mathbf{v}_i \\rangle^2\n= \\sum_{i=1}^k z_i^2 = \\| \\mathbf{z} \\|_2^2.\n\\] Similarly, the distance between reconstructed points \\(\\tilde{\\mathbf{x}}^{(i)}\\) and \\(\\tilde{\\mathbf{x}}^{(j)}\\) is the same as the distance between the latent representations \\(\\mathbf{z}^{(i)}\\) and \\(\\mathbf{z}^{(j)}\\). Do you see why?\nIf the original data is close to the reconstructed data, then \\(\\| \\tilde{\\mathbf{x}} \\|_2 \\approx \\| \\mathbf{x} \\|_2\\), and the latent representation \\(\\| \\mathbf{z} \\|_2\\) will also be similar. Put another way, the latent representations effectively capture the behavior of the original data. Next, we’ll see how PCA can effectively represent language data.\n\n\nSemantic Embeddings\nWhen we discussed transformers, we assumed we could meaningfully represent words as vectors in a continuous space. This idea is at the heart of semantic embeddings, where words with similar meanings are mapped to nearby latent representations.\nOur first attempt will be with a document-word matrix \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\). Each row of \\(\\mathbf{X}\\) corresponds to a document, and each column corresponds to a word. The entry \\(X_{ij}\\) represents whether word \\(j\\) appears in document \\(i\\). When we apply PCA to this matrix, we can obtain a low-dimensional representation of the documents, and of the words!\n\n\n\nAmong other applications, the latent document representations are useful for efficient search. By representing documents in a lower-dimensional space, we can quickly find similar documents using techniques like nearest neighbor search.\nLet \\(\\mathbf{z}\\) be the latent representation of a document \\(\\mathbf{x}\\). Consider the principal components \\(\\mathbf{v}_i\\) and \\(\\mathbf{v}_j\\) associated with words \\(i\\) and \\(j\\). If the reconstruction \\(\\tilde{\\mathbf{X}}\\) is close to the original \\(\\mathbf{X}\\), then \\(\\langle \\mathbf{z}, \\mathbf{v}_i \\rangle \\approx 1\\) if word \\(i\\) appears in the document, and \\(\\langle \\mathbf{z}, \\mathbf{v}_j \\rangle \\approx 1\\) if word \\(j\\) appears. In this case, \\(\\langle \\mathbf{v}_i, \\mathbf{v}_j \\rangle\\) will likely be large. In this way, the principal components can capture the meaning of words.\n\n\n\nOne interesting application is that we can do “word math” like \\[\n\\mathbf{y}_\\text{dog} - \\mathbf{y}_\\text{old} + \\mathbf{y}_\\text{young} \\approx \\mathbf{y}_\\text{puppy}.\n\\]\nFor our document-word matrix, we defined the document representations as \\(\\mathbf{X} \\mathbf{V}_k = \\mathbf{U}_k \\mathbf{\\Sigma}_k\\) and the word representations as \\(\\mathbf{V}_k^\\top\\). But, we could have just as easily defined the document representations as \\(\\mathbf{U}_k\\) and the word representations as \\(\\mathbf{\\Sigma}_k \\mathbf{V}_k^\\top\\). With this definition, we can write the outer product between the word representations and itself as: \\[\n\\left( \\mathbf{\\Sigma}_k \\mathbf{V}_k^\\top\\right)^\\top \\left( \\mathbf{\\Sigma}_k \\mathbf{V}_k^\\top \\right)\n= \\mathbf{V}_k \\mathbf{\\Sigma}_k^\\top \\mathbf{\\Sigma}_k \\mathbf{V}_k^\\top\n= \\tilde{\\mathbf{X}}^\\top \\tilde{\\mathbf{X}}.\n\\] When \\(\\tilde{\\mathbf{X}}\\) is close to \\(\\mathbf{X}\\), the inner product between word vectors \\(\\mathbf{y}_i\\) and \\(\\mathbf{y}_j\\) can be approximated as: \\[\n\\langle \\mathbf{y}_i, \\mathbf{y}_j \\rangle \\approx [\\mathbf{X}^\\top \\mathbf{X}]_{i,j}\n= \\text{\\# documents with both $i$ and $j$}\n\\]\nMore generally, we can compress a variety of language data beyond document and word matrices. As a general recipe, we can compute the similarity between all word pairs \\(i\\) and \\(j\\), and store the results in a matrix \\(\\mathbf{M} \\in \\mathbb{R}^{d \\times d}\\). We find a low-rank approximation \\(\\mathbf{M} \\approx \\mathbf{Y}^\\top \\mathbf{Y}\\) for some \\(\\mathbf{Y} \\in \\mathbb{R}^{d \\times k}\\). Then, we can define the word representations as the rows of \\(\\mathbf{Y}\\).\nFor example, one approach is to define similarity as the number of times a word appears in the same context as another word. This can be captured by counting co-occurrences in a sliding window over a corpus of text.\n\n\n\nWe then could process the co-occurrence counts with non-linearities to account for Zipf’s law. In the popular word2vec algorithm, for example, the similarity is given by \\[\n[\\mathbf{M}]_{i,j} = \\log \\frac{\\text{\\# contexts with both $i$ and $j$}}{\\text{\\# contexts with $i$}\\cdot{\\text{\\# contexts with $j$}}}.\n\\]\nNote: If \\(\\mathbf{M}\\) is not symmetric, then the factorization may be of the form \\(\\mathbf{M} \\approx \\mathbf{W}^\\top \\mathbf{Y}\\), where \\(\\mathbf{W} \\neq \\mathbf{Y}\\). In this case, we can take the rows of either \\(\\mathbf{W}\\) or \\(\\mathbf{Y}\\) as the word representations.\nThere are many interesting applications of latent representations.\n\nUnsupervised Translation\nUsing our co-occurrence strategy, we can turn documents or even transcribed conversations into language data \\(\\mathbf{X}\\), and then semantic embeddings \\(\\mathbf{Y}\\) without any supervision. In fact, we can construct these embeddings for multiple languages in parallel. If the languages have similar structures (e.g., family relationships), we can leverage this to rotate one language’s embeddings into another’s space.\n\n\n\nWhile not perfect, the aligned vectors can give a translation strategy without any supervision. Some researchers are even applying these strategies to map the sounds of monkeys or whales to human language!\n\n\nGraph Representations\nThe applications of autoencoders go far beyond language. One particularly versatile data structure is a graph, where nodes represent entities and edges represent relationships. For example, we can use a graph to represent social networks (people are nodes and friendships are edges), road infrastructure (intersections are nodes and roads are edges), or even knowledge graphs (concepts are nodes and relationships are edges). Even when the underlying graph is massive, we can use a random walk through the graph and apply our co-occurrence strategy to learn embeddings for the nodes.\n\n\n\n\n\nMulti-modal Contrastive Learning\nA particularly interesting feature of modern machine learning is the connection between different modalities of data e.g., generating images from text descriptions or vice versa. The method for achieving this connection is known as contrastive learning, and can be viewed as yet another application of autoencoders.\nConsider a dataset of captioned images where each image is paired with a descriptive caption. We can use neural networks (e.g., a convolutional network for the image, and a transformer for the text) to map the data to latent representations. Then we represent the similarity of the image and text embeddings in a similarity matrix \\(\\mathbf{M} = \\mathbf{I}\\), where pairs of related images and captions that match have a value of 1, and all other pairs have a value of 0. The resulting embeddings, trained with updates to the networks so that \\(\\mathbf{M} \\approx \\mathbf{W}^\\top \\mathbf{Y}\\), can then be used for various tasks such as image captioning or text-to-image generation.\n\n\n\nWe can then use the embeddings in downstream techniques like diffusion to generate images from text descriptions."
  },
  {
    "objectID": "notes/ExplainableAI.html",
    "href": "notes/ExplainableAI.html",
    "title": "Explainable AI and Active Learning",
    "section": "",
    "text": "In this course, we have explored how machine learning can be used to solve tasks, from supervised learning with linear models to reinforcement learning with neural networks. The approaches we’ve discussed, especially when we throw lots of compute and data at them, tend to work very well in practice. However, as these models are deployed in the real world, it becomes increasingly important to understand their behavior. Today, we’ll explore one way to gain insights into model behavior.\nUnderstanding a model like linear regression is easy. The output of the model is a weighted linear combination of the inputs: \\[\nf(\\mathbf{x}) = \\sum_{i=1}^{d} w_i x_i.\n\\] So, changing feature \\(i\\) from baseline \\(b_i\\) to \\(x_i\\) will simply increase the output at a rate of the weight \\(w_i\\). Formally, the attribution of the change in output to feature \\(i\\) from \\(x_i\\) to \\(b_i\\) can be expressed as: \\[\n\\phi_i = w_i (x_i - b_i).\n\\] With these attribution values \\(\\phi_i\\) in hand, we can perform several safety checks. For example, it would be concerning if the attribution of race in a mortgage rate model was non-zero, or if the attribution of gender in a hiring model was non-zero.\nGeneral models (think neural networks with many layers) are unfortunately not so easy to understand. The challenge is that there are non-linear interactions between features, making it difficult to attribute changes in the output to specific input features. For example, consider a model which predicts how weather will impact the growth of plants: Precipitation is beneficial for plant growth, but only when the temperature is high enough; if it’s below freezing, precipitation will instead be harmful. Teasing apart the effects of a feature requires considering not just the feature itself, but also all the other features.\nLet \\(\\mathbf{x} \\in \\mathbb{R}^d\\) be the input to a model, and let \\(f(\\mathbf{x})\\) be the output that we’re trying to understand. We’ll consider the impact of feature \\(x_i\\) relative to a baseline \\(\\mathbf{b} \\in \\mathbb{R}^d\\). In order to tease apart this effect, we will consider all possible ways to set the other features. For a subset of features \\(S \\subseteq [d]\\), define \\(\\mathbf{x}^S\\) so that \\[\n\\mathbf{x}^S_j = \\begin{cases}\nx_i & \\text{if } j \\in S \\\\\nb_j & \\text{if } j \\notin S.\n\\end{cases}\n\\] Then, one natural way to examine the effect of \\(x_i\\) is to compare \\(f(\\mathbf{x}^{S \\cup \\{i\\}}) - f(\\mathbf{x}^S)\\) for all sets \\(S \\subseteq [d] \\setminus \\{i\\}\\). For notational convenience, define a function \\(v: 2^{[d]} \\to \\mathbb{R}\\) so that \\(v(S) = f(\\mathbf{x}^{S})\\).\nWe can think of \\(v(S)\\) as defining a value for each vertex of the \\(d\\)-dimensional hypercube. The attribution of the \\(i\\)th element can then be thought of as the average marginal change from \\(v(S)\\) to \\(v(S \\cup \\{i\\})\\) over all subsets \\(S\\). In order to define this average precisely, we need to decide how to weight the different subsets."
  },
  {
    "objectID": "notes/ExplainableAI.html#shapley-values",
    "href": "notes/ExplainableAI.html#shapley-values",
    "title": "Explainable AI and Active Learning",
    "section": "Shapley Values",
    "text": "Shapley Values\nThere are several desirable properties that we would like our attribution values to satisfy:\n\nNull: If a feature does not affect the output, its attribution should be zero.\nSymmetry: If two features contribute equally to the output, they should receive equal attribution.\nLinearity: If the model is a linear combination of two models, the attribution should be a linear combination of the attributions of the individual models.\nEfficiency: The total attribution across all features should equal the change in output from the baseline.\n\nThese four properties have been studied since the mid-20th century in the context of fairly distributing payouts among players in a cooperative game \\(v: 2^{[d]} \\to \\mathbb{R}\\). The Shapley value, proposed by Nobel laureate Lloyd Shapley, uniquely satisfies all four properties: \\[\n\\begin{align}\n\\phi_i &= \\frac1{d} \\sum_{S \\subseteq [d] \\setminus \\{i\\}} \\frac{v(S \\cup \\{i\\}) - v(S)}{\\binom{d-1}{|S|}}\n\\\\&=\n\\underbrace{\n\\frac1{d}\n\\sum_{\\ell=0}^{d-1}\n  \\underbrace{\n    \\frac1{\\binom{d-1}{\\ell}}\n    \\sum_{\\substack{S \\subseteq [d] \\setminus \\{i\\} \\\\ |S|=\\ell}}\n      \\big( v(S \\cup \\{i\\}) - v(S) \\big)\n  }_{\\text{average over subsets of size }\\ell}\n}_{\\text{average over all subset sizes}}\n\\end{align}\n\\] Intuitively, the Shapley value is the average contribution of a feature to the model’s output, where the distribution equally weights each set size \\(\\ell\\). While different distributions will satisfy the first three properties, only the Shapley value satisfies all four. Mathematically, efficiency requires that \\[\n\\sum_{i=1}^{d} \\phi_i = v([d]) - v(\\emptyset) = f(\\mathbf{x}) - f(\\mathbf{b}).\n\\] Notably, this allows us to decompose the change in the prediction into contributions from each feature.\n\n\n\nBecause of these desirable properties, Shapley values are the de facto method of feature attribution in modern machine learning. But, because there are an exponential number of subsets, computing Shapley values exactly is computationally challenging.\n\nMonte Carlo Estimation\nOur first attempt at approximation will be with the standard Monte Carlo estimator. Any time we have a summation over many terms, we can estimate the entire sum using only a few of its terms. Consider a sampling distribution \\(\\mathcal{D}\\) over subsets \\(S \\subseteq [d] \\setminus \\{i\\}\\). We will let \\(p_S\\) be the probability of sampling subset \\(S\\) from \\(\\mathcal{D}\\). Draw \\(m\\) subsets \\(S_1, \\ldots, S_m\\) with replacement from \\(\\mathcal{D}\\).\nThe Monte Carlo estimator is \\[\n\\tilde{\\phi}_i^\\text{MC} = \\frac1{m} \\sum_{j=1}^{m} \\frac{v(S_j \\cup \\{i\\}) - v(S_j)}{d \\binom{d-1}{|S_j|} p_{S_j}}.\n\\] Often, we set the sampling probabilities to the weights i.e., \\(p_S = \\frac{1}{d \\binom{d-1}{|S|}}\\) so that the estimator can simply be written as \\[\n\\tilde{\\phi}_i^\\text{MC} = \\frac1{m} \\sum_{j=1}^{m} \\left(v(S_j \\cup \\{i\\}) - v(S_j)\\right).\n\\]\nThe expected value of our estimator, with respect to the randomness of the sampling, is \\[\n\\mathbb{E}[\\tilde{\\phi}_i^\\text{MC}]\n= \\mathbb{E}\\left[\\frac1{m} \\sum_{j=1}^{m} \\sum_{S \\subseteq [d] \\setminus \\{i\\}} \\frac{v(S \\cup \\{i\\}) - v(S)}{d \\binom{d-1}{|S|} p_S} \\mathbb{1}[S = S_j]\\right]\n= \\frac1{m} \\sum_{j=1}^{m} \\sum_{S \\subseteq [d] \\setminus \\{i\\}} \\frac{v(S \\cup \\{i\\}) - v(S)}{d \\binom{d-1}{|S|} p_S} \\mathbb{E}[\\mathbb{1}[S = S_j]]\n= \\phi_i,\n\\] where the second equality follows by the linearity of expectation, and the last equality follows because the expectation of an indicator random variable is the probability that the indicator is 1. While it’s important that the estimator is right expectation, we also care about how closely it concentrates around \\(\\phi_i\\). To this end, let’s compute the variance. We’ll use three properties of the variance:\n\nScalar Constant: If \\(c\\) is a constant, then \\(\\text{Var}(cX) = \\mathbb{E}[(cX)^2] - \\mathbb{E}[cX]^2 = c^2 \\text{Var}(X)\\).\nLinearity of Variance: If \\(X\\) and \\(Y\\) are independent random variables, then \\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\). Can you prove why?\nVariance of an Indicator Random Variable: Let \\(\\mathbb{1}[A]\\) be an indicator random variable that is 1 if event \\(A\\) occurs and 0 otherwise. Then, \\(\\text{Var}(\\mathbb{1}[A]) = \\mathbb{E}[\\mathbb{1}[A]^2] - \\mathbb{E}[\\mathbb{1}[A]]^2 = \\Pr(A) - \\Pr(A)^2 \\leq \\Pr(A)\\).\n\nWith these in hand, the variance of our estimator is \\[\n\\text{Var}(\\tilde{\\phi}_i^\\text{MC})\n\\leq \\frac1{m^2} \\sum_{j=1}^{m} \\sum_{S \\subseteq [d] \\setminus \\{i\\}}\n\\left(\\frac{v(S \\cup \\{i\\}) - v(S)}{d \\binom{d-1}{|S|} p_S}\\right)^2\n\\Pr(\\mathbb{1}[S_j = S])\n= \\frac1{m} \\sum_{S \\subseteq [d] \\setminus \\{i\\}}\n\\left(\\frac{v(S \\cup \\{i\\}) - v(S)}{d \\binom{d-1}{|S|} p_S}\\right)^2\np_S\n\\]\nWith our choice of \\(p_S = \\frac{1}{d\\binom{d-1}{|S|}}\\), we get \\[\n\\text{Var}(\\tilde{\\phi}_i^\\text{MC})\n\\leq \\frac1{m} \\sum_{S \\subseteq [d] \\setminus \\{i\\}}\n\\frac{\\left(v(S \\cup \\{i\\}) - v(S)\\right)^2}{d\\binom{d-1}{|S|}}.\n\\]\nUsing Chebyshev’s inequality, we can bound the probability that our estimator deviates from the true value: \\[\n\\Pr\\left(|\\tilde{\\phi}_i^\\text{MC} - \\phi_i| \\geq \\epsilon\\right) \\leq \\frac{\\text{Var}(\\tilde{\\phi}_i^\\text{MC})}{\\epsilon^2}.\n\\] Setting the failure probability to \\(\\delta\\), we could solve for the number of samples \\(m\\) that we need to achieve an \\(\\epsilon\\) approximation to \\(\\phi_i\\).\nWhile an excellent (and simple) estimator, the naive Monte Carlo approach has an unfortunate drawback: Each sampled pair \\(v(S)\\) and \\(v(S \\cup \\{i\\})\\) can only be used for estimating the \\(i\\)th Shapley value \\(\\phi_i\\). In effect, we only use \\(1/d\\) of our total sample budget for each estimate.\n\n\nMaximum Sample Reuse Estimation\nA more sophisticated approach to estimating Shapley values is to rewrite the Shapley value where the subsets are not paired. Observe that \\[\n\\begin{align}\n\\phi_i &= \\sum_{S \\subseteq [d] \\setminus \\{i\\}} \\frac{v(S \\cup \\{i\\}) - v(S)}{d \\binom{d-1}{|S|}}\n\\\\&= \\sum_{S \\subseteq [d] \\setminus \\{i\\} : i \\in S} \\frac{v(S)}{d \\binom{d-1}{|S|-1}}\n- \\sum_{S \\subseteq [d] \\setminus \\{i\\} : i \\notin S} \\frac{v(S)}{d \\binom{d-1}{|S|}}\n\\\\&= \\sum_{S \\subseteq [d] \\setminus \\{i\\}} v(S) \\left(\\frac{\\mathbb{1}[i \\in S]}{d \\binom{d-1}{|S|-1}}\n- \\frac{\\mathbb{1}[i \\notin S]}{d \\binom{d-1}{|S|}}\\right).\n\\end{align}\n\\]\nA natural approach is to use each sampled set \\(S\\) to estimate every Shapley value. This Maximum Sample Reuse (MSR) estimator is\n\\[\n\\begin{align}\n\\tilde{\\phi}_i^\\text{MSR}\n&= \\frac1{m} \\sum_{j=1}^m v(S_j) \\left(\n\\frac{\\mathbb{1}[i \\in S_j]}{d \\binom{d-1}{|S_j|-1}} - \\frac{\\mathbb{1}[i \\notin S_j]}{d \\binom{d-1}{|S_j|}}\n\\right)\n\\frac{1}{p_S}.\n\\end{align}\n\\]\nA similar calculation to before shows that this estimator is also right in expectation. However, its variance depends on \\([v(S)]^2\\) rather than \\([v(S \\cup \\{i\\}) - v(S)]^2\\). In practice, we expect similar inputs \\(\\mathbf{x}^{S \\cup \\{i\\}}\\) and \\(\\mathbf{x}^{S}\\) to yield similar outputs, so the variance of the Monte Carlo estimator is generally much smaller than the variance of the Maximum Sample Reuse estimator."
  },
  {
    "objectID": "notes/ExplainableAI.html#active-linear-regression",
    "href": "notes/ExplainableAI.html#active-linear-regression",
    "title": "Explainable AI and Active Learning",
    "section": "Active Linear Regression",
    "text": "Active Linear Regression\nShapley values have many wonderful and surprising properties. One of them is that they are the best linear approximation to the function \\(v\\), for a certain weighting of the values \\(v(S)\\).\nFor notational convenience, suppose that \\(v(\\emptyset) =0\\). (If not, we could subtract \\(v(\\emptyset)\\) from all values to center them without change the Shapley values.) We can write the vector of Shapley values as \\[\n\\boldsymbol{\\phi} =\n\\begin{bmatrix}\n\\phi_1 \\\\\n\\phi_2 \\\\\n\\vdots \\\\\n\\phi_d\n\\end{bmatrix}\n= \\arg \\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^d: \\langle \\boldsymbol{\\beta}, \\mathbf{1} \\rangle = v([d])}\n\\sum_{S \\subseteq [d]: 0 &lt; |S| &lt; d}\n\\left( v(S) - \\sum_{i \\in S} \\beta_i \\right)^2 w_S,\n\\] where the weights are \\(w_S = \\frac{1}{\\binom{d}{|S|} |S| (d-|S|)}\\). Notice that the weighting in the regression problem is similar to that of the Shapley values themselves. Further, the constraint that \\(\\boldsymbol{\\beta}\\) sums to \\(v([d])\\) ensures that the efficiency property is satisfied, since we assumed that \\(v(\\emptyset) = 0\\).\nUnfortunately, proving this equality is involved, and not particularly informative. But it will be quite useful for estimating Shapley values efficiently.\nWe’ll first need to convert the constrained regression problem to an unconstrained one. Define the input matrix \\(\\mathbf{A}' \\in \\mathbb{R}^{2^d-2 \\times d}\\) so that each row corresponds to a subset \\(S \\subseteq [d]\\) where \\(0 &lt; |S| &lt; d\\), and each column corresponds to an index \\(i \\in [d]\\). The \\((S,i)\\) entry is given by \\([\\mathbf{A}']_{S,i} = \\mathbb{1}[i \\in S] \\sqrt{w_S}\\). Define the target vector \\(\\mathbf{b}' \\in \\mathbb{R}^{2^d-2}\\) so that the \\(S\\)th entry is given by \\([\\mathbf{b}']_S = v(S) \\sqrt{w_S}\\). Then we can rewrite the regression problem as \\[\n\\begin{align}\n\\boldsymbol{\\phi}\n&= \\arg \\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^d: \\langle \\boldsymbol{\\beta}, \\mathbf{1} \\rangle = v([d])} \\| \\mathbf{A}' \\boldsymbol{\\beta} - \\mathbf{b}' \\|^2\n\\\\& = \\arg \\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^d: \\langle \\boldsymbol{\\beta}, \\mathbf{1} \\rangle = 0} \\| \\mathbf{A}' \\boldsymbol{\\beta} + \\mathbf{A}' \\mathbf{1} \\frac{v([d])}{d} - \\mathbf{b}' \\|^2 +  \\mathbf{1} \\frac{v([d])}{d}\n\\\\& = \\arg \\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^d} \\| \\mathbf{A} \\boldsymbol{\\beta} - \\mathbf{b} \\|^2 + \\mathbf{1} \\frac{v([d])}{d},\n\\end{align}\n\\] where we define \\(\\mathbf{b} = \\mathbf{b'} - \\mathbf{A' 1} \\frac{v([d])}{d}\\), we define \\(\\mathbf{A}= \\mathbf{A' P}\\), and we define \\(\\mathbf{P} = \\mathbf{I} - \\frac1{d} \\mathbf{1} \\mathbf{1}^\\top\\) as the projection matrix onto the subspace orthogonal to the constant vector \\(\\mathbf{1}\\).\nNow we can apply our favorite linear regression tools.\n\nActive Learning\nLet \\[\n\\boldsymbol{\\beta}^* = \\arg \\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^d} \\| \\mathbf{A} \\boldsymbol{\\beta} - \\mathbf{b} \\|^2.\n\\]\nThe challenge in solving this linear regression problem is similar to the challenge of directly computing the Shapley values: the input matrix and target vector are exponentially large in \\(d\\). Luckily, we can sample only a fraction of the rows, and find the best linear approximation to these.\n\n\n\nLet \\(\\boldsymbol{\\Pi} \\in \\mathbb{R}^{m \\times (2^d-2)}\\) be the sampling matrix. Each row of \\(\\boldsymbol{\\Pi}\\) corresponds to a sample. In the \\(j\\)th row, we have \\[\n[\\boldsymbol{\\Pi}]_{j,S} = \\begin{cases}\n\\frac1{\\sqrt{p_S}} & \\text{ if } S_j = S \\\\\n0 & \\text{ else}.\n\\end{cases}\n\\] The least squares solution on the sampled problem is given by: \\[\n\\tilde{\\boldsymbol{\\beta}}=\\arg \\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^d}\n\\| \\boldsymbol{\\Pi} \\mathbf{A} \\boldsymbol{\\beta} - \\boldsymbol{\\Pi} \\mathbf{b} \\|_2\n= \\left( \\mathbf{A}^\\top \\boldsymbol{\\Pi}^\\top \\boldsymbol{\\Pi} \\mathbf{A} \\right)^{+} \\mathbf{A}^\\top \\boldsymbol{\\Pi}^\\top \\boldsymbol{\\Pi} \\mathbf{b}\n\\] Notice that we reweight the sampling matrix so that \\(\\mathbf{A}^\\top \\boldsymbol{\\Pi}^\\top \\boldsymbol{\\Pi} \\mathbf{A}\\) and \\(\\mathbf{A}^\\top \\boldsymbol{\\Pi}^\\top \\boldsymbol{\\Pi} \\mathbf{b}\\) are correct in expectation; that is, \\(\\mathbb{E}[ \\boldsymbol{\\Pi}^\\top \\boldsymbol{\\Pi}] = \\mathbf{I}\\). Because of the correlation between samples, this doesn’t actually mean that the model is correct in expectation; that is, \\(\\mathbb{E}[ \\tilde{\\boldsymbol{\\beta}}] \\neq \\boldsymbol{\\beta}^*\\).\nUnlike in most of the problems we’ve seen where sampled points are given to us in a dataset, we get to choose the sampled subsets that we use to solve the regression problem.\n\n\nLeverage Scores\nA particularly powerful tool in active learning is the use of leverage scores. Leverage scores quantify how useful a sample is for fitting the linear regression model. For samples with high leverage, excluding them can deteriorate the quality of the learned model.\n\n\n\nThe idea is to measure how “alone” a fitted sample value could be relative to the other samples. Mathematically, the leverage of a sample \\(S\\) is given by \\[\n\\ell_S = \\max_{\\boldsymbol{\\beta} \\in \\mathbb{R}^d}\n\\frac{([\\boldsymbol{A \\beta}]_S)^2}{\\| \\boldsymbol{A \\beta} \\|_2^2}.\n\\] Notice that \\(\\ell_S\\) is defined independently of the target vector \\(\\mathbf{b}\\): It measures, over all possible learned vectors \\(\\boldsymbol{\\beta}\\), what’s the largest the entry \\([\\boldsymbol{A \\beta}]_S\\) can be relative to the overall norm of the fitted vector \\(\\boldsymbol{A \\beta}\\).\nWhen we sample according to leverage scores, we can get strong guarantees on the performance of our learned function. Roughly, with \\(m=O(d \\log(d) + \\frac{d}{\\epsilon^2})\\) samples, the solution to the approximate regression problem \\(\\boldsymbol{\\beta}\\) satisfies, with high probability, \\[\n\\| \\boldsymbol{A} \\tilde{\\boldsymbol{\\beta}} - \\boldsymbol{b} \\|_2^2 \\leq\n(1+\\epsilon) \\| \\boldsymbol{A} \\boldsymbol{\\beta}^* - \\boldsymbol{b} \\|_2^2.\n\\] That is, the fit of the learned model is close to the fit of the optimal model. We unfortunately won’t cover the proofs of these results here, but the key technical tool is applying matrix concentration inequalities.\n\n\nRegression Adjustment\nOnce we have the learned model \\(\\tilde{\\boldsymbol{\\beta}}\\), we can simply return our estimated Shapley values \\(\\tilde{\\boldsymbol{\\phi}} = \\tilde{\\boldsymbol{\\beta}} + \\mathbf{1} \\frac{v([d])}{d}\\) as our best guess for the Shapley values. While this is generally quite accurate, our linear regression estimates are not correct in expectation (because of the correlated samples when we built the \\(\\boldsymbol{\\beta}\\)).\nIf we wanted to get unbiased estimates with the same accuracy, we could use our estimates to reduce the variance of an unbiased estimator like Monte Carlo or Maximum Sample Reuse. Consider an approximation \\(\\tilde{v}: 2^{[d]} \\to \\mathbb{R}\\) defined by \\[\n\\tilde{v}(S) = \\sum_{i \\in S} \\tilde{\\phi}_i.\n\\] Let \\(\\phi_i(v)\\) be the Shapley value of the \\(i\\)th data point with respect to the function \\(v\\). Since the Shapley values are linear, we can write \\[\n\\begin{align}\n\\phi_i (v) &=\n\\sum_{S \\subseteq [d] \\setminus \\{i\\}} \\frac{v(S \\cup \\{i\\}) - v(S)}{d \\binom{d-1}{|S|}}\n\\\\&= \\sum_{S \\subseteq [d] \\setminus \\{i\\}} \\frac{v(S \\cup \\{i\\}) - v(S) + \\tilde{v}(S \\cup \\{i\\}) - \\tilde{v}(S) - \\tilde{v}(S \\cup \\{i\\}) + \\tilde{v}(S)}{d \\binom{d-1}{|S|}}\n\\\\&= \\sum_{S \\subseteq [d] \\setminus \\{i\\}} \\frac{\\tilde{v}(S \\cup \\{i\\}) - \\tilde{v}(S)}{d \\binom{d-1}{|S|}}\n+ \\sum_{S \\subseteq [d] \\setminus \\{i\\}} \\frac{v(S \\cup \\{i\\}) - v(S) - \\tilde{v}(S \\cup \\{i\\}) + \\tilde{v}(S)}{d \\binom{d-1}{|S|}}\n\\\\&= \\phi_i(\\tilde{v}) + \\phi_i(v - \\tilde{v}).\n\\end{align}\n\\] When \\(\\tilde{v}\\) is a linear function, we can simply read off the Shapley values. It remains to estimate the Shapley values of the residual \\(\\phi_i(v - \\tilde{v})\\), for which we can use any estimator.\nRecall that the Maximum Sample Reuse estimator is both unbiased and sample efficient. The one drawback was that the variance depended on \\([v(S)]^2\\). However, when we use the estimator on the residual \\(v - \\tilde{v}\\), the variance will depend on \\([v(S) - \\tilde{v}(S)]^2\\) instead. As long as \\(\\tilde{v}\\) is a good approximation of \\(v\\), this can lead to much lower variance estimates.\n\n\nA Data Perspective\nWe motivated Shapley values as a way to understand the predictions of models, but their definition and the estimators we discussed can be applied to any function \\(v\\). One particularly relevant choice for \\(v(S)\\) is the quality of a model when trained only on data points in \\(S\\). (Typically, as you may expect, we measure quality via loss on a fixed testing set.) Just like when teasing apart the contributions of individual features, the value of a data point will change in importance depending on the context of the other data points in the training set. With this particular \\(v\\), the Shapley value \\(\\phi_i\\) tells us how valuable the \\(i\\)th data point is to the model performance. As data becomes increasingly important, understanding the contributions of individual data points can help us make more informed decisions about data collection, labeling, and model training. For example, we could use the Shapley values of this data function as a mechanism for assigning value to data owners such as newspapers or artists who created the data."
  },
  {
    "objectID": "notes/LinearAlgebra.html",
    "href": "notes/LinearAlgebra.html",
    "title": "Linear Algebra Review",
    "section": "",
    "text": "When I first heard about machine learning, I imagined a computer that was rewarded every time it gave the right answer. Maybe there were electric carrots and sticks that no one had bothered to tell me about? While I now know as little as I did then about computer hardware, I have learned that machine learning is fundamentally a mathematical process.\nLuckily, we’ve been learning about the very mathematical ideas that make machine learning work for years! We’ll review the basics of these concepts here.\n\nDerivatives\nConsider a function \\({f}: \\mathbb{R} \\to \\mathbb{R}\\). The mapping notation means that \\({f}\\) takes a single real number as input and outputs a single real number. In general, mathematicians tell us to be careful about whether we can differentiate a function. But, we’re computer scientists so we’ll risk it for the biscuit.\nLet \\(x \\in \\mathbb{R}\\) be the input to \\({f}\\). The derivative of \\({f}\\) with respect to its input \\(x\\) is mathematically denoted by \\(f'(x) = \\frac{d}{d x}[{f}(x)]\\).\nFormally, the derivative is defined as \\[\n\\frac{d}{d x}[{f}(x)]\n= \\lim_{h \\to 0} \\frac{{f}(x + h) - {f}(x)}{h}.\n\\] If we were to plot \\({f}\\), the derivative at a point \\(x\\) would be the slope of the tangent line to the curve at that point.\nHere are several examples of functions and their derivatives that you might remember from calculus.\n\n\n\nFunction: \\({f}(x)\\) \n\n\nDerivative: \\(\\frac{\\partial}{\\partial x}[{f}(x)]\\)\n\n\n\n\n\\[x^2\\]\n\n\n\\[2x\\]\n\n\n\n\n\\[x^a\\]\n\n\n\\[a x^{a-1}\\]\n\n\n\n\n\\[ax + b\\]\n\n\n\\[a\\]\n\n\n\n\n\\[\\ln(x)\\]\n\n\n\\[\\frac{1}{x}\\]\n\n\n\n\n\\[e^x\\]\n\n\n\\[e^x\\]\n\n\n\n\n\nChain Rule and Product Rule\nWhile working with a simple basic function is easy, we’re not always so lucky. Modern machine learning chains many many complicated functions together. Fortunately, we will think of these operations modularly.\nLet \\(g: \\mathbb{R} \\to \\mathbb{R}\\) be another function. Consider the composite function \\(g({f}(x))\\).\nBy the chain rule, the derivative of \\(g({f}(x))\\) with respect to \\(x\\) is \\[\n\\frac{d}{d x}[g({f}(x))]\n= g'(f(x)) f'(x).\n\\]\nOften, we will also multiply functions together. The product rule tells us that \\[\n\\frac{d}{d x}[g(x) {f}(x)]\n= g(x) \\frac{d}{d x}[{f}(x)]\n+ {f}(x) \\frac{d}{d x}[g(x)].\n\\]\n\n\nGradients\nIn machine learning, we process high-dimensional data so we are interested in functions with multivariate input. Consider \\({f}: \\mathbb{R}^d \\to \\mathbb{R}\\). The output of the function is still a real number but the input consists of \\(d\\) real numbers. We will use the vector \\(\\mathbf{x} \\in \\mathbb{R}^d\\) to represent all \\(d\\) inputs \\(x_1, \\ldots, x_d\\).\nInstead of the derivative, we will use the partial derivative. The partial derivative with respect to \\(x_i\\) is denoted by \\(\\frac{\\partial}{\\partial x_i}[{f}(\\mathbf{x})]\\). In effect, the partial derivative tells us how \\({f}\\) changes when we change \\(x_i\\), while keeping all other inputs fixed.\nThe gradient stores all the partial derivatives in a vector. The \\(i\\)th entry of this vector is given by the partial derivative of \\({f}\\) with respect to \\(x_i\\). In mathematical notation, \\[\n\\nabla_\\mathbf{x} {f} = \\left[\\begin{smallmatrix} \\frac{\\partial}{\\partial x_1}[{f}(\\mathbf{x})] \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial x_d}[{f}(\\mathbf{x})] \\\\ \\end{smallmatrix}\\right]\n\\]\nJust like the derivative in one dimension, the gradient contains information about the slope of \\({f}\\) with respect to each of the \\(d\\) dimensions in its input.\n\n\nVector and Matrix Multiplication\nVector and matrix multiplication lives at the heart of machine learning. In fact, neural networks only really started to take off when researchers realized that the Graphics Processing Unit (GPU) could be used to perform gradient updates in addition to the matrix multiplication it was designed to do for gaming.\nConsider two vectors \\(\\mathbf{u} \\in \\mathbb{R}^d\\) and \\(\\mathbf{v} \\in \\mathbb{R}^d\\). We will use \\(\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^d u_i v_i\\) to denote the inner product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). We can also write the inner product as \\(\\mathbf{u}^\\top \\mathbf{v}\\), where \\(\\mathbf{u}^\\top \\in \\mathbb{R}^{1\\times d}\\) is the transpose of \\(\\mathbf{u}\\). The \\(\\ell_2\\)-norm of \\(\\mathbf{v}\\) is given by \\(\\|\\mathbf{v}\\|_2 = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}}\\).\nConsider two matrices: \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times m}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{m \\times n}\\) where \\(d \\neq n\\). We can only multiply two matrices when their inner dimension agrees. Because the number of columns in \\(\\mathbf{A}\\) is the same as the number of rows in \\(\\mathbf{B}\\), we can compute \\(\\mathbf{AB}\\). However, because the number of columns in \\(\\mathbf{B}\\) is not the same as the number of rows in \\(\\mathbf{A}\\), the product \\(\\mathbf{BA}\\) is not defined.\nWhen we can multiply two matrices, the \\((i,j)\\) entry in \\(\\mathbf{AB}\\) is given by the inner product between the \\(i\\)th row of \\(\\mathbf{A}\\) and the \\(j\\)th column of \\(\\mathbf{B}\\). In mathematical notation, we write \\[\n[\\mathbf{AB}]_{i,j} = \\sum_{k=1}^m [\\mathbf{A}]_{i,k} [\\mathbf{B}]_{k,j}.\n\\] The resulting dimensions of the matrix product will be the number of rows in \\(\\mathbf{A}\\) by the number of columns in \\(\\mathbf{B}\\).\n\n\n\nBeyond this inner product matrix multiplication that we are used to from linear algebra, there is also an outer product way to multiply two matrices. Consider the index \\(k \\in \\{1, \\ldots, m\\}\\). Let \\([\\mathbf{A}]_{,k}\\) denote the \\(k\\)th column of \\(\\mathbf{A}\\) and \\([\\mathbf{B}]_{k,}\\) denote the \\(k\\)th row of \\(\\mathbf{B}\\). We can write the matrix product as a sum of outer products for each \\(k\\): \\[\n\\mathbf{AB} = \\sum_{k=1}^m [\\mathbf{A}]_{,k} [\\mathbf{B}]_{k,}.\n\\] By checking the \\((i,j)\\) entry of the outer product, we see that the outer product definition is equivalent to the inner product matrix multiplication. We will soon see that this outer product definition is a surprisingly useful way to think about matrix multiplication.\n\n\n\n\n\nEigenvalues and Eigenvectors\nThe primary challenge when dealing with matrices is that our favorite operations on scalar real numbers do not always work on matrices. For example, it’s not obvious how to divide one matrix by another, or how to take the square root of a matrix. Eigenvalues and eigenvectors provide a way to decompose matrices into simpler components that we can work with in more familiar ways.\nConsider a square and symmetric matrix \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times d}\\). As we may remember from linear algebra, an eigenvector of \\(\\mathbf{A}\\) is a vector \\(\\mathbf{v} \\in \\mathbb{R}^d\\) such that \\(\\mathbf{Av} = \\lambda \\mathbf{v}\\) for some scalar \\(\\lambda\\). Let \\(r\\) be the rank, i.e., the number of eigenvectors with nonzero eigenvalues (counting multiplicity). Then, we can write the eigenvectors as \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_r\\), with corresponding eigenvalues \\(\\lambda_1 \\geq \\ldots \\geq \\lambda_r\\). The eigenvectors are orthonormal, meaning that \\(\\mathbf{v}_i \\cdot \\mathbf{v}_j = 0\\) for \\(i \\neq j\\) and \\(\\|\\mathbf{v}_i\\|_2 = 1\\) for all \\(i\\).\nGiven these properties, we can write \\[\\mathbf{A} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^\\top\\] where \\(\\mathbf{V} = [\\mathbf{v}_1, \\ldots, \\mathbf{v}_r] \\in \\mathbb{R}^{d \\times r}\\) is the matrix of eigenvectors and \\(\\mathbf{\\Lambda} = \\text{diag}(\\lambda_1, \\ldots, \\lambda_r) \\in \\mathbb{R}^{r \\times r}\\) is the diagonal matrix of eigenvalues. This decomposition is known as the eigenvalue decomposition of \\(\\mathbf{A}\\).\n\n\n\nUsing our outer product definition of matrix multiplication, we can equivalently write the eigenvalue decomposition as \\[\\mathbf{A} = \\sum_{i=1}^r \\lambda_i \\mathbf{v}_i \\mathbf{v}_i^\\top\\] where \\(\\mathbf{v}_i \\mathbf{v}_i^\\top\\) is the outer product of the eigenvector \\(\\mathbf{v}_i\\) with itself. We can check that the eigenvalue decomposition is correct by multiplying by each eigenvector. Since the eigenvectors are orthonormal, we have \\(\\mathbf{A} \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i\\) for each \\(i\\).\n\n\nMatrix Inversion\nLet’s see the power of the eigenvalue decomposition in the context of matrix inversion. If we have a scalar equation \\(ax = b\\), we can simply solve for \\(x\\) by dividing both sides by \\(a\\). In effect, we are applying the inverse of \\(a\\) to \\(a\\) i.e., \\((a)^{-1} a =1\\). The same principle applies to matrices. The \\(d \\times d\\) identity matrix generalizes the scalar identity \\(1\\). This identity matrix is denoted by \\(\\mathbf{I}_{d \\times d} \\in \\mathbb{R}^{d \\times d}\\): the on-diagonal entries \\((i,i)\\) are 1 while the off-diagonal entries \\((i,j)\\) for \\(i\\neq j\\) are 0.\nFor a matrix \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times d}\\), we use \\(\\mathbf{A}^{-1}\\) to denote its inverse. (When \\(\\mathbf{A}\\) does not have full rank, i.e., \\(r &lt; d\\), the inverse is not defined, but we can still use the eigenvalue decomposition to find a pseudo-inverse.) The inverse is defined so that \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{d \\times d}\\) where \\(\\mathbf{I}_{d \\times d}\\) is the identity matrix. In terms of our eigenvalue decomposition, the inverse of \\(\\mathbf{A}\\) is given by \\[\\mathbf{A}^{-1} = \\mathbf{V} \\mathbf{\\Lambda}^{-1} \\mathbf{V}^\\top\\] where \\(\\mathbf{\\Lambda}^{-1} = \\text{diag}(\\frac{1}{\\lambda_1}, \\ldots, \\frac{1}{\\lambda_r})\\) is the diagonal matrix of the inverses of the eigenvalues. To see this, note that: \\[\n\\mathbf{A}^{-1} \\mathbf{A} = \\left(\\mathbf{V} \\mathbf{\\Lambda}^{-1} \\mathbf{V}^\\top\\right) \\left(\\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^\\top\\right)\n= \\mathbf{V} \\mathbf{\\Lambda}^{-1} \\mathbf{\\Lambda} \\mathbf{V}^\\top\n= \\mathbf{V} \\mathbf{I}_{r \\times r} \\mathbf{V}^\\top\n= \\mathbf{V} \\mathbf{V}^\\top\n= \\mathbf{I}_{d \\times d},\n\\] where we repeatedly used the fact that \\(\\mathbf{V}^\\top \\mathbf{V} = \\mathbf{I}_{r \\times r}\\) because the eigenvectors are orthonormal. The last equality requires that the matrix is full rank, i.e., \\(r=d\\). If \\(r &lt; d\\), the inverse doesn’t exist; instead, the Moore–Penrose pseudoinverse is \\(\\mathbf{A}^{+} = \\mathbf{V}\\,\\mathbf{\\Lambda}^{+}\\mathbf{V}^\\top\\) with \\(\\mathbf{\\Lambda}^{+} = \\mathrm{diag}\\!\\left(\\tfrac{1}{\\lambda_1}, \\ldots, \\tfrac{1}{\\lambda_r}\\right).\\)\nIn our outer product form, we can write the inverse as \\[\\mathbf{A}^{-1} = \\sum_{i=1}^r \\frac{1}{\\lambda_i} \\mathbf{v}_i \\mathbf{v}_i^\\top\\] where \\(\\mathbf{v}_i\\) are the eigenvectors of \\(\\mathbf{A}\\). Can you more easily see why \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{d \\times d}\\)?\nGoing forward, we will apply the linear algebra tools we learned today to understand machine learning algorithms. The first algorithm we will study is PageRank, which, at its foundation, simply uses the eigenvalue decomposition to find the most important pages on the internet."
  },
  {
    "objectID": "notes/Probability.html",
    "href": "notes/Probability.html",
    "title": "Probability",
    "section": "",
    "text": "Instead of predicting a continuous value, there are many applications where we want to predict a discrete value. For example, we might want to predict whether an email is spam or not, whether a patient has a disease or not, or whether an image contains a cat or not. In these cases, we can use a classification model instead of a regression model.\nWe will review some basic concepts of probability that are useful for understanding classification models. We will also introduce Bayes’ Rule, and see how useful it can be for making predictions.\nLet \\(A\\) and \\(B\\) be two events. For example, \\(A\\) could be the event that it rains tomorrow, and \\(B\\) could be the event that I carry an umbrella tomorrow. We will use the following notation:\n- \\(\\Pr(A)\\) represents the probability that event \\(A\\) occurs,\n- \\(\\Pr(A \\cup B)\\) represents the probability that either event \\(A\\) or event \\(B\\) occurs,\n- \\(\\Pr(A \\cap B)\\) represents the probability that both events \\(A\\) and \\(B\\) occur,\n- \\(\\Pr(A | B)\\) represents the probability that event \\(A\\) occurs given that event \\(B\\) has occurred.\n\n\n\nWe can reason through several properties of probabilities:\nProbability Range The probability of an event is always between 0 and 1, i.e., \\(0 \\leq \\Pr(A) \\leq 1\\) for all events \\(A\\). An event with probability 0 never occurs, while an event with probability 1 is certain to occur.\nConditional Probabilities We can write the probability of both events \\(A\\) and \\(B\\) occurring in terms of conditional probabilities: \\[\\Pr(A \\cap B) = \\Pr(B) \\Pr(A | B)  = \\Pr(A) \\Pr(B | A).\\] This means that the probability of both events occurring is the product of the probability of one event and the conditional probability of the other event given that the first event has occurred.\nUnion of Events The probability that either event \\(A\\) or event \\(B\\) occurs is given by: \\[\\Pr(A \\cup B) = \\Pr(A) + \\Pr(B) - \\Pr(A \\cap B).\\] This means that the probability of either event occurring is the sum of the probabilities of each event minus the probability of both events occurring together (we subtract the event that both occur because it is counted twice in the sum).\nComplement Rule The complement of an event \\(A\\) is the event that \\(A\\) does not occur, denoted as \\(\\neg A\\). Since either \\(A\\) occurs or \\(\\neg A\\) occurs, we have that \\(Pr(A) + \\Pr(\\neg A) = 1\\). Rearranging this gives us that \\(\\Pr(\\neg A) = 1 - \\Pr(A)\\).\nIndependence Two events \\(A\\) and \\(B\\) are independent if the occurrence of one event does not affect the probability of the other event occurring. In this case, we have that \\(\\Pr(A \\cap B) = \\Pr(A) \\Pr(B)\\). Equivalently, \\(\\Pr(A | B) = \\Pr(A)\\) and \\(\\Pr(B | A) = \\Pr(B)\\), do you see why this follows?\nWe will often model random events with random variables. A random variable is a function that maps outcomes to real numbers. For example, we could define a random variable \\(X\\) that takes the outcome of a die roll and maps it to the number on the die. The probability distribution of a random variable describes the probabilities of each possible outcome. In our die example, if we roll a fair six-sided die, the probability distribution of the random variable \\(X\\) is given by \\(\\Pr(X = x) = \\frac{1}{6}\\) for \\(x \\in \\{1, 2, 3, 4, 5, 6\\}\\).\nBayes’ Rule is a particularly useful rule in machine learning: For any two events \\(A\\) and \\(B\\) with \\(\\Pr(B) &gt; 0\\), \\[\\Pr(A | B) = \\frac{\\Pr(B | A) \\Pr(A)}{\\Pr(B)}.\\]\nBased on what we have seen so far, can you prove Bayes’ Rule?\n\nMaximum A Posteriori (MAP) Estimation\nWhen we justified the mean squared error loss for regression, we said that it was the maximum likelihood estimate (MLE) of the parameters of a linear model. We can extend this idea to classification models as well. Suppose we have a binary classification problem, where we want to predict whether the random variable \\(Y=0\\) or \\(Y=1\\). We observe evidence \\(\\mathbf{X} = \\mathbf{x}\\), e.g., \\(\\mathbf{X}\\) is the random variable that takes on the value of the features of an email, and we want to predict whether the email is spam or not. We will compare the posteriors \\[\\Pr(Y = 1 | \\mathbf{X} = \\mathbf{x})\\] and \\[\\Pr(Y = 0 | \\mathbf{X} = \\mathbf{x})\\] to determine which class is more likely given the evidence.\nHowever, it’s not immediately clear how to compute these probabilities. Luckily, we can use Bayes’ Rule to rewrite the posteriors. Without loss of generality, consider the event that \\(Y = 1\\). Then \\[\n\\begin{align*}\n\\Pr(Y = 1 | \\mathbf{X} = \\mathbf{x})\n&= \\frac{\\Pr(\\mathbf{X} = \\mathbf{x} | Y = 1) \\Pr(Y = 1)}{\\Pr(\\mathbf{X} = \\mathbf{x})} \\\\\n&= \\frac{\\textnormal{likelihood} \\cdot \\textnormal{prior}}{\\textnormal{evidence}}\n\\end{align*}.\n\\]\nLet’s get some familiarity through a medical example. Suppose we have a medical test that can detect whether a patient has a particular disease. The disease is rare, affecting only 1% of the population. The test is unfortunately not perfect: it has a false positive rate of 5% and a false negative rate of 10%. Suppose \\(X=1\\) i.e., the test is positive. Is it more likely that the patient has the disease or not?\nIn this medical example, we were explicitly given the likelihood of the disease. What can we do when our only information is the labelled data?\n\n\nNaive Bayes Classifier\nThe Naive Bayes Classifier is a simple yet effective classification algorithm that uses Bayes’ Rule to make predictions. The key assumption of the Naive Bayes Classifier is that the features are conditionally independent given the class label. This means that the presence or absence of a feature does not affect the presence or absence of another feature, given the class label.\nLet’s see an example with a spam classifier. Suppose we have a dataset of emails, each labelled as either spam or not spam. We want to predict whether a new email is spam or not based on its features, such as the presence of certain words. In particular, let \\(\\mathbf{X} = (X_1, X_2, \\ldots, X_d)\\) be the features of the email, where \\(X_i\\) is a binary variable indicating whether the \\(i\\)th word is present in the email. Let \\(p_i^{(1)} = \\Pr(X_i=1 | Y=1)\\) be the probability that the \\(i\\)th word is present in a spam email, while \\(p_i^{(0)} = \\Pr(X_i=1 | Y=0)\\) be the probability that the \\(i\\)th word is present in a non-spam email. We will use the independence assumption to compute the likelihood of the features given the class label. For example, \\[\n\\Pr(\\mathbf{X} = (0, 1, 0, 0, 1) | Y = 1) =\n(1-p_1^{(1)}) p_2^{(1)} (1-p_3^{(1)}) (1-p_4^{(1)}) p_5^{(1)}.\n\\]\nBeyond the likelihood, we also need the prior probability of the class label. This is even easier to compute, e.g., we can simply compute the fraction of spam and non-spam emails in the training set. Then we can use Bayes’ Rule to determine whether the posterior probability of the email being spam is greater than the posterior probability of the email being non-spam.\nMore formally, we can use the Naive Bayes Classifier by\n\nComputing \\(\\Pr(Y = 1)\\) and \\(\\Pr(Y = 0)\\) from the training data.\nComputing the observed probabilities \\(\\Pr(X_i = 1 | Y = 1)\\) and \\(\\Pr(X_i = 1 | Y = 0)\\) for each feature \\(X_i\\) from the training data.\nComputing the likelihoods \\(\\Pr(\\mathbf{X} = \\mathbf{x} | Y=1)\\) and \\(\\Pr(\\mathbf{X} = \\mathbf{x} | Y=0)\\) using the independence assumption e.g., \\[\n\\Pr(\\mathbf{X} = \\mathbf{x} | Y=1) = \\prod_{i=1}^d \\Pr(X_i = x_i | Y=1).\n\\]\nUsing Bayes’ Rule to compute the posteriors, and predicting the class label \\(y \\in \\{0,1\\}\\) with the highest posterior probability: \\[\n\\Pr(Y = y | \\mathbf{X} = \\mathbf{x}) = \\frac{\\Pr(\\mathbf{X} = \\mathbf{x} | Y=y) \\Pr(Y=y)}{\\Pr(\\mathbf{X} = \\mathbf{x})}.\n\\]"
  },
  {
    "objectID": "notes/code.html",
    "href": "notes/code.html",
    "title": "Code for Producing Images in Lecture Notes",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport scienceplots\n\nplt.style.use('science')\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D"
  },
  {
    "objectID": "notes/code.html#linear-regression-figures",
    "href": "notes/code.html#linear-regression-figures",
    "title": "Code for Producing Images in Lecture Notes",
    "section": "Linear Regression Figures",
    "text": "Linear Regression Figures\n\nnp.random.seed(0) # Seed randomness\nplt.figure(figsize=(6, 3))\n\nn = 10 # Number of observations\nw = 2 # True parameter\nX = np.random.rand(n) # x-values\ny = X.dot(w).T + np.random.normal(size=n) * .2 #y-values\n\nplt.scatter(X,y, color='black', label=r'Data: $(x^{(i)}, y^{(i)})$')\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\nxaxis = np.arange(0,1,.01)\nplt.plot(xaxis, xaxis*.5, label=r'Line: $f(x) = .5x$', color='red')\nplt.plot(xaxis, xaxis*w, label=r'Line: $f(x) = 2x$', color='green')\nplt.legend()\nplt.title(r'Linear Regression in $\\mathbb{R}^1$')\nplt.savefig('images/regression_1d.svg')\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(6, 3))\nplt.xlabel(r'$z$')\nplt.ylabel(r'$\\mathcal{L}(z)$')\nxaxis = np.arange(-1.5,1.5,.001)\nplt.plot(xaxis, xaxis**2, label=r'Squared Loss: $\\mathcal{L}(z)=z^2$', color='blue')\nplt.plot(xaxis, np.abs(xaxis), label=r'Absolute Loss: $\\mathcal{L}(z)=|z|$', color='purple', linestyle='dotted')\nplt.legend()\nplt.title(r'Squared and Absolute Losses')\nplt.savefig('images/regression_losses.svg')\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\n\n# Seed randomness\nnp.random.seed(1234)\nn = 10  # Number of observations\nw = np.array([2, .5])  # True parameter\nX = np.random.rand(n, 2)  # x-values\ny = X.dot(w).T + np.random.normal(size=n) * .1  # y-values\n\n# Create figure and 3D axis\nfig = plt.figure(figsize=(5, 5))\nax = fig.add_subplot(111, projection='3d')\n\n# Scatter plot for data points\nax.scatter(X[:, 0], X[:, 1], y, color='black', label=r'Data: $(x_1^{(i)}, x_2^{(i)}, y^{(i)})$')\n\n# Hyperplane 1: Green\nx1 = np.arange(0, 1, .01)\nx2 = np.arange(0, 1, .01)\nX1, X2 = np.meshgrid(x1, x2)\nZ = w[0] * X1 + w[1] * X2\nax.plot_surface(X1, X2, Z, alpha=.5, color='green')\n\n# Hyperplane 2: Red\nax.plot_surface(X1, X2, .5 * X1 + 0 * X2, alpha=.5, color='red')\n\n# Labels and title\nax.set_xlabel(r'$x_1$')\nax.set_ylabel(r'$x_2$')\nax.set_zlabel(r'$y$')\nax.set_title(r'Linear Regression in $\\mathbb{R}^2$')\nax.grid(False)\n\n# Manually create custom legend handles for the surfaces\nhandles = [\n    Line2D([0], [0], marker='o', color='black', markerfacecolor='black', markersize=6, label=r'Data: $(x_1^{(i)}, x_2^{(i)}, y^{(i)})$'),\n    Line2D([0], [0], color='green', lw=4, label=r'Hyperplane: $f(x) = 2x_1 + .5x_2$'),\n    Line2D([0], [0], color='red', lw=4, label=r'Hyperplane: $f(x) = .5x_1 + 0x_2$')\n]\n\n# Add legend\nplt.legend(handles=handles, loc='upper left', framealpha=1)\n\n# Save the figure\nplt.savefig('images/regression_2d.svg', bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "notes/code.html#non-linear-regression-figures",
    "href": "notes/code.html#non-linear-regression-figures",
    "title": "Code for Producing Images in Lecture Notes",
    "section": "Non-linear Regression Figures",
    "text": "Non-linear Regression Figures\n\n## Gradient descent\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom matplotlib import cm\nimport torch\n\n%matplotlib inline\n\nclass QuadFunc:\n  def __init__(self, a, b, c, d, e):\n    self.a = a\n    self.b = b\n    self.c = c\n    self.d = d\n    self.e = e\n\n  def getParams(self, x, y):\n    if y is None:\n      y = x[1]\n      x = x[0]\n    return x,y\n\n  def __call__(self, x, y=None):\n    x,y = self.getParams(x,y)\n    return 0.5 * (self.a*x**2 + self.b*y**2) + self.c * x * y + self.d * x + self.e * y\n\n  def grad(self, x, y=None):\n    #df/dx = ax + cy + d\n    #df/dy = by + cx + e\n    x,y = self.getParams(x,y)\n    return torch.tensor([self.a * x + self.c * y + self.d, self.b * y + self.c * x + self.e])\n\n  def hess(self, x, y=None):\n    #d2f/dx2 = a\n    #d2f/dy2 = b\n    #d2f/dxdy = c\n    #d2f/dydx = c\n    x, y = self.getParams(x,y)\n    return torch.tensor([[self.a, self.c], [self.c, self.b]])\n\nclass GradientDescent:\n    def __init__(self, lr=1, b1=0.9, b2=0.999):\n        # b1 -&gt; Momentum\n        # b2 -&gt; ADAM\n        # ADAM Paper -&gt; https://arxiv.org/abs/1412.6980\n        self.lr = lr # learning rate\n        self.b1 = b1 # grad aggregation param (for Momentum)\n        self.b2 = b2 # grad^2 aggregation param (for ADAM)\n\n        self.v = 0 # grad aggregation param\n        self.w = 0 # grad^2 aggregation param\n        self.t = 0\n\n        self.eps = 1e-9\n\n    def __call__(self, grad,hess):\n\n        self.t += 1\n\n\n        # aggregation\n        self.v = self.b1*self.v + (1-self.b1)*grad\n        self.w = self.b2*self.w + (1-self.b2)*grad**2\n\n        # bias correction\n        vcorr = self.v/(1-self.b1**self.t)\n        wcorr = self.w/(1-self.b2**self.t) if self.b2 != 0 else 1\n\n        return -1*self.lr*vcorr/(wcorr**0.5 + self.eps)\n\nclass Newtons:\n    # https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization\n    def __init__(self, lr=1):\n        self.lr = lr\n\n    def __call__(self,grad,hess):\n        return -1*self.lr*torch.matmul(torch.inverse(hess), grad)\n\ndef runOptim(init,optim,func,steps):\n\n    curpos = init # current position\n    path = [curpos]\n\n\n    for _ in range(steps):\n\n        grad = func.grad(curpos)\n        hess = func.hess(curpos)\n\n        dx = optim(grad,hess)\n        curpos = curpos + dx\n        path.append(curpos)\n\n    return path\n\n\ndef showPath(func,init,paths,labels,colors,levels):\n\n    x = torch.arange(-10,10,0.05)\n    y = torch.arange(-10,10,0.05)\n\n    # create meshgrid\n    xx, yy = torch.meshgrid(x,y)\n    zz = func(xx,yy)\n\n    # create contour\n    fig, ax = plt.subplots(1,1,figsize=(10,4))\n    cp = ax.contourf(xx,yy,zz,levels)\n    fig.colorbar(cp)\n\n    # mark initial point\n    ax.plot(init[0],init[1],'ro')\n    ax.text(init[0]+0.5,init[1],'Intial Point',color='white')\n\n    # Plot paths\n    for pnum in range(len(paths)):\n        for i in range(len(paths[pnum])-1):\n            curpos = paths[pnum][i]\n            d = paths[pnum][i+1] - curpos\n            ax.arrow(curpos[0],curpos[1],d[0],d[1],color=colors[pnum],head_width=0.2)\n            ax.text(curpos[0]+d[0],curpos[1]+d[1],str(i),color='white')\n\n    # Add legend\n    legends = []\n    for col in colors:\n        legends.append(mpatches.Patch(color=col))\n    # Put legend in top left corner\n    ax.legend(legends,labels, loc='upper left')\n\n\nplt.figure(figsize=(4, 2))\na = 1/torch.sqrt(torch.tensor(2.0))\ninit = torch.matmul(torch.tensor([[a,a],[-a,a]]),torch.tensor([-5.0,7.5]))\nell = QuadFunc(a,a,-0.8*a,a,a)\nsteps = 7\nlr = 1.5\nregGD = GradientDescent(lr,0,0) # Without Momentum\nmomGD = GradientDescent(lr,0.9,0) # Momentum\npath1 = runOptim(init,regGD,ell,steps)\npath2 = runOptim(init,momGD,ell,steps)\n# Set figure size\nshowPath(ell,init,[path1,path2],['Gradient Descent','Momentum'],['r','y'], 15)\n# Turn off axis ticks\nplt.xticks([])\nplt.yticks([])\nplt.xlabel(r'$w_1$', fontsize=14)\nplt.ylabel(r'$w_2$', fontsize=14)\nplt.savefig('images/regression_momentum.svg', bbox_inches='tight', dpi=300)\n\n/opt/homebrew/Caskroom/miniconda/base/envs/rads/lib/python3.10/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:4316.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n\n\n&lt;Figure size 400x200 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Step 1: Generate noisy linear data\nn = 10 # Number of observations\nnp.random.seed(0)\nX = np.linspace(-1, 1, n).reshape(-1, 1)\nnoise = .2 * np.random.normal(0, 1, n).reshape(-1, 1)  # Gaussian noise\ny = X**2 + noise\ny -= np.mean(y)\ny = y.ravel()\n\ny_preds = {}\nweight_cache = {}\n\n# Step 2: Fit linear regression model\nweights = np.linalg.lstsq(X, y, rcond=None)[0]  # Get weights from least squares solution\ny_preds['Degree 1 (Linear)'] = X.dot(weights).ravel()  # Recompute predictions using weights\nweight_cache['Degree 1 (Linear)'] = weights\n\nfor power in [3, 11]:\n    X_powers = X ** np.arange(0, power)\n    weights = np.linalg.lstsq(X_powers, y, rcond=None)[0]\n    y_preds[f'Degree {power-1}'] = X_powers.dot(weights).ravel()\n    weight_cache[f'Degree {power-1}'] = weights    \n\n# Step 4: Plotting\nplt.figure(figsize=(5, 3))\n\nlinestyles = ['-', '--', '-.']\n\ncolors = ['#a8ddb5', '#41ab5d', '#005a32']\n\nfor idx, (label, y_pred) in enumerate(y_preds.items()):\n    plt.plot(X, y_pred, label=label, linewidth=3, alpha=0.7, zorder=1, linestyle=linestyles[idx], color=colors[idx])\n\nplt.scatter(X, y, marker='o', color='#3B5998', s=60, zorder=2)  # Deep Cornflower Blue\n\nplt.legend()\nplt.title('Polynomial Regression')\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\nplt.tight_layout()\nplt.savefig('images/regression_polynomial.svg', bbox_inches='tight', dpi=300)\n\n\nfig, ax = plt.subplots(figsize=(5, 3))\n\n# collect global min&gt;0 and max across both sets\nall_abs = np.concatenate([np.abs(w) for w in weight_cache.values()])\nmin_pos = np.min(all_abs[all_abs &gt; 0])\nbins = np.logspace(np.log10(min_pos), np.log10(all_abs.max()), 10)\n\nfor idx, (label, weights) in enumerate(weight_cache.items()):\n    print(weights)\n\n    ax.hist(np.abs(weights), bins=bins, alpha=0.7, label=label, color=colors[idx])  # density=True for shape comparison\n\nax.set_xscale('log')\nax.set_xlabel('Magnitude of Weights')\nax.set_ylabel('Frequency')\nax.set_title('Histogram of Weights')\nax.legend()\nplt.savefig('images/regression_polynomial_weights.svg', bbox_inches='tight', dpi=300)\n\n[-0.15312242]\n[-0.40540951 -0.15312242  0.99509606]\n[  -0.52284668   -3.01040003    5.89533097   38.37227831  -23.76167019\n -158.37031364   25.43098462  242.78437408   11.69445087 -119.9112841\n  -18.07379656]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Loss curves\n\nn = 100 # Number of observations\nnp.random.seed(0)\nX = np.linspace(0, 1, n).reshape(-1, 1)\nnoise = np.random.normal(0, 1, n).reshape(-1, 1)  # Gaussian noise\nX_powers = X ** np.arange(0, 20)\ny = X_powers.dot(np.random.rand(20, 1)) + noise - np.mean(noise)  # Center noise\ny = y.ravel()\n\nX_train, y_train = X[:80], y[:80]\nX_val, y_val = X[80:], y[80:]\nlr = .1\n\ndegrees = list(range(1, 11, 2))  # [1, 5, 10]\n\nweights_by_degree = {degree : np.random.rand(degree) for degree in degrees}\n\nloss_test = {degree : [] for degree in degrees}\nloss_train = {degree : [] for degree in degrees}\n\nfor epoch in range(100000):\n    for degree, weights in weights_by_degree.items():\n        X_train_powers = X_train ** np.arange(0, degree)\n        y_train_pred = X_train_powers.dot(weights).ravel()\n        train_loss = np.mean((y_train - y_train_pred) ** 2)\n        loss_train[degree].append(train_loss)\n\n        X_val_powers = X_val ** np.arange(0, degree)\n        y_val_pred = X_val_powers.dot(weights).ravel()\n        val_loss = np.mean((y_val - y_val_pred) ** 2)\n        loss_test[degree].append(val_loss)\n\n        gradient = -2 * X_train_powers.T.dot(y_train - y_train_pred) / len(y_train)\n        weights -= lr * gradient\n        weights_by_degree[degree] = weights\n\n# x positions for log-scale (start at 1, not 0)\nepochs = np.arange(1, len(next(iter(loss_train.values()))) + 1)\n\n# Different vertical scales: remove sharey\nfig, (ax_tr, ax_val) = plt.subplots(1, 2, figsize=(7, 3))\n\n# Green shades by degree (darker = larger degree)\ncmap = plt.cm.Greens\ndmin, dmax = min(degrees), max(degrees)\ndef green_for_degree(d):\n    # map degree -&gt; [0.3, 1.0] so the lightest line is still visible\n    t = 0.0 if dmax == dmin else (d - dmin) / (dmax - dmin)\n    t = 0.3 + 0.7 * t\n    return cmap(t)\n\nfor degree in degrees:\n    c = green_for_degree(degree)\n    ax_tr.plot(epochs, loss_train[degree], linestyle='--', color=c)\n    ax_val.plot(epochs, loss_test[degree], label=f'Degree {degree}', linestyle='-', color=c)\n\nfor ax, title in [(ax_tr, 'Training Loss'), (ax_val, 'Validation Loss')]:\n    ax.set_xscale('log')\n    ax.set_xlabel('Epoch')\n    ax.set_title(title)\n\nax_tr.set_ylabel('Mean Squared Error (MSE)')\n# Single legend for the whole figure, outside the plots\nfig.legend(loc='center left', bbox_to_anchor=(.85, 0.5))\n\nfig.tight_layout(rect=[0, 0, 0.85, 1])  # leave space on right for legend\nplt.savefig('images/regression_polynomial_loss.svg', bbox_inches='tight', dpi=300)"
  },
  {
    "objectID": "notes/code.html#logistic-regression",
    "href": "notes/code.html#logistic-regression",
    "title": "Code for Producing Images in Lecture Notes",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(5, 3))\nX = np.linspace(-5, 5, 1000).reshape(-1, 1)\nys = {}\n\nys['Step'] = np.where(X &gt; 0, 1, 0)  # Step function\nys['Logistic'] = 1 / (1 + np.exp(-X))  # Logistic function\n\ncolors = ['red', 'green']\nlinestyles = ['--', '-']\nfor idx, (label, y) in enumerate(ys.items()):\n    plt.plot(X, y, label=label, linewidth=3, alpha=0.7, zorder=1, linestyle=linestyles[idx], color=colors[idx])\n\nplt.legend()\nplt.title('Classification Outputs')\nplt.xlabel(r'$\\langle \\mathbf{x}, \\mathbf{w} \\rangle$')\nplt.ylabel(r'Output')\nplt.tight_layout()\nplt.savefig('images/classification_outputs.svg', bbox_inches='tight', dpi=300)\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(5, 3))\nX = np.linspace(-5, 5, 1000).reshape(-1, 1)\nys = {}\n\nys[r'$\\ell_1$'] = np.abs(X - 1)\nys[r'$\\ell_2$'] =  (X-1)**2 /4 # Squared loss\nys['Cross Entropy'] = -np.log(1 / (1 + np.exp(-X)))  # Logistic function\n\ncolors = ['red', 'blue', 'green']\nlinestyles = ['--', '-.', '-']\nfor idx, (label, y) in enumerate(ys.items()):\n    plt.plot(X, y, label=label, linewidth=3, alpha=0.7, zorder=1, linestyle=linestyles[idx], color=colors[idx])\n\nplt.legend()\nplt.title('Classification Loss')\nplt.xlabel(r'$\\langle \\mathbf{x}, \\mathbf{w} \\rangle$')\nplt.ylabel(r'Loss if $y=1$')\nplt.tight_layout()\nplt.savefig('images/classification_loss.svg', bbox_inches='tight', dpi=300)\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Set seed and generate balanced data\nnp.random.seed(42)\nn = 1000\n\n# Positive class: inside circle (radius &lt; 1.5)\npos = np.random.normal(0, 0.8, size=(n * 2, 2))\nr_pos = np.linalg.norm(pos, axis=1)\npos = pos[r_pos &lt; 1.4][:n // 2]\n\n# Negative class: just outside circle (radius &gt; 1.7)\nneg = np.random.normal(0, 1.2, size=(n * 2, 2))\nr_neg = np.linalg.norm(neg, axis=1)\nneg = neg[r_neg &gt; 1.6][:len(pos)]\n\n# Combine\nX = np.vstack([pos, neg])\ny = np.hstack([np.ones(len(pos)), np.zeros(len(neg))])\n\n# Colors\ncolors = np.array(['blue', 'red'])\nmarker_size = 10\n\n# Create figure\nfig = plt.figure(figsize=(9, 4))\n\n# --- 2D plot ---\nax1 = fig.add_subplot(1, 2, 1)\nax1.scatter(X[y == 0, 0], X[y == 0, 1], s=marker_size, c='blue', marker='^', edgecolors='none', label='Negative')\nax1.scatter(X[y == 1, 0], X[y == 1, 1], s=marker_size, c='green', marker='s', edgecolors='none', label='Positive')\n# Decision boundary\ncircle = plt.Circle((0, 0), 1.5, edgecolor='gray', fill=False, linewidth=2)\nax1.add_artist(circle)\n\n# Axis settings\nax1.set_xlim(-4, 4)\nax1.set_ylim(-4, 4)\nax1.set_aspect('equal')\n#ax1.set_title(\"Input Space\")\nax1.set_xlabel('$x_1$')\nax1.set_ylabel('$x_2$')\nax1.tick_params(left=True, bottom=True)\nfor spine in ax1.spines.values():\n    spine.set_visible(True)\n\n# --- 3D plot ---\nax2 = fig.add_subplot(1, 2, 2, projection='3d')\n\n# Feature transformation: z = - (x1^2 + x2^2)\nX3 = np.sum(X**2, axis=1)\nX_3d = np.hstack([X, X3[:, np.newaxis]])\n\nax2.scatter(X_3d[y == 0, 0], X_3d[y == 0, 1], X_3d[y == 0, 2],\n            c='blue', s=marker_size, marker='^', label='Negative', alpha=0.6)\nax2.scatter(X_3d[y == 1, 0], X_3d[y == 1, 1], X_3d[y == 1, 2],\n            c='green', s=marker_size, marker='s', label='Positive', alpha=0.6)\n\n\n# Separating plane: z = -2.25\nx1_range = np.linspace(-2, 2, 50)\nx2_range = np.linspace(-2, 2, 50)\nx1_grid, x2_grid = np.meshgrid(x1_range, x2_range)\nz_plane = 2.25 * np.ones_like(x1_grid)\n\nax2.plot_surface(x1_grid, x2_grid, z_plane, color='grey', alpha=0.5, edgecolor='none')\n\n# View and axis cleanup\nax2.view_init(elev=25, azim=135)\nax2.set_xlim(-4, 4)\nax2.set_ylim(-4, 4)\nax2.set_zlim(-20, 2)\n#ax2.set_title(\"Transformed Feature Space\")\nax2.set_xlabel(\"$x_1$\")\nax2.set_ylabel(\"$x_2$\")\nax2.set_zlabel(r\"$(x_1^2 + x_2^2)$\")\nax2.grid(False)\n\n# Adjust view and axis limits for a pointed cone effect\nax2.view_init(elev=10, azim=45)\nax2.set_xlim(-3, 3)\nax2.set_ylim(-3, 3)\nax2.set_zlim(-2, 10)  # Compress vertical scale to make cone sharper\n\n# Final layout adjustments\n# Set white background and black cube lines for 3D plot\nax2.set_facecolor('white')\nax2.xaxis.pane.set_edgecolor('black')\nax2.yaxis.pane.set_edgecolor('black')\nax2.zaxis.pane.set_edgecolor('black')\n\nplt.tight_layout()\nplt.savefig('images/classification_transformation.svg', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\n\n# 1. Generate overlapping data\nnp.random.seed(0)\nn = 200\nmean_pos = [1, 1]\nmean_neg = [-1, -1]\ncov = [[1.5, 0.5], [0.5, 1.5]]\n\nX_pos = np.random.multivariate_normal(mean_pos, cov, size=n//2)\nX_neg = np.random.multivariate_normal(mean_neg, cov, size=n//2)\nX = np.vstack((X_pos, X_neg))\ny = np.hstack((np.ones(n//2), np.zeros(n//2)))\n\n# 2. Fit logistic regression\nclf = LogisticRegression()\nclf.fit(X, y)\n\n# 3. Create meshgrid\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 400),\n                     np.linspace(y_min, y_max, 400))\ngrid = np.c_[xx.ravel(), yy.ravel()]\nprobs = clf.predict_proba(grid)[:, 1].reshape(xx.shape)\n\n# 4. Plot\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Scatter data points\nax.scatter(X[y == 1, 0], X[y == 1, 1], c='green', s=10, marker='s', label='Positive')\nax.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', s=10, marker='^', label='Negative')\n\n\n# Thresholds and colors\nthresholds = [0.3, 0.5, 0.7]\nimport matplotlib\noranges = matplotlib.colormaps.get_cmap('Oranges')\ncolors = [oranges(i) for i in [0.4, 0.65, 0.9]] \n\nfor tau, color in zip(thresholds, colors):\n    CS = ax.contour(xx, yy, probs, levels=[tau], colors=[color], linewidths=2)\n    \n    # If there's a contour segment, label it manually\n    if len(CS.allsegs[0]) &gt; 0:\n        seg = CS.allsegs[0][0]\n        if len(seg) &gt; 0:\n            # Pick the right-most point (max x)\n            x_text, y_text = seg[np.argmax(seg[:, 0])]\n            ax.text(x_text + 0.2, y_text, fr'$\\tau$={tau:.1f}',\n                    color=color, fontsize=9, ha='left', va='center',\n                    bbox=dict(boxstyle='round,pad=0.2', fc='white', ec='none', alpha=0.7))\n\n\n# Final formatting\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.legend()\nax.set_title(\"Decision Boundaries with Varying Thresholds\")\nplt.tight_layout()\nplt.savefig('images/classification_boundaries.svg', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\n# --- 1. Generate overlapping data ---\nnp.random.seed(0)\nn = 200\nmean_pos = [1, 1]\nmean_neg = [-1, -1]\ncov = [[1.5, 0.5], [0.5, 1.5]]\n\nX_pos = np.random.multivariate_normal(mean_pos, cov, size=n//2)\nX_neg = np.random.multivariate_normal(mean_neg, cov, size=n//2)\nX = np.vstack((X_pos, X_neg))\ny = np.hstack((np.ones(n//2), np.zeros(n//2)))\n\n# --- 2. Fit logistic regression ---\nclf = LogisticRegression()\nclf.fit(X, y)\nprobs = clf.predict_proba(X)[:, 1]\n\n# --- 3. Compute ROC curve and AUC ---\nfpr, tpr, thresholds = roc_curve(y, probs)\nauc = roc_auc_score(y, probs)\n\n# --- 4. Plot ROC curve ---\nplt.figure(figsize=(6, 4))\nplt.plot(fpr, tpr, label=f\"Linear Regression (AUC = {auc:.2f})\", color='steelblue', linewidth=2)\nplt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Guessing (AUC = 0.5)')\n\n# --- 5. Highlight specific τ values ---\ntau_values = [0.3, 0.5, 0.7]\nimport matplotlib\noranges = matplotlib.colormaps.get_cmap('Oranges')\ncolors = [oranges(i) for i in [0.4, 0.65, 0.9]] \n\nfor tau, color in zip(tau_values, colors):\n    # Find closest index in thresholds array\n    idx = np.argmin(np.abs(thresholds - tau))\n    plt.scatter(fpr[idx], tpr[idx], color=color, edgecolor='black', zorder=5)\n    plt.text(fpr[idx]+0.02, tpr[idx]-0.02, fr'$\\tau$={tau}', fontsize=9, color=color, verticalalignment='center')\n\n# --- 6. Format plot ---\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend()\nplt.tight_layout()\n\nplt.savefig('images/classification_roc.svg', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "notes/code.html#support-vector-machines",
    "href": "notes/code.html#support-vector-machines",
    "title": "Code for Producing Images in Lecture Notes",
    "section": "Support Vector Machines",
    "text": "Support Vector Machines\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib\n\n# ==== 1. Generate *linearly separable* data ====\nseed = 42        # change seed to get different random draws\nnp.random.seed(seed)\n\nn = 30\nmean_pos = [ 1,  1]\nmean_neg = [-1, -1]\ncov = [[0.4, 0.0],   # tight, nearly spherical clusters =&gt; separable\n       [0.0, 0.4]]\n\nX_pos = np.random.multivariate_normal(mean_pos, cov, size=n//2)\nX_neg = np.random.multivariate_normal(mean_neg, cov, size=n//2)\nX = np.vstack((X_pos, X_neg))\ny = np.hstack((np.ones(n//2), np.zeros(n//2)))\n\n# Stretch x so data width : height ≈ 6 : 4  (i.e., multiply by 1.5)\nstretch = 6 / 3  # 1.5\nX[:, 0] *= stretch\n\n# ==== 2. Fit logistic regression ====\nclf = LogisticRegression(max_iter=1000)\nclf.fit(X, y)\n\n# ==== 3. Create meshgrid ====\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 400),\n                     np.linspace(y_min, y_max, 400))\ngrid = np.c_[xx.ravel(), yy.ravel()]\nprobs = clf.predict_proba(grid)[:, 1].reshape(xx.shape)\n\n# ==== 4. Plot ====\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Scatter data points — match your style\nax.scatter(X[y == 1, 0], X[y == 1, 1],\n           c='green', marker='s', label='Positive')\nax.scatter(X[y == 0, 0], X[y == 0, 1],\n           c='blue', marker='^', label='Negative')\n\n# Final formatting\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\n#ax.legend()\n#ax.set_title(\"Linearly Separable Data\")\nax.set_aspect('equal')\nplt.tight_layout()\n\n# Save & show\nplt.savefig('images/svm_data.svg', dpi=300, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\n\n# ==== 1. Generate *linearly separable* data ====\nseed = 42\nnp.random.seed(seed)\n\nn = 30\nmean_pos = [1, 1]\nmean_neg = [-1, -1]\ncov = [[0.4, 0.0], [0.0, 0.4]]\n\nX_pos = np.random.multivariate_normal(mean_pos, cov, size=n//2)\nX_neg = np.random.multivariate_normal(mean_neg, cov, size=n//2)\nX = np.vstack((X_pos, X_neg))\ny = np.hstack((np.ones(n//2), np.zeros(n//2)))\n\n# Stretch x so data width : height ≈ 6 : 4  (i.e., multiply by 1.5)\nstretch = 6 / 3  # 1.5\nX[:, 0] *= stretch\n\n# ==== 2. Fit SVM with linear kernel ====\nclf = SVC(kernel='linear', C=1e5)  # large C for hard-margin behavior\nclf.fit(X, y)\n\n# ==== 3. Create meshgrid for plotting (optional) ====\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 400),\n                     np.linspace(y_min, y_max, 400))\n\n# ==== 4. Plot ====\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Scatter data points\nax.scatter(X[y == 1, 0], X[y == 1, 1], c='green', marker='s', label='Positive')\nax.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', marker='^', label='Negative')\n\n# ==== 5. Add three different (non-parallel) separating hyperplanes ====\ndef plot_separator(ax, slope, intercept, style, color, label):\n    x_vals = np.linspace(x_min, x_max, 200)\n    y_vals = slope * x_vals + intercept\n    ax.plot(x_vals, y_vals, style, color=color, linewidth=1.8, label=label)\n\n# Line 1: SVM decision boundary (maximum margin)\nw = clf.coef_[0]\nintercept_svm = clf.intercept_[0]\nslope_svm = -w[0] / w[1]\nintercept_line = -intercept_svm / w[1]\nplot_separator(ax, slope_svm, intercept_line, '-', 'black', 'Separator 1 (SVM)')\n\n# Line 2: custom separator with small margin, different slope\nplot_separator(ax, -0.4, 0, '--', 'black', 'Separator 2 (Small Margin)')\n\n# Line 3: another custom separator with small margin, different slope\nplot_separator(ax, -1.8, 0.2, '-.', 'black', 'Separator 3 (Small Margin)')\n\n# ==== Final formatting ====\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_aspect('equal')\nplt.tight_layout()\n\n# Save and show\nplt.savefig('images/svm_possible_lines.svg', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\n\n# ==== 1. Generate *linearly separable* data ====\nseed = 42\nnp.random.seed(seed)\n\nn = 30\nmean_pos = [1, 1]\nmean_neg = [-1, -1]\ncov = [[0.4, 0.0], [0.0, 0.4]]\n\n\nX_pos = np.random.multivariate_normal(mean_pos, cov, size=n//2)\nX_neg = np.random.multivariate_normal(mean_neg, cov, size=n//2)\nX = np.vstack((X_pos, X_neg))\ny = np.hstack((np.ones(n//2), np.zeros(n//2)))\n\n# Stretch x so data width : height ≈ 6 : 4  (i.e., multiply by 1.5)\nstretch = 6 / 3  # 1.5\nX[:, 0] *= stretch\n\n# ==== 2. Fit SVM with linear kernel ====\nclf = SVC(kernel='linear', C=1e5)  # large C for hard-margin behavior\nclf.fit(X, y)\n\n# ==== 3. Create meshgrid for plotting (optional) ====\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 400),\n                     np.linspace(y_min, y_max, 400))\n\n# ==== 4. Plot ====\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Scatter data points\nax.scatter(X[y == 1, 0], X[y == 1, 1], c='green', marker='s')\nax.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', marker='^')\n\n# ==== 5. Add three different (non-parallel) separating hyperplanes ====\ndef plot_separator(ax, slope, intercept, style, color, label=\"\"):\n    x_vals = np.linspace(x_min, x_max, 200)\n    y_vals = slope * x_vals + intercept\n    ax.plot(x_vals, y_vals, style, color=color, linewidth=1.8, label=label)\n\n# Line 1: SVM decision boundary (maximum margin)\nw = clf.coef_[0]\nintercept_svm = clf.intercept_[0]\nslope_svm = -w[0] / w[1]\nintercept_line = -intercept_svm / w[1]\nmargin = 1 / np.linalg.norm(w)\nplot_separator(ax, slope_svm, intercept_line, '-', 'black')\n\nax.text(x_min + 0.5, y_max - 0.5,\n        r'$\\langle \\mathbf{x}, \\mathbf{w} \\rangle - b = 0$',\n        fontsize=12,\n        bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.2'))\n\n# Line 2 and 3: small-margin lines\n# Correct: plot margin boundaries using (w · x + b = ±1)\n# These yield: x2 = -(w0/w1) * x1 - (b ± 1)/w1\n\nintercept_plus = -(intercept_svm - 1) / w[1]  # w·x + b = 1 ⇒ b - 1\nintercept_minus = -(intercept_svm + 1) / w[1] # w·x + b = -1 ⇒ b + 1\n\nplot_separator(ax, slope_svm, intercept_plus, '--', 'gray')\nplot_separator(ax, slope_svm, intercept_minus, '--', 'gray')\n\n\n# ==== 6. Draw normal vector w ====\n# ==== Compute & draw the normal vector w ====\n# w is the normal vector to the hyperplane\nw = clf.coef_[0]\nb = clf.intercept_[0]\n\n# Unit vector in direction of w\nw_unit = w / np.linalg.norm(w)\n\n# Find a point (x0, x1) on the hyperplane\n# We'll pick x = 0 and solve for x1 using the decision boundary equation:\n# w0*x + w1*y + b = 0 → y = -(w0*x + b)/w1\nx0 = 0\ny0 = -(w[0] * x0 + b) / w[1]\nstart = np.array([x0, y0])\nend = start + 1.0 * w_unit  # extend for visibility\n\n# Draw arrow from the hyperplane in direction of w\nax.annotate('', xy=end, xytext=start,\n            arrowprops=dict(arrowstyle='-&gt;', color='black', linewidth=2))\nax.text(end[0] - 0.1, end[1], r'$\\mathbf{w}$', fontsize=12)\n\n\n# ==== Final formatting ====\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_aspect('equal') \n#ax.legend(loc='upper left', framealpha=1)\nplt.tight_layout()\n\n# Save and show\nplt.savefig('images/svm_definition.svg', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\n\n# ==== 1. Generate *linearly separable* data ====\nseed = 42\nnp.random.seed(seed)\n\nn = 30\nmean_pos = [1, 1]\nmean_neg = [-1, -1]\ncov = [[0.4, 0.0], [0.0, 0.4]]\n\n\nX_pos = np.random.multivariate_normal(mean_pos, cov, size=n//2)\nX_neg = np.random.multivariate_normal(mean_neg, cov, size=n//2)\nX = np.vstack((X_pos, X_neg))\ny = np.hstack((np.ones(n//2), np.zeros(n//2)))\n\n# Stretch x so data width : height ≈ 6 : 4  (i.e., multiply by 1.5)\nstretch = 6 / 3  # 1.5\nX[:, 0] *= stretch\n\n# ==== 2. Fit SVM with linear kernel ====\nclf = SVC(kernel='linear', C=1e5)  # large C for hard-margin behavior\nclf.fit(X, y)\n\n# ==== 3. Create meshgrid for plotting (optional) ====\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 400),\n                     np.linspace(y_min, y_max, 400))\n\n# ==== 4. Plot ====\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Scatter data points\nax.scatter(X[y == 1, 0], X[y == 1, 1], c='green', marker='s')\nax.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', marker='^')\n\n# ==== 5. Add three different (non-parallel) separating hyperplanes ====\ndef plot_separator(ax, slope, intercept, style, color, label=\"\"):\n    x_vals = np.linspace(x_min, x_max, 200)\n    y_vals = slope * x_vals + intercept\n    ax.plot(x_vals, y_vals, style, color=color, linewidth=1.8, label=label)\n\n# Line 1: SVM decision boundary (maximum margin)\nw = clf.coef_[0]\nintercept_svm = clf.intercept_[0]\nslope_svm = -w[0] / w[1]\nintercept_line = -intercept_svm / w[1]\nmargin = 1 / np.linalg.norm(w)\nplot_separator(ax, slope_svm, intercept_line, '-', 'black')\n\n# Correct: plot margin boundaries using (w · x + b = ±1)\n# These yield: x2 = -(w0/w1) * x1 - (b ± 1)/w1\n\nintercept_plus = -(intercept_svm - 1) / w[1]  # w·x + b = 1 ⇒ b - 1\nintercept_minus = -(intercept_svm + 1) / w[1] # w·x + b = -1 ⇒ b + 1\n\nplot_separator(ax, slope_svm, intercept_plus, '--', 'gray')\nplot_separator(ax, slope_svm, intercept_minus, '--', 'gray')\n\n\n# ==== 6. Add z1 and z2 on margin boundaries ====\n\n# Unit vector in direction of w (normal to boundary)\n\n# Point on decision boundary (midpoint between margins)\n# We can pick any x along the boundary; let's choose x = 0\nx0 = 0\ny0 = slope_svm * x0 + intercept_line\npoint_on_hyperplane = np.array([x0, y0])\n\n# Compute z1 and z2 by moving along ±w_unit * margin\nz1 = point_on_hyperplane - margin * w / np.linalg.norm(w)\nz2 = point_on_hyperplane + margin * w/ np.linalg.norm(w)\n\n# Plot z1 and z2\nax.plot(z1[0], z1[1], 'ko')  # black point\nax.plot(z2[0], z2[1], 'ko')\n\nax.text(z1[0] - 0.3, z1[1] - 0.2, r'$z_2$', fontsize=12)\nax.text(z2[0] + 0.1, z2[1] + 0.05, r'$z_1$', fontsize=12)\n\n\n# ==== Final formatting ====\nax.set_xlim(x_min/3, x_max/3)\nax.set_ylim(y_min/3, y_max/3)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_aspect('equal') \n#ax.legend(loc='upper left', framealpha=1)\nplt.tight_layout()\n\n# Save and show\nplt.savefig('images/svm_margin.svg', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\n\n# ==== 1. Generate *linearly separable* data ====\nseed = 42\nnp.random.seed(seed)\n\nn = 30\nmean_pos = [1, 1]\nmean_neg = [-1, -1]\ncov = [[0.4, 0.0], [0.0, 0.4]]\n\n\nX_pos = np.random.multivariate_normal(mean_pos, cov, size=n//2)\nX_neg = np.random.multivariate_normal(mean_neg, cov, size=n//2)\nX = np.vstack((X_pos, X_neg))\ny = np.hstack((np.ones(n//2), np.zeros(n//2)))\n\n# Stretch x so data width : height ≈ 6 : 4  (i.e., multiply by 1.5)\nstretch = 6 / 3  # 1.5\nX[:, 0] *= stretch\n\n# ==== 2. Fit SVM with linear kernel ====\nclf = SVC(kernel='linear', C=1e5)  # large C for hard-margin behavior\nclf.fit(X, y)\n\n# ==== 3. Create meshgrid for plotting (optional) ====\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 400),\n                     np.linspace(y_min, y_max, 400))\n\n# ==== 4. Plot ====\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Scatter data points\nax.scatter(X[y == 1, 0], X[y == 1, 1], c='green', marker='s')\nax.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', marker='^')\n\n# ==== 5. Add three different (non-parallel) separating hyperplanes ====\ndef plot_separator(ax, slope, intercept, style, color, label=\"\"):\n    x_vals = np.linspace(x_min, x_max, 200)\n    y_vals = slope * x_vals + intercept\n    ax.plot(x_vals, y_vals, style, color=color, linewidth=1.8, label=label)\n\n# Line 1: SVM decision boundary (maximum margin)\nw = clf.coef_[0]\nintercept_svm = clf.intercept_[0]\nslope_svm = -w[0] / w[1]\nintercept_line = -intercept_svm / w[1]\nmargin = 1 / np.linalg.norm(w)\nplot_separator(ax, slope_svm, intercept_line, '-', 'black')\n\n# Correct: plot margin boundaries using (w · x + b = ±1)\n# These yield: x2 = -(w0/w1) * x1 - (b ± 1)/w1\n\nintercept_plus = -(intercept_svm - 1) / w[1]  # w·x + b = 1 ⇒ b - 1\nintercept_minus = -(intercept_svm + 1) / w[1] # w·x + b = -1 ⇒ b + 1\n\nplot_separator(ax, slope_svm, intercept_plus, '--', 'gray')\nplot_separator(ax, slope_svm, intercept_minus, '--', 'gray')\n\n# ==== 6. Highlight support vectors with red dotted circles ====\nsupport_vectors = clf.support_vectors_\nfor sv in support_vectors:\n    circle = plt.Circle((sv[0], sv[1]), radius=0.2, edgecolor='red',\n                        facecolor='none', linestyle=':', linewidth=1.5)\n    ax.add_patch(circle)\n\n\n# ==== Final formatting ====\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_aspect('equal') \n#ax.legend(loc='upper left', framealpha=1)\nplt.tight_layout()\n\n# Save and show\nplt.savefig('images/svm_supports.svg', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\n\n# ==== 1. Generate *linearly separable* data ====\nseed = 42\nnp.random.seed(seed)\n\nn = 30\nmean_pos = [.75, .75]\nmean_neg = [-.75, -.75]\ncov = [[0.6, 0.0], [0.0, 0.6]]\n\n\nX_pos = np.random.multivariate_normal(mean_pos, cov, size=n//2)\nX_neg = np.random.multivariate_normal(mean_neg, cov, size=n//2)\nX = np.vstack((X_pos, X_neg))\ny = np.hstack((np.ones(n//2), np.zeros(n//2)))\n\n# Stretch x so data width : height ≈ 6 : 4  (i.e., multiply by 1.5)\nstretch = 6 / 3  # 1.5\nX[:, 0] *= stretch\n\n# ==== 2. Fit SVM with linear kernel ====\nclf = SVC(kernel='linear', C=1)  # large C for hard-margin behavior\nclf.fit(X, y)\n\n# ==== 3. Create meshgrid for plotting (optional) ====\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 400),\n                     np.linspace(y_min, y_max, 400))\n\n# ==== 4. Plot ====\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Scatter data points\nax.scatter(X[y == 1, 0], X[y == 1, 1], c='green', marker='s')\nax.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', marker='^')\n\n# ==== 5. Add three different (non-parallel) separating hyperplanes ====\ndef plot_separator(ax, slope, intercept, style, color, label=\"\"):\n    x_vals = np.linspace(x_min, x_max, 200)\n    y_vals = slope * x_vals + intercept\n    ax.plot(x_vals, y_vals, style, color=color, linewidth=1.8, label=label)\n\n# Line 1: SVM decision boundary (maximum margin)\nw = clf.coef_[0]\nintercept_svm = clf.intercept_[0]\nslope_svm = -w[0] / w[1]\nintercept_line = -intercept_svm / w[1]\nmargin = 1 / np.linalg.norm(w)\nplot_separator(ax, slope_svm, intercept_line, '-', 'black')\n\n# Correct: plot margin boundaries using (w · x + b = ±1)\n# These yield: x2 = -(w0/w1) * x1 - (b ± 1)/w1\n\nintercept_plus = -(intercept_svm - 1) / w[1]  # w·x + b = 1 ⇒ b - 1\nintercept_minus = -(intercept_svm + 1) / w[1] # w·x + b = -1 ⇒ b + 1\n\nplot_separator(ax, slope_svm, intercept_plus, '--', 'gray')\nplot_separator(ax, slope_svm, intercept_minus, '--', 'gray')\n\n# ==== 6. Draw red lines from misclassified points to correct margin ====\nfrom numpy.linalg import norm\n\n# Signed distance from the hyperplane\ndecision_values = clf.decision_function(X)\n\n# Normalized w\nw = clf.coef_[0]\nw_unit = w / norm(w)\n\n# Go through all points\nfor xi, yi, di in zip(X, y, decision_values):\n    correct_margin = 1 if yi == 1 else -1\n    if di * (2 * yi - 1) &lt; 1:  # misclassified\n        # Distance to move along w to reach the correct margin\n        delta = (correct_margin - (np.dot(w, xi) + intercept_svm)) / norm(w)**2\n        projection = xi + delta * w  # point on correct margin\n        ax.plot([xi[0], projection[0]], [xi[1], projection[1]], 'r-', linewidth=1, alpha=0.5)\n\n# Scatter data points\nax.scatter(X[y == 1, 0], X[y == 1, 1], c='green', marker='s')\nax.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', marker='^')\n\n# ==== Final formatting ====\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_aspect('equal') \n#ax.legend(loc='upper left', framealpha=1)\nplt.tight_layout()\n\n# Save and show\nplt.savefig('images/svm_soft.svg', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(5, 3))\nX = np.linspace(-5, 5, 1000).reshape(-1, 1)\nys = {}\n\n\nys['Hinge'] = np.maximum((1-X), 0)\nys['Cross Entropy'] = -np.log(1 / (1 + np.exp(-X)))  # Logistic function\n\n\ncolors = ['green', 'blue']\nlinestyles = ['-', '--']\nfor idx, (label, y) in enumerate(ys.items()):\n    plt.plot(X, y, label=label, linewidth=3, alpha=0.7, zorder=1, linestyle=linestyles[idx], color=colors[idx])\n\nplt.legend()\nplt.title('Classification Loss')\nplt.xlabel(r'$\\langle \\mathbf{x}, \\mathbf{w} \\rangle$')\nplt.ylabel(r'Loss if $y=1$')\nplt.tight_layout()\nplt.savefig('images/svm_loss.svg', bbox_inches='tight', dpi=300)"
  },
  {
    "objectID": "notes/code.html#kernel-methods",
    "href": "notes/code.html#kernel-methods",
    "title": "Code for Producing Images in Lecture Notes",
    "section": "Kernel Methods",
    "text": "Kernel Methods\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import to_rgb\nfrom scipy.spatial import Voronoi\n\n# Helper to convert infinite Voronoi regions into finite polygons (clipped to a bounding box)\ndef voronoi_finite_polygons_2d(vor, radius=None):\n    if vor.points.shape[1] != 2:\n        raise ValueError(\"Requires 2D input\")\n\n    new_regions = []\n    new_vertices = vor.vertices.tolist()\n\n    center = vor.points.mean(axis=0)\n    if radius is None:\n        radius = vor.points.ptp().max()*2\n\n    # Construct a map containing all ridges for a given point\n    all_ridges = {}\n    for (p1, p2), (v1, v2) in zip(vor.ridge_points, vor.ridge_vertices):\n        all_ridges.setdefault(p1, []).append((p2, v1, v2))\n        all_ridges.setdefault(p2, []).append((p1, v1, v2))\n\n    # Reconstruct infinite regions\n    for p1, region_index in enumerate(vor.point_region):\n        vertices = vor.regions[region_index]\n        if all(v &gt;= 0 for v in vertices):\n            # Finite region\n            new_regions.append(vertices)\n            continue\n\n        # Rebuild a finite region\n        ridges = all_ridges[p1]\n        new_region = [v for v in vertices if v &gt;= 0]\n\n        for p2, v1, v2 in ridges:\n            if v1 &gt;= 0 and v2 &gt;= 0:\n                continue\n            # Compute the missing endpoint of an infinite ridge\n            t = vor.points[p2] - vor.points[p1]  # tangent\n            t /= np.linalg.norm(t)\n            n = np.array([-t[1], t[0]])  # normal\n\n            midpoint = vor.points[[p1, p2]].mean(axis=0)\n            direction = np.sign(np.dot(midpoint - center, n)) * n\n            far_point = vor.vertices[v1 if v1 &gt;= 0 else v2] + direction * radius\n\n            new_vertices.append(far_point.tolist())\n            new_region.append(len(new_vertices) - 1)\n\n        # Sort region vertices counterclockwise\n        vs = np.asarray([new_vertices[v] for v in new_region])\n        c = vs.mean(axis=0)\n        angles = np.arctan2(vs[:,1] - c[1], vs[:,0] - c[0])\n        new_region = [v for _, v in sorted(zip(angles, new_region))]\n\n        new_regions.append(new_region)\n\n    return new_regions, np.asarray(new_vertices)\n\n# Generate points\n\nW, H = 6, 4\nnp.random.seed(7)\nn_points = 22\npts = np.random.rand(n_points, 2)\npts[:,0] *= W\npts[:,1] *= H\n\n# Build Voronoi\nvor = Voronoi(pts)\nregions, vertices = voronoi_finite_polygons_2d(vor, radius=2)\n\n# Colors\ncmap = plt.get_cmap(\"tab20\")\nbase_colors = [cmap(i % cmap.N) for i in range(n_points)]\n\ndef lighten(color, factor=0.65):\n    r, g, b = to_rgb(color)\n    r = r + (1 - r) * factor\n    g = g + (1 - g) * factor\n    b = b + (1 - b) * factor\n    return (r, g, b)\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Draw regions with light fill and no line shading\nfor i, region in enumerate(regions):\n    polygon = vertices[region]\n    polygon[:,0] = np.clip(polygon[:,0], 0, W)\n    polygon[:,1] = np.clip(polygon[:,1], 0, H)\n    ax.fill(\n        polygon[:,0], polygon[:,1],\n        facecolor=lighten(base_colors[i], 0.7),\n        edgecolor='none'\n    )\n\n# Plot points with no black edge\nax.scatter(pts[:,0], pts[:,1], s=70, c=base_colors, zorder=5)\n\nax.set_xlim(0, W)\nax.set_ylim(0, H)\nax.set_aspect(\"equal\", adjustable=\"box\")\nax.set_xticks([])\nax.set_yticks([])\nax.set_axis_off()\nax.set_title(\"$k$ Nearest Neighbors ($k=1$)\")\n\nplt.savefig(\"images/kernel_knn.svg\", dpi=300)"
  },
  {
    "objectID": "notes/code.html#autoencoders",
    "href": "notes/code.html#autoencoders",
    "title": "Code for Producing Images in Lecture Notes",
    "section": "Autoencoders",
    "text": "Autoencoders\n\nfrom PIL import Image, ImageOps\n\nimg = Image.open(\"images/cmc_seal_rgb.png\").convert(\"L\")\nimg_arr = np.array(img) / 255\ncmc_red = np.array([139, 39, 52])\n\n\n# scipy linalg\nimport scipy.linalg\nU, s, Vh = scipy.linalg.svd(img_arr, full_matrices=False)\nrank = len(s)\n\ndef build_k_approx(k):\n    return (U[:, :k] * s[:k]) @ Vh[:k, :]\n\n\napprox = build_k_approx(100)\ngray_image = Image.fromarray((approx * 255).astype(np.uint8), mode=\"L\")\nred_image = ImageOps.colorize(gray_image, black=cmc_red, white=\"white\")\nred_image\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib as mpl\nmpl.rcParams['text.usetex'] = False  # avoid LaTeX dependency\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageOps\n\n# If not already defined:\n# cmc_red = (152, 26, 49)\n\ndef to_red_pil(a, low=(152, 26, 49), high=\"white\"):\n    \"\"\"Map a grayscale array [0,1] to a red-toned PIL image.\"\"\"\n    a = np.asarray(a, dtype=float)\n    # Ensure it's in [0,1]\n    if a.max() &gt; 1.0 or a.min() &lt; 0.0:\n        a = (a - a.min()) / (a.max() - a.min() + 1e-12)\n    gray = Image.fromarray((a * 255).astype(np.uint8), mode=\"L\")\n    return ImageOps.colorize(gray, black=low, white=high)\n\n# Your desired ranks\ndesired = [1, 5, 10, 50, 100, 500, 1000]\n\n# Get a sample to infer full rank (min(H, W))\n# If you already have `approx` from build_k_approx(100), you can use that:\nsample = build_k_approx(1)\nfull_k = min(sample.shape)\n\n# Cap any requested ranks to full_k to avoid errors\nranks = [min(k, full_k) for k in desired]\n# Add \"full rank\" explicitly at the end\nranks_with_full = ranks + [full_k]\n\n# Make subplots\nn = len(ranks_with_full)\ncols = 4\nrows = int(np.ceil(n / cols))\nfig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows), constrained_layout=True)\naxes = np.ravel(axes)\n\nfor ax, k in zip(axes, ranks_with_full):\n    arr = build_k_approx(k)\n    red_img = to_red_pil(arr, low=cmc_red, high=\"white\")\n    ax.imshow(red_img)\n    ax.set_title(rf\"Rank ${k}$ Approximation\" if k != full_k else f\"Full Rank\")\n    ax.axis(\"off\")\n\n\n# Hide any unused axes\nfor ax in axes[n:]:\n    ax.axis(\"off\")\n\nplt.savefig(\"images/pca_approximations.svg\")\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Example data: replace with your singular values array\n# s should be sorted descending from your SVD\n# U, s, Vt = np.linalg.svd(A)\nrank = len(s)\n\n# Convert cmc_red (0–255 tuple) to Matplotlib's 0–1 float\ncmc_red_norm = tuple(np.array(cmc_red) / 255)\n\n# Compute approximation error: error[k] = sum_{i=k}^rank σᵢ²\n# Note: If you want k starting from 1, adjust indexing below\nerrors = [np.sum(s[k:]**2) for k in range(rank)]\n\n# Create subplots\nfig, axes = plt.subplots(1, 2, figsize=(9, 4))\n\n# ---- Left subplot: Singular Value Spectrum ----\naxes[0].plot(range(1, rank+1), s, color=cmc_red_norm, linewidth=2)\naxes[0].set_yscale(\"log\")\naxes[0].set_xlabel(\"Singular Value Index\")\naxes[0].set_ylabel(\"Singular Values\")\naxes[0].set_title(\"Singular Value Spectrum\")\n\n# ---- Right subplot: Approximation Error ----\naxes[1].plot(range(1, rank+1), errors, color=cmc_red_norm, linewidth=2)\naxes[1].set_yscale(\"log\")\naxes[1].set_xlabel(r\"$k$\")\naxes[1].set_ylabel(r\"$\\sum_{i=k+1}^{r} \\sigma_i^2$\")\naxes[1].set_title(\"Approximation Error\")\n\nplt.tight_layout()\nplt.savefig(\"images/pca_spectrum.svg\")"
  },
  {
    "objectID": "notes/code.html#concentration-inequalities",
    "href": "notes/code.html#concentration-inequalities",
    "title": "Code for Producing Images in Lecture Notes",
    "section": "Concentration Inequalities",
    "text": "Concentration Inequalities\n\nfig, ax = plt.subplots(figsize=(4, 3))\n\nt = np.linspace(.01, 10, 100)\nax.set_ylim(bottom=0,top=1)\nax.plot(t, 1/t, linewidth=2, color='blue')\nax.set_xlabel(r\"$\\epsilon$\")\nax.set_ylabel(r\"$\\Pr(X \\geq \\epsilon \\cdot \\mathbb{E}[X])$\")\nax.set_title(\"Markov's Inequality\")\nplt.tight_layout()\nplt.savefig(\"images/concentration_markov.svg\", dpi=300, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(4, 3))\n\nax.plot(t, 1/t**2, linewidth=2, color='blue')\n\nax.set_xlabel(r\"$\\epsilon$\")\nax.set_ylabel(r\"$\\Pr(|X-\\mathbb{E}[X]| \\geq \\epsilon \\cdot \\sqrt{Var(X)})$\")\nax.set_ylim(bottom=0,top=1)\nax.set_title(\"Chebyshev's Inequality\")\nplt.tight_layout()\nplt.savefig(\"images/concentration_chebyshev.svg\", dpi=300, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(4, 3))\n\nax.plot(t, 2*np.exp(-2*t**2), linewidth=2, color='blue')\n\nax.set_xlabel(r\"$\\epsilon$\")\nax.set_ylabel(r\"$\\Pr(|\\bar{X}-\\mathbb{E}[\\bar{X}]| \\geq \\epsilon \\cdot \\frac{b-a}{\\sqrt{2n}})$\")\nax.set_ylim(bottom=0,top=1)\nax.set_title(\"Hoeffding's Inequality\")\nplt.tight_layout()\nplt.savefig(\"images/concentration_hoeffding.svg\", dpi=300, bbox_inches='tight')"
  },
  {
    "objectID": "notes/Concentration.html",
    "href": "notes/Concentration.html",
    "title": "Concentration Inequalities",
    "section": "",
    "text": "In our exploration of reinforcement learning, we saw how the outcome of actions can be random. More generally, we are surrounded by random processes, from the weather to stock prices. Today, we will discuss a powerful tool for understanding the behavior of random variables: concentration inequalities. In the remainder of the course, we will apply these inequalities in a simplified model of reinforcement learning, explainable AI, and active learning.\nLet \\(X\\) be a random variable drawn from a distribution. We will let \\(\\Pr(X = x)\\) denote the probability that \\(X\\) takes on the particular value \\(x\\). The expectation, or mean, of \\(X\\) is defined as \\[\n\\mathbb{E}[X] = \\sum_{x} x \\cdot \\Pr(X = x).\n\\] Note: When \\(X\\) is a continuous random variable, the expectation is defined using an integral instead of a sum.\nThe variance of \\(X\\) measures how closely it concentrates around its expectation: \\[\n\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2].\n\\]\n\n\n\nConcentration inequalities will help us bound the probability that \\(X\\) deviates from its mean by a certain amount.\n\nMarkov’s Inequality\nThe building block of concentration inequalities is Markov’s inequality. Let \\(\\mathbf{X}\\) be a non-negative random variable. Markov’s inequality says that, for any \\(\\epsilon &gt; 0\\), \\[\n\\Pr(\\mathbf{X} \\geq \\epsilon) \\leq \\frac{\\mathbb{E}[\\mathbf{X}]}{\\epsilon}.\n\\]\n\n\n\nProof of Markov’s Inequality\n\nWe will use the definition of expectation to prove Markov’s inequality: \\[\n\\begin{align}\n\\mathbb{E}[\\mathbf{X}]\n&= \\sum_{x} x \\cdot \\Pr(\\mathbf{X} = x)\n\\\\&= \\sum_{x: x \\geq \\epsilon} x \\cdot \\Pr(\\mathbf{X} = x) + \\sum_{x: x &lt; \\epsilon} x \\cdot \\Pr(\\mathbf{X} = x)\n\\\\&\\geq \\sum_{x: x \\geq \\epsilon} \\epsilon \\cdot \\Pr(\\mathbf{X} = x) + \\sum_{x: x &lt; \\epsilon} 0 \\cdot \\Pr(\\mathbf{X} = x)\n\\\\&= \\epsilon \\sum_{x: x \\geq \\epsilon} \\Pr(\\mathbf{X} = x)\n\\\\&= \\epsilon \\cdot \\Pr(\\mathbf{X} \\geq \\epsilon),\n\\end{align}\n\\] where the second equality is by partitioning the sum, the inequality is because \\(x \\geq 0\\) by assumption in the first term and \\(x\\) is non-negative, and the last equality follows because the probability of the event \\(\\{\\mathbf{X} \\geq \\epsilon\\}\\) is the sum of the probabilities of all outcomes where \\(\\mathbf{X}\\) is at least \\(\\epsilon\\).\n\n\n\n\n\nAs \\(\\epsilon\\) increases, the probability that \\(X \\geq \\epsilon\\) decays at a rate of \\(1/\\epsilon\\). This is certainly in the right direction (as \\(\\epsilon\\) increases, the probability that \\(X\\) exceeds \\(\\epsilon\\) decreases), but we could hope for a stronger bound. For example, the tail bound of the Gaussian distribution decays at an exponential rate. However, there are also some distributions where Markov’s inequality is tight i.e., \\(\\Pr(X \\geq \\epsilon) = \\frac{\\mathbb{E}[X]}{\\epsilon}\\). Can you construct such a distribution?\nIn order to distinguish between distributions with different tail behaviors, we will make use of additional information. The next inequality we consider will incorporate the variance.\n\n\nChebyshev’s Inequality\nChebyshev’s inequality is a stronger concentration inequality that applies to any random variable with a finite mean and variance. Let \\(\\sigma^2 = \\text{Var}(X)\\) be the variance of \\(X\\), and \\(\\sigma = \\sqrt{\\sigma^2}\\) be the standard deviation. Chebyshev’s inequality states that, for any \\(\\epsilon &gt; 0\\), \\[\n\\Pr(|X - \\mathbb{E}[X]| \\geq \\epsilon \\sigma) \\leq \\frac{1}{\\epsilon^2}.\n\\] In words, the probability that a random variable deviates from its mean by more than \\(\\epsilon\\) standard deviations is at most \\(\\frac{1}{\\epsilon^2}\\).\n\n\n\nProof of Chebyshev’s Inequality\n\nWhile the two inequalities appear quite different, we will actually use Markov’s to prove Chebyshev’s. Define a new random variable \\(Z = (\\mathbf{X} - \\mathbb{E}[X])^2\\). Then, we can apply Markov’s inequality to \\(Z\\): \\[\n\\Pr((\\mathbf{X} - \\mathbb{E}[X])^2 \\geq \\epsilon^2) \\leq \\frac{\\mathbb{E}[(\\mathbf{X} - \\mathbb{E}[X])^2]}{\\epsilon^2}.\n\\] Notice that \\(\\mathbb{E}[(\\mathbf{X} - \\mathbb{E}[X])^2] = \\text{Var}(X) = \\sigma^2\\). Taking the squareroot of both sides of the event \\((\\mathbf{X} - \\mathbb{E}[X])^2 \\geq \\epsilon^2\\) yields \\[\n\\Pr(|\\mathbf{X} - \\mathbb{E}[X]| \\geq \\epsilon) \\leq \\frac{\\sigma^2}{\\epsilon^2}.\n\\] Setting \\(\\epsilon = \\epsilon' \\sigma\\) gives us \\[\n\\Pr(|\\mathbf{X} - \\mathbb{E}[X]| \\geq \\epsilon' \\sigma) \\leq \\frac{\\sigma^2}{(\\epsilon' \\sigma)^2} = \\frac{1}{\\epsilon'^2}.\n\\] The statement follows by relabeling \\(\\epsilon'\\) as \\(\\epsilon\\).\n\n\n\n\n\nThe advantage of Chebyshev’s inequality is that information about the distribution’s variance yields tighter bounds. But, for fixed variance, the probability of deviating more than \\(\\epsilon\\) from the mean is still only \\(1/\\epsilon^2\\). When we are interested in the behavior of many random variables simultaneously, we’ll need an even stronger dependence on \\(\\epsilon\\) to get meaningful bounds.\n\n\nUnion Bound\nBefore we develop the stronger bound, we’ll cover another important concept: the union bound. Consider events \\(E_1, \\ldots E_m\\). The union bound states that \\[\n\\Pr\\left( E_1, \\ldots, E_m \\right) \\leq \\Pr(E_1) + \\ldots + \\Pr(E_m).\n\\]\nThe simplest way to see this is through a Venn diagram. Each event \\(E_i\\) corresponds to a region in the diagram, and the union of all events corresponds to the total area covered by these regions. By the properties of probability, the area of the union is at most the sum of the areas of the individual regions.\n\n\n\n\n\nHoeffding’s Inequality\nThe central limit theorem tells us that the sum of many independent random variables will be approximately normally distributed, regardless of the original distribution of the variables. (Check out this excellent 3blue1brown video for an intuitive explanation.) Hoeffding’s inequality allows us to formalize this idea for a finite number of random variables.\nConsider \\(n\\) independent random variables \\(X_1, X_2, \\ldots, X_n\\) that are bounded such that \\(a \\leq X_i \\leq b\\) for all \\(i\\). Let \\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\\) be the sample mean, and \\(\\mu = \\mathbb{E}[\\bar{X}]\\) be the expected value of the sample mean. Hoeffding’s inequality states that, for any \\(\\epsilon &gt; 0\\), \\[\n\\Pr\\left(\\left|\\bar{X} - \\mathbb{E}[\\bar{X}]\\right| \\geq \\epsilon\\right) \\leq 2 \\exp\\left(-\\frac{2n\\epsilon^2}{(b-a)^2}\\right).\n\\] In words, Hoeffding’s gives an exponentially decaying bound on the probability that the sample mean deviates from the true mean by more than \\(\\epsilon\\).\n\n\n\nProof of Hoeffding’s Inequality\n\nThe key technical tool is a result known as Hoeffding’s Lemma: For any random variable \\(Y\\) bounded between \\(a\\) and \\(b\\), \\[\n\\mathbb{E}[\\exp(\\lambda(Y - \\mathbb{E}[Y]))]\n\\leq \\exp\\left(\\frac{\\lambda^2(b-a)^2}{8}\\right).\n\\] See the Wikipedia page for several proofs.\nAs in the proof of Chebyshev’s, we will use Markov’s inequality. Define \\(Z = \\bar{X} - \\mathbb{E}[X] = \\frac1{n} \\sum_{i=1}^n (X_i - \\mathbb{E}[X_i])\\), where the last equality follows by the linearity of expectation. Let \\(\\lambda &gt; 0\\). Then, we can apply Markov’s inequality to the exponential of \\(\\lambda Z\\): \\[\n\\begin{align}\n\\Pr(Z \\geq \\epsilon) &= \\Pr\\left(\\exp({\\lambda Z}) \\geq \\exp({\\lambda \\epsilon})\\right)\n\\\\&\\leq \\frac{\\mathbb{E}[\\exp({\\lambda Z})]}{\\exp({\\lambda \\epsilon})}\n\\\\&= \\exp({-\\lambda \\epsilon})\n\\mathbb{E}\\left[\\exp\\left({\\frac{\\lambda}{n} \\sum_{i=1}^n (X_i - \\mathbb{E}[X_i])}\\right)\\right]\n\\\\&=  \\exp({-\\lambda \\epsilon}) \\prod_{i=1}^n \\mathbb{E}\\left[\\exp\\left({\\frac{\\lambda}{n} (X_i - \\mathbb{E}[X_i])}\\right)\\right]\n\\\\&\\leq \\exp({-\\lambda \\epsilon}) \\prod_{i=1}^n \\exp\\left({\\frac{\\lambda^2(b-a)^2}{8n^2}}\\right)\n\\\\&= \\exp\\left({-\\lambda \\epsilon} + \\frac{\\lambda^2(b-a)^2}{8n}\\right),\n\\end{align}\n\\] where the first inequality follows by Markov’s, the second equality follows by the definition of \\(Z\\), the third equality follows because the product of exponentials is the exponential of the sum, and the second inequality follows by Hoeffding’s Lemma.\nRecall that \\(\\lambda\\) is a free parameter since the inequality holds for all \\(\\lambda &gt; 0\\). So, in particular, we are free to minimize over \\(\\lambda\\). Let \\(A = (b-a)^2/n\\) for notational simplicity. The expression \\(-\\lambda \\epsilon + \\lambda^2 A/8\\) is convex in \\(\\lambda\\). Setting the derivative equal to 0 yields \\(-\\epsilon + 2\\lambda A / 8=0\\), and, rearranging, \\(\\lambda = \\frac{4\\epsilon}{A}\\). Plugging in this choice of \\(\\lambda\\), we get \\[\n{-\\lambda \\epsilon} + \\frac{\\lambda^2(b-a)^2}{8n}\n= -\\left( \\frac{4\\epsilon}{A} \\right) \\epsilon + \\left( \\frac{4\\epsilon}{A} \\right)^2 \\frac{A}{8}\n= -\\frac{2\\epsilon^2}{A}\n= -\\frac{2n\\epsilon^2}{(b-a)^2}.\n\\]\nRecall that \\(Z = \\bar{X} - \\mathbb{E}[\\bar{X}]\\). All together, we have \\[\n\\Pr(\\bar{X} - \\mathbb{E}[\\bar{X}] \\geq \\epsilon)\n= \\Pr(Z \\geq \\epsilon)\n\\leq \\exp\\left(-\\frac{2n\\epsilon^2}{(b-a)^2}\\right).\n\\]\nThe final statement follows by similarly bounding \\(-Z\\), and then union bounding over the event that \\(-Z &gt; \\epsilon\\) and \\(Z \\geq \\epsilon\\).\n\n\n\n\n\nFor a fixed interval \\([a, b]\\) and number of variables \\(n\\), Hoeffding’s inequality gives an exponentially decaying bound on the probability that the sample mean deviates from the true mean by more than \\(\\epsilon\\). When we discuss a simplified model of reinforcement learning, we will see how this strong inequality can be applied to simultaneously bound many events."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSCI 145: Data Mining",
    "section": "",
    "text": "A course on the mathematical foundations of machine learning.\n\n\n\n\nInstructor: R. Teal Witter. Please call me Teal.\nClass Times: Tuesdays and Thursdays from 4:15 to 5:30pm in Kravis 164.\nOffice Hours: Mondays and Thursdays from 12:30 to 2pm in Adams 213.\nProblem Sets: Your primary opportunity to learn the material will be on problem sets. You may work with others to solve the problems, but you must write your solutions by yourself, and explicitly acknowledge any outside help (e.g., websites, people, LLMs).\n\n\nQuizzes: There will be short quizzes at the beginning of (randomly) selected classes. These quizzes will test your understanding of the problem sets and the concepts from the prior week.\nExams: The two midterm exams are the primary method of assessing your understanding of the material.\nProject: The project offers a chance to explore an area that interests you, practice writing high quality code, and develop your ability to communicate technical ideas to an audience.\n\n\nResources: Most of the material we cover comes from either Chris Musco’s phenomenal machine learning course, or Chinmay Hegde’s fantastic deep learning course. While we do not have a textbook, we do have readings for each lecture; I highly recommend you do these readings before each class.\n\n\n\nWeek\n\n\nTuesday\n\n\nThursday\n\n\nSlides\n\n\nAssignments\n\n\n\n\nWarm Up\n\n\n\n\nWeek 1 (1/20 and 1/22)\n\n\nMath Review\n\n\nLinear Algebra\n\n\n\n\n\n\n\n\nWeek 2 (1/27 and 1/29)\n\n\nPageRank\n\n\nPageRank\n\n\n\n\n\n\n\n\nSupervised Learning\n\n\n\n\nWeek 3 (2/3 and 2/5)\n\n\nLinear Regression\n\n\nLinear Regression\n\n\n\n\n\n\n\n\nWeek 4 (2/10 and 2/12)\n\n\nExact Optimization\n\n\nGradient Descent\n\n\n\n\n\n\n\n\nWeek 5 (2/17 and 2/19)\n\n\nPolynomial Regression\n\n\nProbability\n\n\n\n\n\n\n\n\nWeek 6 (2/24 and 2/26)\n\n\nLogistic Regression\n\n\nNeural Networks\n\n\n\n\n\n\n\n\nWeek 7 (3/3 and 3/5)\n\n\nBackpropagation\n\n\nMidterm Exam\n\n\n\n\n\n\n\n\nWeek 8 (3/10 and 3/12)\n\n\nConvolutional Networks\n\n\nTransformers\n\n\n\n\n\n\n\n\nWeek 9 (3/17 and 3/19)\n\n\nSpring Break (No Class)\n\n\nSpring Break (No Class)\n\n\n\n\n\n\n\n\nWeek 10 (3/24 and 3/26)\n\n\nDecision Trees and Random Forests\n\n\nAdaBoost\n\n\n\n\n\n\n\n\nUnsupervised Learning\n\n\n\n\nWeek 11 (3/31 and 4/2)\n\n\nGradient Boosting\n\n\nAutoencoders\n\n\n\n\n\n\n\n\nWeek 12 (4/7 and 4/9)\n\n\nAutoencoders\n\n\nPrincipal Component Analysis\n\n\n\n\n\n\n\n\nWeek 13 (4/14 and 4/16)\n\n\nReinforcement Learning Applied\n\n\nReinforcement Learning Applied\n\n\n\n\n\n\n\n\nWeek 14 (4/21 and 4/23)\n\n\nConcentration Inequalities\n\n\nConcentration Inequalities\n\n\n\n\n\n\n\n\nWeek 15 (4/28 and 4/30)\n\n\nMulti-armed Bandits\n\n\nMidterm Exam\n\n\n\n\n\n\n\n\nWeek 16 (5/5 and 5/7)\n\n\nProject Preparation\n\n\nReading Day (No Class)\n\n\n\n\n\n\n\n\nWeek 17 (5/12 and 5/14)\n\n\nProject Presentation 7-10pm\n\n\nFinals (No Class)"
  },
  {
    "objectID": "notes/LinearRegression.html",
    "href": "notes/LinearRegression.html",
    "title": "Linear Regression and Optimization",
    "section": "",
    "text": "Machine learning is incredibly popular. The area has seen extensive progress in the past several decades, and especially recently with the advent of generative AI. There are many problems that fall under the umbrella of machine learning. For example:\n\nPredicting temperature based on present weather conditions,\nIdentifying objects in an image, and\nGenerating the next word in a sentence.\n\nThis course covers how to solve these problems. The first half of the course will cover supervised learning, where we are given labeled data, and our goal is to train a function to approximately match the labels.\nConcretely, we are given \\(n\\) data points \\(\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(n)} \\in \\mathbb{R}^d\\), each with \\(d\\) dimensions, and associated labels \\(y^{(1)}, y^{(2)}, \\ldots, y^{(n)} \\in \\mathbb{R}\\). Our goal is to learn a function \\(f: \\mathbb{R}^d \\to \\mathbb{R}\\) so that \\(f(\\mathbf{x}^{(i)}) \\approx y^{(i)}\\) for all data points \\(i \\in \\{1,2,\\ldots,n\\}\\).\nOur general approach to solving supervised learning problems will be to use empirical risk minimization, which gives a flexible scaffolding that encompasses many of the topics we’ll discuss in this course. Given a function class, the idea is to select the function that most closely explains the data. In particular, there are three components to empirical risk minimization:\n\nFunction Class: The function class \\(\\mathcal{F}\\) from which we will select the function \\(f\\) that most closely fits the observed data.\nLoss: The loss function that measures how well a function \\(f\\) fits the observed data. (Without loss of generality, we will assume that lower is better.)\nOptimizer: The method of selecting the function from the function class.\n\nEmpirical risk minimization is an abstract idea. Luckily, we will revisit it again and again. Our first example will be linear regression, where the function class is the set of linear functions and the loss is the squared difference between the true label and our prediction. Let’s dive in!"
  },
  {
    "objectID": "notes/LinearRegression.html#supervised-learning",
    "href": "notes/LinearRegression.html#supervised-learning",
    "title": "Linear Regression and Optimization",
    "section": "",
    "text": "Machine learning is incredibly popular. The area has seen extensive progress in the past several decades, and especially recently with the advent of generative AI. There are many problems that fall under the umbrella of machine learning. For example:\n\nPredicting temperature based on present weather conditions,\nIdentifying objects in an image, and\nGenerating the next word in a sentence.\n\nThis course covers how to solve these problems. The first half of the course will cover supervised learning, where we are given labeled data, and our goal is to train a function to approximately match the labels.\nConcretely, we are given \\(n\\) data points \\(\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(n)} \\in \\mathbb{R}^d\\), each with \\(d\\) dimensions, and associated labels \\(y^{(1)}, y^{(2)}, \\ldots, y^{(n)} \\in \\mathbb{R}\\). Our goal is to learn a function \\(f: \\mathbb{R}^d \\to \\mathbb{R}\\) so that \\(f(\\mathbf{x}^{(i)}) \\approx y^{(i)}\\) for all data points \\(i \\in \\{1,2,\\ldots,n\\}\\).\nOur general approach to solving supervised learning problems will be to use empirical risk minimization, which gives a flexible scaffolding that encompasses many of the topics we’ll discuss in this course. Given a function class, the idea is to select the function that most closely explains the data. In particular, there are three components to empirical risk minimization:\n\nFunction Class: The function class \\(\\mathcal{F}\\) from which we will select the function \\(f\\) that most closely fits the observed data.\nLoss: The loss function that measures how well a function \\(f\\) fits the observed data. (Without loss of generality, we will assume that lower is better.)\nOptimizer: The method of selecting the function from the function class.\n\nEmpirical risk minimization is an abstract idea. Luckily, we will revisit it again and again. Our first example will be linear regression, where the function class is the set of linear functions and the loss is the squared difference between the true label and our prediction. Let’s dive in!"
  },
  {
    "objectID": "notes/LinearRegression.html#univariate-linear-regression",
    "href": "notes/LinearRegression.html#univariate-linear-regression",
    "title": "Linear Regression and Optimization",
    "section": "Univariate Linear Regression",
    "text": "Univariate Linear Regression\nLinear regression is a simple but powerful tool that we will use to understand the basics of machine learning. For simplicity, we will first consider the univariate case where the inputs are all one-dimensional i.e., \\(x^{(1)}, \\ldots, x^{(n)} \\in \\mathbb{R}\\).\n\nLinear functions\nAs its name suggests, linear regression uses a linear function to process the input into an approximation of the output. Let \\(w \\in \\mathbb{R}\\) be a weight parameter. The linear function (for one-dimensional inputs) is given by \\(f(x) = wx\\).\nUnlike many machine learning functions, we can visualize the linear function since it is given by a line. In the plot, we have \\(n=10\\) data points plotted in two dimensions. There is one linear function \\(f(x) = 2x\\) that closely approximates the data and another linear function \\(f(x)=\\frac12 x\\) that poorly approximates the data.\n\n\n\nOur goal is to learn how to find a linear function that fits the data well. Before we can do this, though, we will need to define what a “good” fit means.\n\n\nMean Squared Error Loss\nOur goal for the loss function is to measure how closely the data fits the prediction made by our function. Intuitively, we should take the difference between the prediction and the true outcome \\(f(x^{(i)})-y^{(i)}\\).\nThe issue with this approach is that \\(f(x^{(i)})-y^{(i)}\\) can be small (negative) even when \\(f(x^{(i)}) \\neq y^{(i)}\\). A natural fix is to take the absolute value \\(|f(x^{(i)}) - y^{(i)}|\\). The benefit of the absolute value is that the loss is \\(0\\) if and only if \\(f(x^{(i)}) = y^{(i)}\\). However, the absolute value function is not differentiable, which is a property we’ll need for optimization. Instead, we use the squared loss:\n\\[\n\\mathcal{L}(w) = \\frac1{n} \\sum_{i=1}^n (f(x^{(i)}) - y^{(i)})^2.\n\\]\nHere, we use the mean squared error loss, which is the average squared difference between the prediction and the true output over the dataset. Unlike the absolute value function, the squared function is differentiable everywhere. In addition, the squared error disproportionately penalizes predictions that are far from the true labels, a property that may be desirable when we want all of our predictions to be reasonably accurate.\n\n\n\nThe plot above compares the squared function to the absolute value function. While both are \\(0\\) if and only if their input is \\(0\\), the squared function is differentiable everywhere and penalizes large errors more.\n\n\nExact Optimization\nWe now have our function class and loss function: linear functions and mean squared error loss. The question becomes how to update the weights of the function to minimize the loss. In particular, we want to find \\(w\\) that minimizes \\(\\mathcal{L}(w)\\). While the language we’re using is new, the problem is not. We’ve actually been studying how to do this since pre-calculus!\nThe squared loss is convex (a bowl facing up versus the downward facing cave of concave, see the plot above). In this case, we know there is only one minimum. Not only that but we can find the minimum by setting the derivative to \\(0\\).\nOur game plan is to set \\(\\frac{\\partial \\mathcal{L}}{\\partial w}\\) to \\(0\\) and solve for \\(w\\). Recall that \\(f(x) = wx\\). We will use the linearity of the derivative, the chain rule, and the power rule to compute the derivative of \\(\\mathcal{L}\\) with respect to \\(w\\):\n\\[\n\\begin{align}\n\\frac{\\partial}{\\partial w}[\\mathcal{L}(w)]\n&= \\frac1{n} \\sum_{i=1}^n \\frac{\\partial}{\\partial w} [(w x^{(i)} - y^{(i)})^2]\n\\notag \\\\&= \\frac1{n} \\sum_{i=1}^n 2(w x^{(i)} - y^{(i)}) \\frac{\\partial}{\\partial w} [(w x^{(i)} - y^{(i)})]\n\\notag \\\\&= \\frac1{n} \\sum_{i=1}^n 2(w x^{(i)} - y^{(i)}) x^{(i)}.\n\\end{align}\n\\]\nSetting the derivative to \\(0\\) and solving for \\(w\\), we get \\(\\frac2{n} \\sum_{i=1}^n w \\cdot (x^{(i)})^2 = \\frac2{n} \\sum_{i=1}^n y^{(i)} x^{(i)}\\) and so \\[\nw = \\frac{\\sum_{i=1}^n y^{(i)} \\cdot x^{(i)}}{\\sum_{i=1}^n (x^{(i)})^2}.\n\\]\nThis is the exact solution to the univariate linear regression problem! We can now use this formula to find the best linear function for our univariate data. However, we’ll have to work slightly harder for the general case with multidimensional data."
  },
  {
    "objectID": "notes/LinearRegression.html#multivariate-linear-regression",
    "href": "notes/LinearRegression.html#multivariate-linear-regression",
    "title": "Linear Regression and Optimization",
    "section": "Multivariate Linear Regression",
    "text": "Multivariate Linear Regression\nConsider the more general setting where the input is \\(d\\)-dimensional. As before, we observe \\(n\\) training observations \\((\\mathbf{x}^{(1)}, y^{(1)}), \\ldots, (\\mathbf{x}^{(n)}, y^{(n)})\\) but now \\(\\mathbf{x}^{(i)} \\in \\mathbb{R}^d\\). We will generalize the ideas from univariate linear regression to the multivariate setting.\n\nLinear function\nInstead of using a single weight \\(w \\in \\mathbb{R}\\), we will use \\(d\\) weights \\(\\mathbf{w} \\in \\mathbb{R}^d\\). Then the function is given by \\(f(x) = \\langle \\mathbf{w}, \\mathbf{x} \\rangle\\).\nInstead of using a line to fit the data, we use a hyperplane. While visualizing the function is difficult in high dimensions, we can still plot the function when \\(d=2\\).\n\n\n\nIn the plot above, we have \\(n=10\\) data points in 3 dimensions. There is one linear function \\(\\mathbf{w} = \\begin{bmatrix} 2 \\\\ \\frac12 \\end{bmatrix}\\) that closely approximates the data and another linear function \\(\\mathbf{w} = \\begin{bmatrix} \\frac12 \\\\ 0 \\end{bmatrix}\\) that poorly approximates the data.\n\n\nMean Squared Error\nSince the output of \\(f\\) is still a single real number, we do not have to change the loss function. However, we can use our linear algebra notation to write the mean squared error in an elegant way.\nLet \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) be the data matrix where the \\(i\\)th row is \\((\\mathbf{x}^{(i)})^\\top\\). Similarly, let \\(\\mathbf{y} \\in \\mathbb{R}^n\\) be the target vector where the \\(i\\)th entry is \\(y^{(i)}\\). We can then write the mean squared error loss as \\[\n\\mathcal{L}(\\mathbf{w}) = \\frac1{n} \\| \\mathbf{X w - y} \\|_2^2.\n\\]\n\n\nExact Optimization\nJust like computing the derivative and setting it to \\(0\\), we can compute the gradient and set it to the zero vector \\(\\mathbf{0} \\in \\mathbb{R}^d\\). In mathematical notation, we will set \\(\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}^*) = \\mathbf{0}\\) and solve for \\(\\mathbf{w}^*\\). The intuition is that such a point is a local minimum in every direction; that is, we cannot improve the loss by moving in any of the dimensions. Since the loss is convex, there can be only one minima.\nAs you may recall from multivariate calculus, the gradient \\(\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w})\\) is simply a vector with the same dimension as \\(\\mathbf{w}\\); the value in the \\(i\\)th dimension is \\(\\frac{\\partial \\mathcal{L}(\\mathbf{w})}{\\partial w_i}\\).\nLet’s compute this quantity \\[\n\\begin{align}\n\\frac{\\partial \\mathcal{L}(\\mathbf{w})}{\\partial w_i}\n&= \\lim_{\\Delta \\to 0} \\frac{\\mathcal{L}(\\mathbf{w}+\\Delta \\mathbf{e}_i) - \\mathcal{L}(\\mathbf{w})}{\\Delta}\n\\\\&=\\lim_{\\Delta \\to 0} \\frac{\\| \\mathbf{X} (\\mathbf{w}+\\Delta \\mathbf{e}_i) - \\mathbf{y}\\|^2 - \\| \\mathbf{X} \\mathbf{w} - \\mathbf{y}\\|^2 }{\\Delta}.\n\\end{align}\n\\] Let \\(\\mathbf{a}, \\mathbf{b}\\) be two vectors in the same dimensional space. We have \\(\\|\\mathbf{a} + \\mathbf{b} \\|^2 = \\langle \\mathbf{a} + \\mathbf{b} , \\mathbf{a} + \\mathbf{b} \\rangle = \\| \\mathbf{a} \\|^2 + 2\\langle \\mathbf{a}, \\mathbf{b} \\rangle + \\| \\mathbf{b}\\|^2\\), where we foiled to reach the final equality, and used that \\(\\langle \\mathbf{a}, \\mathbf{b} \\rangle = \\langle \\mathbf{b}, \\mathbf{a} \\rangle\\). By letting \\(\\mathbf{a} = \\mathbf{X w - y}\\) and \\(\\mathbf{b} = \\Delta \\mathbf{X} \\mathbf{e}_i\\), we reach \\[\n\\begin{align}\n\\frac{\\partial \\mathcal{L}(\\mathbf{w})}{\\partial w_i}\n&=\\lim_{\\Delta \\to 0} \\frac{\\| \\mathbf{X} \\mathbf{w} - \\mathbf{y}\\|^2\n+ 2 \\langle \\mathbf{X} \\mathbf{w} - \\mathbf{y}, \\Delta \\mathbf{X e}_i \\rangle\n+\\|\\Delta \\mathbf{X e}_i\\|^2\n- \\| \\mathbf{X} \\mathbf{w} - \\mathbf{y}\\|^2 }{\\Delta}.\n\\\\&= \\lim_{\\Delta \\to 0} \\frac{2 \\Delta \\langle \\mathbf{X} \\mathbf{w} - \\mathbf{y}, \\mathbf{X e}_i \\rangle\n+\\Delta^2 \\|\\mathbf{X e}_i\\|^2}{\\Delta}\n\\\\&=2\\langle \\mathbf{X} \\mathbf{w} - \\mathbf{y}, \\mathbf{X e}_i \\rangle.\n\\end{align}\n\\] Let \\(\\mathbf{X}_i = \\mathbf{Xe}_i\\) be the \\(i\\)th column of \\(\\mathbf{X}\\). Then, the full gradient is given by \\[\n\\begin{align}\n\\nabla_\\mathbf{w}\\mathcal{L}(\\mathbf{w})\n= 2 \\begin{bmatrix}\n\\mathbf{X}_1^\\top (\\mathbf{X} \\mathbf{w} - \\mathbf{y}) \\\\ \\mathbf{X}_2^\\top (\\mathbf{X} \\mathbf{w} - \\mathbf{y}) \\\\ \\vdots\n\\end{bmatrix}\n\\end{align}\n= 2 \\mathbf{X}^\\top (\\mathbf{X} \\mathbf{w} - \\mathbf{y}).\n\\]\nBy the convexity of the loss function \\(\\mathcal{L}\\), we know that \\(\\nabla_\\mathbf{w}\\mathcal{L}(\\mathbf{w}^*)=0\\) at the optimal weights \\(\\mathbf{w}^*\\). Solving for \\(\\mathbf{w}^*\\) yields \\[\n\\begin{align}\n\\nabla_\\mathbf{w}\\mathcal{L}(\\mathbf{w}^*)\n&= 2 \\mathbf{X}^\\top (\\mathbf{X} \\mathbf{w}^* - \\mathbf{y})\n= 0\n\\\\ \\Leftrightarrow\n\\mathbf{X}^\\top \\mathbf{X} \\mathbf{w}^* &= \\mathbf{X}^\\top \\mathbf{y}\n\\\\ \\Leftrightarrow\n\\mathbf{w}^* &= (\\mathbf{X}^\\top \\mathbf{X})^+ \\mathbf{X}^\\top \\mathbf{y}\n\\end{align}\n\\] where \\((\\cdot)^+\\) is the pseudoinverse.\n\n\nSingular Value Decomposition\nEven when a matrix is not square, we can still use a generalization of the eigendecomposition. Suppose \\(\\mathbf{X} = \\sum_{i=1}^d \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^\\top\\) is the singular value decomposition (SVD) of \\(\\mathbf{X}\\), where \\(\\mathbf{u}_i \\in \\mathbb{R}^n\\) are the left singular vectors, \\(\\mathbf{v}_i \\in \\mathbb{R}^d\\) are the right singular vectors, and \\(\\sigma_i \\in \\mathbb{R}\\) are the singular values. (We’ll assume that all the columns of \\(\\mathbf{X}\\) are linearly independent; otherwise, we could simply remove one without changing the optimization problem.) The pseudoinverse is \\[\n\\mathbf{X}^+ = \\sum_{i=1}^d \\frac{1}{\\sigma_i} \\mathbf{v}_i \\mathbf{u}_i^\\top.\n\\]\nFor our equation, we can compute \\(\\mathbf{X}^\\top \\mathbf{X} = \\sum_{i=1}^d \\sigma_i^2 \\mathbf{v}_i \\mathbf{v}_i^\\top\\). So \\((\\mathbf{X}^\\top \\mathbf{X})^+ = \\sum_{i=1}^d \\frac{1}{\\sigma_i^2} \\mathbf{v}_i \\mathbf{v}_i^\\top\\). Together, we have \\[\n(\\mathbf{X}^\\top \\mathbf{X})^+ \\mathbf{X}^\\top\n= \\left( \\sum_{i=1}^d \\frac{1}{\\sigma_i^2} \\mathbf{v}_i \\mathbf{v}_i^\\top\\right)\n\\sum_{j=1}^d \\sigma_j \\mathbf{v}_j \\mathbf{u}_j^\\top\n= \\sum_{i=1}^d \\frac{1}{\\sigma_i} \\mathbf{v}_i \\mathbf{u}_i^\\top\n= \\mathbf{X}^+.\n\\]"
  },
  {
    "objectID": "notes/LinearRegression.html#empirical-risk-minimization",
    "href": "notes/LinearRegression.html#empirical-risk-minimization",
    "title": "Linear Regression and Optimization",
    "section": "Empirical Risk Minimization",
    "text": "Empirical Risk Minimization\nWe have now seen how to fit a linear function to data using the mean squared error loss. However, we have not given a satisfying answer to the question:\n\nWhy use mean squared error as our loss function?\n\n\nSo far, our answer has been that the quadratic function is differentiable (which we use to find the optimal solution), and that it naturally penalizes predictions which are farther away more. The first point is one of convenience and, a priori, should not be particularly persuasive. The second seems somewhat arbitrary, why penalize at a quadratic rate rather than an e.g., quartic rate? We’ll now consider a more compelling answer.\nOn our way to the answer, let’s take a step back and consider another question:\n\nWhy fit the data with a linear function?\n\n\nWell, we may do so when we expect the data truly has a linear relationship with the labels. To make things interesting, we will assume that there is random noise added to the labels, but that this noise is mean-centered so that, on average, the labels come from the linear model. Concretely, we observe some point \\(\\mathbf{x}\\) with a label that comes from a linear model \\(\\mathbf{w}^*\\) but with added noise, i.e., \\[\ny= \\langle \\mathbf{w}^*, \\mathbf{x} \\rangle + \\eta.\n\\] We will model this noise as distributed from a normal distribution, i.e., \\(\\eta \\sim \\mathcal{N}(0, \\sigma^2)\\) for some unknown standard deviation \\(\\sigma\\). (To justify this choice, we imagine the noise as a sum of random variables from some other distribution(s) which, by the law of large numbers, will follow the normal distribution when the sum contains sufficiently many terms.)\nRecall that the goal of our empirical risk minimization strategy is to find the function which most closely aligns with the data, or, put differently, we want the function that most likely generated the data we observed. In order to compute this likelihood, we will use the probability density function of the normal distribution: The probability we observe a random variable \\(y\\) drawn from a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) is given by \\[\n\\frac1{\\sqrt{2\\pi \\sigma}} \\exp\\left( - \\frac{(y- \\mu)^2}{2\\sigma^2} \\right).\n\\] If the noisy linear model \\(\\mathbf{w}\\) did generate the training data \\((\\mathbf{x}^{(i)}, y^{(i)})\\), then the expectation of the generation would be \\(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle\\). Then, combined with the assumption that the training data was drawn independently, the probability of observing the training data is given by the product of the probabilities of each individual observation: \\[\n\\prod_{i=1}^n\n\\frac1{\\sqrt{2\\pi \\sigma}} \\exp\\left( - \\frac{(y^{(i)}- \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)^2}{2\\sigma^2} \\right).\n\\] Our goal is to find the function \\(\\mathbf{w}\\) that maximizes this likelihood i.e., \\[\n\\begin{align}\n&{\\arg\\max}_{\\mathbf{w} \\in \\mathbb{R}^d}\n\\prod_{i=1}^n\n\\frac{1}{\\sqrt{2\\pi \\sigma}} \\exp\\left( - \\frac{(y^{(i)}- \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)^2}{2\\sigma^2} \\right)\n\\notag \\\\\n&= {\\arg\\min}_{\\mathbf{w} \\in \\mathbb{R}^d}\n- \\log \\left(\n\\frac{1}{\\sqrt{2\\pi \\sigma}} \\exp\\left(\\sum_{i=1}^n - \\frac{(y^{(i)}- \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)^2}{2\\sigma^2} \\right)\\right)\n\\notag \\\\\n&= {\\arg\\min}_{\\mathbf{w} \\in \\mathbb{R}^d}\n- \\sum_{i=1}^n - (y^{(i)}- \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)^2.\n\\end{align}\n\\] Here, we used the following facts: maximizing an objective is equivalent to minimizing the negative of that objective, the logarithmic function is monotonically increasing so minimizing the likelihood is equivalent to minimizing the log-likelihood, the product of exponentials is the exponential of the sum, and removing a constant scalar factor or additive constant does not change the minimum.\nThe punchline is that the function \\(\\mathbf{w}\\) that maximizes the likelihood of observing the training data is the same function that minimizes the mean squared error loss. This is a powerful result that justifies our use of the mean squared error loss."
  },
  {
    "objectID": "notes/LinearRegression.html#looking-forward",
    "href": "notes/LinearRegression.html#looking-forward",
    "title": "Linear Regression and Optimization",
    "section": "Looking Forward",
    "text": "Looking Forward\nWhile we have seen the benefits of exactly optimizing linear regression functions, there are several limitations that we will address.\nComputational Complexity We saw that the exact solution to linear regression is given by \\(\\mathbf{w}^* = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\\). This requires building the matrix \\(\\mathbf{X}^\\top \\mathbf{X}\\), which takes \\(O(nd^2)\\) time, and then inverting it, which takes \\(O(d^3)\\). When we have a large number of data points \\(n\\) and/or a large number of features \\(d\\), this can be prohibitively expensive.\nFunction Class Misspecification We have assumed that the data has a linear relationship (or close to linear relationship) with the labels. What happens when this is not true? That is, even the best linear function gives a poor approximation?"
  },
  {
    "objectID": "notes/KernelMethods.html",
    "href": "notes/KernelMethods.html",
    "title": "Kernel Methods",
    "section": "",
    "text": "Many of the methods we have discussed so far are linear in nature, i.e., we make predictions based on a linear combination of the input features. Next, we will explore non-linear methods, specifically kernel methods, and neural networks. Both are closely related to feature transformations, which we have already discussed in the context of linear regression and support vector machines.\n\n\\(k\\)-Nearest Neighbors\nWe’ll start with the \\(k\\)-nearest neighbors algorithm, which is a simple yet powerful non-parametric method for classification and regression. As usual in the supervised learning setting, we have \\(n\\) labeled training examples \\(\\{(\\mathbf{x}^{(i)}, y^{(i)})\\}_{i=1}^n\\), where \\(\\mathbf{x}^{(i)} \\in \\mathbb{R}^d\\) is the feature vector and \\(y^{(i)} \\in \\mathbb{R}\\) is the label. When we get a new unlabeled example \\(\\mathbf{x}\\), we find the \\(k\\) training examples that are closest to \\(\\mathbf{x}\\), then we predict the label of \\(\\mathbf{x}\\) as the average of the labels of these \\(k\\) neighbors.\nWe can visualize the \\(k\\)-nearest neighbors predictions as a kind of Voronoi diagram, where each point in the feature space is assigned to the label of its nearest neighbor(s).\n\n\n\nWe typically measure closeness via cosine similarity i.e., the cosine of the angle between two vectors. Consider two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{w}\\), the cosine similarity is: \\[\n\\cos(\\theta) = \\frac{\\langle \\mathbf{x}, \\mathbf{w} \\rangle}{\\|\\mathbf{x}\\| \\|\\mathbf{w}\\|}\n\\] where \\(\\theta\\) is the angle between the two vectors.\n\n\n\nProof of Cosine Identity\n\nConsider the triangle with vertices at the origin, \\(\\mathbf{x}\\), and \\(\\mathbf{w}\\). The lengths of the sides are \\(a = \\|\\mathbf{x}\\|_2\\), \\(b = \\|\\mathbf{w}\\|_2\\), and \\(c = \\|\\mathbf{x} - \\mathbf{w}\\|_2\\). The law of cosines tells us that: \\[\n\\cos(\\theta) = \\frac{a^2 + b^2 - c^2}{2ab}.\n\\] Then \\(a^2 + b^2 - c^2 = \\|\\mathbf{x}\\|_2^2 + \\|\\mathbf{w}\\|_2^2 - \\|\\mathbf{x} - \\mathbf{w}\\|_2^2\\). Since \\(\\|\\mathbf{x} - \\mathbf{w}\\|_2^2 = \\|\\mathbf{x}\\|_2^2 + \\|\\mathbf{w}\\|_2^2 - 2\\langle \\mathbf{x}, \\mathbf{w} \\rangle\\), we have \\[\n\\cos(\\theta) = \\frac{2 \\langle \\mathbf{x}, \\mathbf{w} \\rangle}{ 2\\|\\mathbf{x}\\|_2 \\|\\mathbf{w}\\|_2},\n\\] and the identity follows.\n\n\nThe cosine similarity identity tells us that the following are equivalent:\n- Large cosine similarity \\(\\cos(\\theta)\\).\n- Small angle \\(\\theta\\).\n- Small distance \\(\\|\\mathbf{x} - \\mathbf{w}\\|_2\\).\nLet’s apply the \\(k\\)-nearest neighbors algorithm to the MNIST dataset, which contains images of handwritten digits. Each image is a \\(28 \\times 28\\) pixel grayscale image, which we can flatten into a vector of length \\(784\\). When we compute the cosine similarity between two images, we are essentially measuring how similar the two images are based on their pixel values in each dimension. A search for the nearest neighbors will find the images that are closest in terms of pixel values, which often corresponds to similar handwriting styles.\n\n\n\nWhile quite accurate out of the box, it’s difficult to improve its accuracy because we do not have parameters to tune. Instead, we can use feature transformations to more expressively represent the data in higher dimensions.\n\n\nKernel Trick\nWe can apply feature transformations to the input data to create a new representation that captures more complex relationships. Let \\(\\phi: \\mathbb{R}^d \\to \\mathbb{R}^m\\) be a feature transformation that maps the inputs to a higher-dimensional space.\n\n\n\nFor most transformations, \\(m\\) is very large, and explicitly computing the transformation \\(\\phi(\\mathbf{x}^{(i)})\\) is infeasible. Fortunately, we can use the kernel trick to compute the inner product without explicitly computing the transformation. The kernel trick allows us to define a kernel function \\(\\mathbf{K}: \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}\\) such that \\[\n\\begin{align}\n[\\mathbf{K}]_{i,j}\n=k(\\mathbf{x}^{(i)}, \\mathbf{x}^{(j)})\n= \\langle \\phi(\\mathbf{x}^{(i)}), \\phi(\\mathbf{x}^{(j)}) \\rangle.\n\\end{align}\n\\]\nFor many years, I was confused about why the kernel trick works. Doesn’t computing the inner product require us to apply the transformation \\(\\phi\\)?\nLet’s see two examples:\nPolynomial Kernel: Consider the polynomial kernel \\(k(\\mathbf{x}, \\mathbf{x}') = (\\langle \\mathbf{x}, \\mathbf{x}' \\rangle + 1)^p\\) for \\(p \\in \\mathbb{N}\\). The explicit transformation \\(\\phi\\) when \\(p=2\\) and \\(d=3\\) is given by: \\[\n\\phi(\\mathbf{x}) = \\begin{bmatrix}\n1 \\\\\n\\sqrt{2} x_1 \\\\\n\\sqrt{2} x_2 \\\\\n\\sqrt{2} x_3 \\\\\nx_1^2 \\\\\nx_2^2 \\\\\nx_3^2 \\\\\n\\sqrt{2} x_1 x_2 \\\\\n\\sqrt{2} x_1 x_3 \\\\\n\\sqrt{2} x_2 x_3 \\\\\n\\end{bmatrix}\n\\] However, we can efficiently compute the inner product without explicitly computing \\(\\phi\\): \\[\n\\begin{align}\nK(\\mathbf{x}, \\mathbf{x}') &= (\\langle \\mathbf{x}, \\mathbf{x}' \\rangle + 1)^2\n\\\\&= (1 + x_1 x_1' + x_2 x_2' + x_3 x_3')^2\n\\\\&= 1 + 2x_1 x_1' + 2x_2 x_2' + 2x_3 x_3' + x_1^2 x_1'^2 + x_2^2 x_2'^2\n\\\\& + x_3^2 x_3'^2 + 2x_1 x_2 x_1' x_2' + 2x_1 x_3 x_1' x_3' + 2x_2 x_3 x_2' x_3'\n\\\\&= \\langle \\phi(\\mathbf{x}), \\phi(\\mathbf{x}') \\rangle.\n\\end{align}\n\\]\nGaussian Kernel: Consider the Gaussian kernel \\(K(\\mathbf{x}, \\mathbf{x}') = e^{-\\frac{\\|\\mathbf{x} - \\mathbf{x}'\\|_2^2}{2\\sigma^2}}\\) for some \\(\\sigma &gt; 0\\). As shown here, the Gaussian kernel can be expressed as an infinite series. Instead of explicitly computing the transformation \\(\\phi\\), we can compute the kernel in \\(O(d)\\) time by computing the inner product \\(\\langle \\mathbf{x}, \\mathbf{x}' \\rangle\\), and then applying the exponential function with a scaling.\nWe have seen how to apply the kernel trick to the \\(k\\)-nearest neighbors algorithm, which allows us to compute the inner product in a higher-dimensional space without explicitly computing the feature transformation. Let’s see how we can apply the kernel trick to a more familiar tool: multi-class classification.\n\n\nReparameterization Trick\nLet \\(q\\) be the number of classes. We will learn \\(q\\) weight vectors \\(\\mathbf{w}^{(1)}, \\ldots, \\mathbf{w}^{(q)} \\in \\mathbb{R}^d\\). Our final prediction is the class with the highest value (we previously normalized these outputs via softmax so that we could interpret them as probabilities): \\[\n\\arg \\max_{\\ell \\in [q]} \\langle \\mathbf{w}^{(\\ell)}, \\mathbf{x} \\rangle.\n\\] Equivalently, we can view the problem as a similarity search, where we want to find the class with smallest distance: \\[\n\\arg \\min_{\\ell \\in [q]} \\|\\mathbf{w}^{(\\ell)} - \\mathbf{x}\\|_2^2.\n\\]\nIn the context of MNIST, the weight vectors look something like this:\n\n\n\nWhile vaguely resembling the digits, they are not very interpretable, and it’s unclear how much they capture the underlying structure of the data. We would really like to represent the data in a more expressive way without explicitly computing the feature transformation, similar to the kernel trick we applied to the \\(k\\)-nearest neighbors algorithm.\nFor notational simplicity, consider binary logistic regression, where we have a single weight vector \\(\\mathbf{w} \\in \\mathbb{R}^d\\). The optimization problem is to find \\[\n\\arg \\min_{\\mathbf{w} \\in \\mathbb{R}^{d}} \\mathcal{L}(\\sigma(\\mathbf{Xw}), \\mathbf{y})\n\\] \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) is the input data matrix, \\(\\mathbf{y} \\in \\{0,1\\}^n\\) is the label vector, \\(\\mathcal{L}\\) is the binary cross-entropy loss, and \\(\\sigma\\) is the entrywise softmax function. But it’s unclear how to apply the kernel trick here, since we are not computing the inner product between two feature vectors, but rather between a feature vector and a weight vector.\nReparameterization Trick: We can equivalently represent the weight vector as a linear combination of the training examples: \\[\n\\mathbf{w} = \\sum_{i=1}^n \\alpha_i \\mathbf{x}^{(i)}\n\\] for some coefficients \\(\\alpha_i \\in \\mathbb{R}\\).\n\n\n\nProof of Reparameterization Trick\n\nSuppose for contradiction that \\(\\mathbf{w}\\) is not in the span of the rows of \\(\\mathbf{X}\\). Then we can write \\(\\mathbf{w} = \\mathbf{v} + \\mathbf{u}\\), where \\(\\mathbf{v}\\) is in the span of the rows of \\(\\mathbf{X}\\) and \\(\\mathbf{u}\\) is orthogonal to the span of the rows of \\(\\mathbf{X}\\). But then \\(\\mathbf{X w} = \\mathbf{Xv} + \\mathbf{Xu} = \\mathbf{Xv}\\), since \\(\\mathbf{Xu} = 0\\), so it suffices to optimize over \\(\\mathbf{v}\\).\n\n\nThe reparameterization trick tells us that we can equivalently represent the weight vector in the span of the rows of the input data matrix \\(\\mathbf{X}\\).\n\n\n\nWith the reparameterization trick in hand, we can represent the weight vector as \\(\\mathbf{X}^\\top \\mathbf{v}\\). The optimization problem becomes: \\[\n\\arg \\min_{\\mathbf{v} \\in \\mathbb{R}^{n}} \\mathcal{L}(\\sigma(\\mathbf{X X}^\\top \\mathbf{v}), \\mathbf{y}).\n\\]\nNow, we can apply the kernel trick to compute the inner products of \\(\\mathbf{X X^\\top}\\). At inference time, we can similarly use the kernel trick to compute the inner product between the input vector \\(\\mathbf{x}\\) and the weight vector \\(\\mathbf{X}^\\top \\mathbf{v}\\).\nThe reparameterization trick allows us to expressively represent data without explicitly computing the feature transformation. We can also apply the trick to other linear models such as linear regression.\nWhile they allow us to expressively represent data without explicitly computing the feature transformation, the kernel and reparameterization tricks are still computationally expensive: We need to compute the \\(n \\times n\\) kernel matrix \\(\\mathbf{K}\\).\nWe will next see how neural networks allow us to automatically learn feature transformations, while basically maintaining the training and inference efficiency of linear models."
  },
  {
    "objectID": "notes/GradientDescent.html",
    "href": "notes/GradientDescent.html",
    "title": "Gradient Descent and Polynomial Regression",
    "section": "",
    "text": "Today, we continue our discussion of supervised learning, where we have labeled training data and our goal is to train a model that accurately predicts the labels of unseen testing data. Recall that our general approach to supervised learning is to use empirical risk minimization: We focus on a class of models, define a loss function that quantifies how accurately a particular model explains the training data, and search for a model with low loss.\nLast week, we considered the class of linear models, i.e., the prediction is a weighted sum of the input features. We chose to measure loss via mean squared error, a choice both rooted in convenience and a compelling modeling assumption (if the data is generated by a model in our class plus some Gaussian noise, then the mean squared error is the maximum likelihood estimator). In order to find the best parameters of the linear model, we used our knowledge of gradients to exactly compute the parameters that minimize the mean squared error loss.\nThis week, we will address two of the nagging issues with computing the best parameters of a linear model. We begin with the issue of runtime; computing the optimal parameters requires building a large matrix and inverting it, which takes \\(O(nd^2 + d^3)\\) time, where \\(n\\) is the number of data points and \\(d\\) is the number of features. We will now see how we can use gradient descent to speed up this process.\n\nGradient Descent\nThe mean squared error of a linear model is particularly well-behaved because it is convex i.e., there is a single minimum. Previously, we computed the parameters where the gradient is 0, which, by convexity, immediately gave us the single optimal point. However, we could instead use a more relaxed approach; rather than jumping immediately to the best parameters, we can iterate towards better parameters by taking steps towards lower loss.\n\n\n\nGradient descent is an iterative method for moving in the direction of steepest descent. Concretely, the process produces a sequence of parameters \\(\\mathbf{w}^{(1)}, \\mathbf{w}^{(2)}, \\ldots\\). At each step, we compute the direction of steepest ascent i.e., the gradient of the loss function with respect to each parameter. The gradient quantifies how the loss function responds as we tweak each parameter. If the partial derivative is positive (increasing the parameter increases the loss), then we will want to decrease the parameter. Analogously, if the partial derivative is negative (increasing the parameter decreases the loss), then we will want to increase the parameter. In both cases, we are moving in the direction away from the gradient. Hence, we reach the next parameter vector by subtracting the gradient from the current parameter vector: \\[\n\\begin{align*}\n\\mathbf{w}^{(t+1)} \\gets \\mathbf{w}^{(t)} - \\alpha \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}^{(t)}),\n\\end{align*}\n\\] where \\(\\alpha\\) is a small positive constant called the step size or learning rate.\nNotice that, for one, this approach stops when we reach a point where the gradient is 0, i.e., we have reached a local minimum.\nBeyond the stopping condition, why does this work? Consider the one dimensional setting. The derivative of the loss function is \\[\n\\begin{align*}\n\\frac{\\partial \\mathcal{L}(w)}{\\partial w} = \\lim_{\\Delta \\to 0} \\frac{\\mathcal{L}(w + \\Delta) - \\mathcal{L}(w)}{\\Delta}.\n\\end{align*}\n\\] so, for small \\(\\Delta\\), we can approximate the loss function as \\[\n\\begin{align*}\n\\mathcal{L}(w+\\Delta) - \\mathcal{L}(w) &\\approx \\frac{\\partial \\mathcal{L}(w)}{\\partial w} \\cdot \\Delta \\\\\n\\end{align*}\n\\] We want \\(\\mathcal{L}(w+\\Delta)\\) to be smaller than \\(\\mathcal{L}(w)\\), so we want \\(\\frac{\\partial \\mathcal{L}(w)}{\\partial w} \\Delta &lt; 0\\). This can be achieved by setting \\(\\Delta = -\\alpha \\frac{\\partial \\mathcal{L}(w)}{\\partial w}\\), where \\(\\alpha\\) is a small positive constant. Then \\(w^{(t+1)} = w^{(t)} - \\alpha \\frac{\\partial \\mathcal{L}(w^{(t)})}{\\partial w}\\) is a step in the direction of descent.\nIn the multi-dimensional setting, the partial derivative of the loss function with respect to each parameter is given by the gradient: \\[\n\\frac{\\partial \\mathcal{L}}{\\partial w_i} = \\lim_{\\Delta \\to 0} \\frac{\\mathcal{L}(\\mathbf{w} + \\Delta \\mathbf{e}_i) - \\mathcal{L}(\\mathbf{w})}{\\Delta},\n\\] where \\(\\mathbf{e}_i\\) is the \\(i\\)th standard basis vector. Then, for small \\(\\Delta\\), we can approximate the loss function as \\[\n\\mathcal{L}(\\mathbf{w} + \\Delta \\mathbf{e}_i) - \\mathcal{L}(\\mathbf{w}) \\approx \\frac{\\partial \\mathcal{L}}{\\partial w_i} \\cdot \\Delta\n= \\langle \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}), \\Delta \\mathbf{e}_i \\rangle.\n\\] For a general vector \\(\\mathbf{v}\\), we can write \\[\n\\mathcal{L}(\\mathbf{w} + \\Delta \\mathbf{v}) - \\mathcal{L}(\\mathbf{w}) \\approx \\langle \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}), \\Delta \\mathbf{v} \\rangle.\n\\] If we want to move in the direction of steepest descent, we can set \\(\\Delta \\mathbf{v} = -\\alpha \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w})\\), where \\(\\alpha\\) is a small positive constant. Then, we have \\(\\mathcal{L}(\\mathbf{w} - \\alpha \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w})) - \\mathcal{L}(\\mathbf{w}) \\approx -\\alpha \\langle \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}), \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}) \\rangle = -\\alpha \\|\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w})\\|^2\\).\nCould we choose a better direction \\(\\mathbf{v}'\\) to move in? Well, recall for any vectors \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\), we have \\(\\langle \\mathbf{a}, \\mathbf{b} \\rangle = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos(\\theta)\\), where \\(\\theta\\) is the angle between the two vectors. The largest value of \\(\\cos(\\theta)\\) is \\(1\\), which occurs when the two vectors are in the same direction. Notice we achieve the largest magnitude of the inner product when we take the step in the direction of the gradient, i.e., \\(- \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w})\\).\nFor linear models, we already know the gradient of the mean squared error loss: \\[\n\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}) = \\frac2{n} \\mathbf{X}^\\top (\\mathbf{X w - y}).\n\\] In contrast to the \\(O(nd^2 + d^3)\\) time required to compute the exact solution, we can now compute the gradient in \\(O(nd)\\) time, as long as we restrict ourselves to matrix-vector multiplications rather than matrix-matrix multiplications. The final time complexity of gradient descent is \\(O(T nd)\\), where \\(T\\) is the number of iterations of gradient descent.\nWhile we have achieved a significant speedup, \\(O(T nd)\\) could still be prohibitively large when we have a large number of data points \\(n\\) and/or a large number of features \\(d\\). Our solution will be a stochastic approach, where we only use a small random subset of the data to compute the gradient.\n\n\nStochastic Gradient Descent\nOur approach will be similar to gradient descent, except now we will compute the gradient using only the data in the batch. For the mean squared error loss, we can write the loss function for a random subset \\(S\\) of the data as \\[\n\\mathcal{L}_S(\\mathbf{w}) = \\frac1{|S|} \\sum_{i \\in S} (f(\\mathbf{x}^{(i)}) - y^{(i)})^2.\n\\] Then, for our linear model, the gradient of the loss function with respect to the parameters \\(\\mathbf{w}\\) is given by \\[\n\\nabla_\\mathbf{w} \\mathcal{L}_S(\\mathbf{w}) = \\frac2{|S|} \\mathbf{X}_S^\\top (\\mathbf{X}_S \\mathbf{w} - \\mathbf{y}_S),\n\\] where \\(\\mathbf{X}_S\\) is the data matrix for the subset \\(S\\) and \\(\\mathbf{y}_S\\) is the target vector for the subset \\(S\\). One iteration of stochastic gradient descent takes time \\(O(|S|d)\\), which can be much faster than the \\(O(nd)\\) time required to compute the gradient for the full dataset.\n\n\nAdaptive Step Sizes\nThe step size \\(\\alpha\\) is a crucial hyperparameter in gradient descent. If \\(\\alpha\\) is too small, then the algorithm will take a long time to converge because it will take small steps towards the minimum. If \\(\\alpha\\) is too large, then the algorithm may overshoot the minimum and diverge by repeatedly moving in the right direction but by too much. Instead, we want to choose a step size \\(\\alpha\\) that is just right, allowing us to make progress towards the minimum without overshooting.\n\n\n\nThere are several strategies for choosing the step size:\n\nWhen searching manually, we often exponentially increase and decrease the step size e.g., multiply by a factor of \\(2\\) or \\(1/2\\). If the loss consistently improves over several iterations of gradient descent, then we try increasing the step size; if the loss is unstable, then we try decreasing the step size.\nLearning rate schedules offer a more automated approach, where we start with a large step size and then decrease it over time. This is often done by multiplying the step size by a factor less than \\(1\\) after each iteration. The intuition is that we want to take large steps at the beginning to quickly find a good region of the parameter space, and then take smaller steps so as to not overshoot the minima as we get closer.\nAn even more automated approach is to use an adaptive learning rate, where we adjust the step size based on the gradient. If the gradient is large, then we can decrease the step size to avoid overshooting; if the gradient is small, then we can increase the step size to speed up convergence. One implementation of this idea is as follows: \\[\n\\alpha^{(t+1)} \\gets \\frac{\\alpha^{(t)}}{(\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}^{(t)}))^2}.\n\\] Notice that the division is element-wise, so we are adjusting the step size for each parameter individually based on the squared partial derivative of that parameter.\n\nIn addition the step size, the direction of each step is also important.\n\n\nMomentum\nThe idea of gradient descent is to converge to a local minimum of the loss function, but things can go wrong even if we have the right step size: The gradient may not point in the direction of the minima if, for example, the loss function is not symmetric. The plot illustrates this issue for a convex loss function on two parameters \\({w}_1\\) and \\({w}_2\\), where the loss function is given by level sets. In the plot, a standard gradient descent approach takes many steps but the directions cancel out, resulting in a zig-zag pattern that slows convergence.\n\n\n\nOur solution is to keep track of the direction we have been moving in and use that to inform our next step. This idea, called momentum, retains a running average of the gradients, which allows us to smooth out the direction of the steps. We can think of momentum as a ball rolling down a hill, even when the ball is pushed left or right, it will continue to roll downwards. An implementation of momentum is as follows: \\[\n\\begin{align}\n\\mathbf{m}^{(t+1)} &= \\beta \\mathbf{m}^{(t)} + (1 - \\beta) \\nabla_\\mathbf{w} \\mathcal{L}_S(\\mathbf{w}^{(t)}) \\\\\n\\mathbf{w}^{(t+1)} &= \\mathbf{w}^{(t)} - \\alpha \\mathbf{m}^{(t+1)},\n\\end{align}\n\\] where \\(\\beta\\) is a hyperparameter that controls the amount of history we keep in the momentum vector \\(\\mathbf{m}^{(t)}\\)."
  },
  {
    "objectID": "notes/DecisionTrees.html",
    "href": "notes/DecisionTrees.html",
    "title": "Decision Trees and Boosting",
    "section": "",
    "text": "Feature transformations and neural networks are powerful tools for supervised learning, but they can be difficult to interpret. Today, we’ll consider a more intuitive model class called trees, which, until very recently, gave the best performance on many supervised learning tasks.\n\nDecision Trees\nDecision trees are a simple yet powerful model class that can be used for both classification and regression tasks. For simplicity, we’ll define them in terms of classification, but the same ideas apply to regression.\nDecision trees work by recursively partitioning the feature space into regions that are as homogeneous as possible with respect to the target variable.\n\n\n\nThe tree structure is incredibly intuitive, and likely the most natural way to represent a decision-making process. At each internal node, the tree splits the data based on a feature and a threshold. The goal is to find the split that best (more on this later) separates the classes.\nLike gradient descent, decision trees use a greedy approach to learning. As we build the tree, we recursively evaluate the data at each leaf, choosing the feature and threshold that best separates the classes. Let \\(\\ell\\) be a leaf after the split at the prior leaf, and \\(p^{(\\ell)}_c\\) be the proportion of points in leaf \\(\\ell\\) that are of class \\(c\\). Without loss of generality, suppose that \\(c=0\\) is the majority class. (Soon, we’ll consider weighted points, in which case we will define the weighted majority.) If we reach leaf \\(\\ell\\), we predict that the point belongs to the majority class.\nThe question is what split minimizes the loss of the resulting predictions. In particular, we’d like to minimize the expected loss of the predictions where the expectation is taken over the proportion of points that make it to the leaf \\(p^{(\\ell)}\\): \\[\n\\mathbb{E}_\\ell[\\mathcal{L}(\\ell)] = \\sum_\\ell \\mathcal{L}(\\ell) \\cdot p^{(\\ell)}.\n\\] There are several ways to define the loss of a leaf, but several common ones are:\nError Rate: The error rate is the proportion of points in the leaf that are not of the predicted majority class. That is, \\[\n\\mathcal{L}_\\text{error}(\\ell) = 1 - p^{(\\ell)}_0.\n\\]\nGini Impurity: The Gini impurity measures the probability of misclassifying a randomly chosen point from the leaf if we were to randomly assign it to a class according to the class proportions. That is, \\[\n\\mathcal{L}_\\text{Gini}(\\ell) = \\sum_{c} p^{(\\ell)}_c (1 - p^{(\\ell)}_c).\n\\] With a little algebra, we can rewrite this as: \\[\n\\mathcal{L}_\\text{Gini}(\\ell) = \\sum_{c} p^{(\\ell)}_c - \\left(p^{(\\ell)}_c\\right)^2 = 1 - \\sum_{c} \\left(p^{(\\ell)}_c\\right)^2.\n\\]\nInformation Gain: Like logistic regression, information gain uses the entropy of the leaf to measure the error. That is, \\[\n\\mathcal{L}_\\text{InfoGain}(\\ell) = H(\\ell) = -\\sum_{c} p^{(\\ell)}_c \\log(p^{(\\ell)}_c).\n\\] Since the entropy is a measure of uncertainty, we call the reduction in entropy from the prior leaf to the resulting leaves after the split the information gain.\nPreviously, we updated the parameters of our model to minimize the loss of our predictions. In decision trees, we search over all possible splits of the data to find the one that minimizes the expected loss. While this sounds expensive, in practice, there are a limited number of feasible splits.\nWe’ve discussed decision trees in terms of classification, but they can also be used for regression tasks. What prediction would a leaf make in a regression task? How could we measure the loss of this prediction?\nOn their own, decision trees are not particularly expressive. But, they can be boosted to create remarkably powerful models. A boosted model is a collection of weaker learners that together make the final prediction.\n\n\nAdaptive Boosting (AdaBoost)\nIn adaptive boosting, we iteratively train a model to improve the composite model we’ve built so far. For simplicity, we’ll explore adaptive boosting in the context of binary classification, but the method can be generalized to regression tasks.\nLet \\(f_t : \\mathbb{R}^d \\to \\{-1, 1\\}\\) be the model learned at iteration \\(t\\). The combined prediction is given by an ensemble model \\(F_t : \\mathbb{R}^d \\to \\mathbb{R}\\). The ensemble model is defined as a weighted sum of the predictions of the individual models: \\[\nF_t(x) = \\alpha_1 f_1(x) + \\alpha_2 f_2(x) + \\ldots + \\alpha_t f_t(x)\n= F_{t-1}(x) + \\alpha_t f_t(x),\n\\] where \\(\\alpha_k &gt; 0\\) is a weight that determines the importance of the model \\(f_k\\) in the ensemble.\nGiven \\(F_{t-1}\\), we are interested in learning a new model \\(f_t\\) and a new weight \\(\\alpha_t\\) that minimizes the loss of the composite model. We’ll define the loss as \\[\n\\mathcal{L}(F_t) = \\sum_{i=1}^n e^{-y^{(i)} F_t(\\mathbf{x}^{(i)})},\n\\] where \\(y^{(i)} \\in \\{-1, 1\\}\\) is the true label of the \\(i\\)-th training example. When the composite model output is the same sign as the true label, the term in the exponent is negative, and the contribution to the error is less than 1. When the composite model output is the opposite sign as the true label, the term in the exponent is positive, and the contribution to the loss is (potentially much) greater than 1.\nUsing the multiplication property of exponents, and the definition of \\(F_t\\), we can rewrite the loss as \\[\n\\mathcal{L}(F_t) = \\sum_{i=1}^n e^{-y^{(i)} (F_{t-1}(\\mathbf{x}^{(i)}) + \\alpha_t f_t(\\mathbf{x}^{(i)}))}\n= \\sum_{i=1}^n e^{-y^{(i)} F_{t-1}(\\mathbf{x}^{(i)})} e^{-y^{(i)} \\alpha_t f_t(\\mathbf{x}^{(i)})}.\n\\] Let \\(w^{(i)}_{t-1} = e^{-y^{(i)} F_{t-1}(\\mathbf{x}^{(i)})}\\) be the weight of the \\(i\\)th training example at iteration \\(t\\). Notice that these weights are independent of the new model \\(f_t\\) and the new weight \\(\\alpha_t\\). Then, partitioning the summation, we can write the error as \\[\n\\begin{align}\n\\mathcal{L}(F_t) &=\n\\sum_{i: y^{(i)} = f_t(\\mathbf{x}^{(i)})} w^{(i)}_{t-1} e^{-\\alpha_t} + \\sum_{i: y^{(i)} \\neq f_t(\\mathbf{x}^{(i)})} w^{(i)}_{t-1} e^{\\alpha_t}\n\\\\&= \\sum_{i=1}^n w^{(i)}_{t-1} e^{-\\alpha_t} + \\sum_{i: y^{(i)} \\neq f_t(\\mathbf{x}^{(i)})} w^{(i)}_{t-1} (e^{\\alpha_t} - e^{-\\alpha_t})\n\\\\&= \\sum_{i=1}^n w^{(i)}_{t-1} e^{-\\alpha_t} + (e^{\\alpha_t} - e^{-\\alpha_t}) \\sum_{i: y^{(i)} \\neq f_t(\\mathbf{x}^{(i)})} w^{(i)}_{t-1}.\n\\end{align}\n\\]\nWith \\(F_{t-1}\\) fixed, minimizing \\(\\mathcal{L}(F_t)\\) is equivalent to minimizing the weighted error of the new model \\(f_t\\) i.e., the sum of the weights for all misclassified points. Once we learn a decision tree \\(f_t\\) that minimizes the weighted error, we can find the optimal weight \\(\\alpha_t\\) by differentiating the loss with respect to \\(\\alpha_t\\) and setting it to zero. In particular, we have \\[\n\\frac{\\partial \\mathcal{L}(F_t)}{\\partial \\alpha_t}\n= \\sum_{i : y^{(i)} = f_t(\\mathbf{x}^{(i)})} w^{(i)}_{t-1} (-e^{-\\alpha_t})\n+ \\sum_{i : y^{(i)} \\neq f_t(\\mathbf{x}^{(i)})} w^{(i)}_{t-1} e^{\\alpha_t}\n= 0\n\\] which tells us that \\[\ne^{-\\alpha_t} \\sum_{i : y^{(i)} = f_t(\\mathbf{x}^{(i)})} w^{(i)}_{t-1}\n= e^{\\alpha_t} \\sum_{i : y^{(i)} \\neq f_t(\\mathbf{x}^{(i)})} w^{(i)}_{t-1}.\n\\] Taking the logarithm of both sides, we have: \\[\n-\\alpha_t + \\log\\left(\\sum_{i : y^{(i)} = f_t(\\mathbf{x}^{(i)})} w^{(i)}_{t-1}\\right)\n= \\alpha_t + \\log\\left(\\sum_{i : y^{(i)} \\neq f_t(\\mathbf{x}^{(i)})} w^{(i)}_{t-1}\\right).\n\\] Rearranging, we have \\[\n\\alpha_t = \\frac{1}{2} \\log\\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right)\n\\] where \\(\\epsilon_t = \\frac{\\sum_{i : y^{(i)} \\neq f_t(\\mathbf{x}^{(i)})} w^{(i)}_{t-1}}{\\sum_{i=1}^n w^{(i)}_{t-1}}\\) is the weighted error rate of the model \\(f_t\\).\nAt each iteration \\(t\\), we compute the weights \\(w^{(i)}_{t-1}\\) based on which points the current ensemble model \\(F_{t-1}\\) are misclassifying. We then learn a new model \\(f_t\\) that minimizes the weighted error, adapting to correct the mistakes of the current ensemble. Finally, we compute the optimal weight \\(\\alpha_t\\) for the new model based on its weighted error rate.\n\n\n\nThe final prediction algorithm combines each of the weak learners.\n\n\n\nThe AdaBoost algorithm is agnostic to the choice of weak learner, but decision trees are efficient and interpretable: Each weak learner can be trained quickly, and the resulting decision tree can be understood as a weighted vote of the individual trees.\n\n\nGradient Boosting\nRemarkably, AdaBoost can be viewed as gradient descent where the loss function is defined over the predictions on points rather than the parameters of the model.\nLet \\(\\mathcal{L}\\) be a differentiable loss function defined over the predictions of the model on the training data. In AdaBoost, we used the exponential loss, but we could use other loss functions like squared error or logistic loss. Rather than considering weak learners that are binary classifiers, we will now consider real-valued functions \\(f_t : \\mathbb{R}^d \\to \\mathbb{R}\\).\nAs before, the combined prediction is given by an ensemble model \\(F_t : \\mathbb{R}^d \\to \\mathbb{R}\\). The ensemble model is defined as a weighted sum of the predictions of the individual models: \\[\nF_t(x) = \\alpha_1 f_1(x) + \\alpha_2 f_2(x) + \\ldots + \\alpha_t f_t(x) = F_{t-1}(x) + \\alpha_t f_t(x).\n\\]\nOur goal is to find a weak learner \\(f_t\\) that minimizes the loss \\[\n\\arg \\min_{f_t} \\sum_{i=1}^n \\mathcal{L}(y^{(i)}, F_{t-1}(\\mathbf{x}^{(i)}) + f_t(\\mathbf{x}^{(i)})).\n\\] Recall that the gradient descent update subtracts the gradient of the parameter (opposite the direction of steepest ascent) for a small learning rate \\(\\alpha\\). Instead of updating the parameters of the model, we will update the predictions of the model. In our boosting language, the gradient descent update for the \\(i\\)th prediction is \\[\nF_t(\\mathbf{x}^{(i)}) = F_{t-1}(\\mathbf{x}^{(i)}) - \\alpha \\frac{ \\partial \\mathcal{L}(y^{(i)}, F_{t-1}(\\mathbf{x}^{(i)}))}{\\partial F_{t-1}(\\mathbf{x}^{(i)})}.\n\\] In practice, finding a function that exactly matches the gradient is difficult, so we will instead find a function that approximates the gradient. That is, we train a weak learner \\(f_t\\) to fit the points \\((\\mathbf{x}^{(i)}, \\frac{ \\partial \\mathcal{L}(y^{(i)}, F_{t-1}(\\mathbf{x}^{(i)}))}{\\partial F_{t-1}(\\mathbf{x}^{(i)})})\\) for \\(i \\in \\{1, \\ldots n\\}\\). When the loss is the squared error loss, i.e., \\[\n\\mathcal{L}(y, F_{t-1}(\\mathbf{x})) = \\frac12 (F_{t-1}(\\mathbf{x}) - y)^2,\n\\] the gradient of the loss with respect to the prediction is simply the residual between the prior prediction and the label, i.e., \\[\n\\frac{ \\partial \\mathcal{L}(y, F_{t-1}(\\mathbf{x}))}{\\partial F_{t-1}(\\mathbf{x})} = F_{t-1}(\\mathbf{x}) - y.\n\\] In this, gradient boosting has a nice interpretation as an iterative process of fitting a model to the residuals of the prior model. Once we have trained the weak learner \\(f_t\\), we can select the weight \\(\\alpha_t\\) to minimize the loss of the composite model: \\[\n\\alpha_t = \\arg \\min_\\alpha \\sum_{i=1}^n \\mathcal{L}(y^{(i)}, F_{t-1}(\\mathbf{x}^{(i)}) + \\alpha f_t(\\mathbf{x}^{(i)})).\n\\] When the loss is differentiable and convex (e.g., mean squared error), we can find the optimal weight \\(\\alpha_t\\) by differentiating the loss with respect to \\(\\alpha_t\\) and setting it to zero.\nThe gradient boosting algorithm called XGBoost is a popular implementation. Since 2014, XGboost has basically been the best performing model on many supervised learning tasks. However, foundation models like TabPFN are starting to outperform XGBoost on small to medium-sized datasets. Like modern models, TabPFN uses a transformer architecture, and is trained on some 130 million synthetic datasets. While XGBoost requires relearning an ensemble for each new dataset, TabPFN can be run without any training by simply passing labeled example data, and unlabeled validation data as input to the model."
  },
  {
    "objectID": "notes/LogisticRegression.html",
    "href": "notes/LogisticRegression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "With a little probability, we saw how the Naive Bayes Classifier can be used to make predictions. However, the Naive Bayes Classifier assumes that the value of each feature is independent of the others, which is often not the case in practice (e.g., if the word “discount” appears in an email, it is more likely that the word “sale” also appears). As an alternative approach to classification problems, we will see how we can generalize linear regression, with a little non-linearity.\nOur setup will be the standard supervised learning setting, where we have labelled data \\((\\mathbf{x}^{(1)}, y^{(1)}), \\ldots, (\\mathbf{x}^{(n)}, y^{(n)})\\), for \\(\\mathbf{x}^{(i)} \\in \\mathbb{R}^d\\) the feature vector for the \\(i\\)th example. However, unlike regression where we predict a continuous value \\(y^{(i)} \\in \\mathbb{R}\\), we will now predict a binary value \\(y^{(i)} \\in \\{0, 1\\}\\). We can use the same linear model as before, i.e., we will predict the output as \\(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle\\), where \\(\\mathbf{w} \\in \\mathbb{R}^d\\) is the weight vector. But, we run into an issue: the output of the linear model can take on any real value, but we want to predict a binary value.\n\nModel and Loss\nWe’ll explore several attempts to convert our linear model into a binary classifier.\nAttempt #1: We could simply apply a step function to the output of the linear model, i.e., predict \\(y^{(i)} = 1\\) if \\(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle &gt; 0\\) and \\(y^{(i)} = 0\\) otherwise. The loss could be the difference between the predicted value and the true value, i.e., \\(\\mathcal{L}(\\mathbf{w}) = \\sum_{i=1}^n |y^{(i)} - \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle|\\). However, this loss is not differentiable, so we cannot use gradient descent to optimize it.\nAttempt #2: We could use the mean squared error loss, i.e., \\(\\mathcal{L}(\\mathbf{w}) = \\sum_{i=1}^n (y^{(i)} - \\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)^2\\). This loss is differentiable, but it does not work well for classification problems: if we have a large positive value for \\(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle\\), the loss will be large even if \\(y^{(i)} = 1\\).\nAttempt #3: We can apply the sigmoid function to the output of the linear model to map it to the range \\((0, 1)\\), i.e., we will predict \\(f(\\mathbf{x}^{(i)}) = \\sigma(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)\\), where \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. The sigmoid function is a smooth, non-linear function that maps any real number to the range \\((0, 1)\\). We can then interpret \\(f(\\mathbf{x}^{(i)})\\) as the probability that \\(y^{(i)} = 1\\) given the features \\(\\mathbf{x}^{(i)}\\). If we need to report a class label, we can threshold the predicted probability, e.g., predict \\(y^{(i)} = 1\\) if \\(f(\\mathbf{x}^{(i)}) &gt; \\frac12\\) and \\(y^{(i)} = 0\\) otherwise.\n\n\n\nTo train our model, we need a loss function that measures how well our predicted probabilities match the true labels. A common choice is the binary cross-entropy loss, which is defined as \\[\\mathcal{L}(\\mathbf{w}) = - \\sum_{i=1}^n \\left[y^{(i)} \\log(f(\\mathbf{x}^{(i)})) + (1 - y^{(i)}) \\log(1 - f(\\mathbf{x}^{(i)}))\\right].\\] This loss function measures the distance (in a sense we’ll explore later in the course) between the predicted probabilities and the true labels. It is differentiable, so we can use gradient descent to optimize it.\n\n\n\n\n\nOptimization\nOnce we have a model and loss function, we have seen two ways to optimize the model: Exact optimization, where we compute the gradient of the loss function with respect to the model parameters and set the gradient to zero to find the optimal parameters. Gradient descent, where we iteratively update the model parameters in the direction of the negative gradient of the loss function. Both approaches require the gradient of the loss function with respect to the model parameters, so let’s compute the gradient of \\(\\mathcal{L}(\\mathbf{w})\\) with respect to \\(\\mathbf{w}\\).\nPlugging in the sigmoid function, we have \\[\n\\begin{align*}\n\\mathcal{L}(\\mathbf{w}) &= - \\sum_{i=1}^n \\left[y^{(i)} \\log\\left(\\frac{1}{1 + e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}}\\right) + (1 - y^{(i)}) \\log\\left(1 - \\frac{1}{1 + e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}}\\right)\\right] \\\\\n\\end{align*}\n\\]\nObserve that \\(\\frac1{1 + e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}} = \\frac{e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}}{e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} + 1}\\), so \\(1-\\frac1{1 + e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}} = \\frac{1}{e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} + 1}\\).\nThen, we can rewrite the loss as \\[\n\\begin{align*}\n\\mathcal{L}(\\mathbf{w}) &= \\sum_{i=1}^n \\left[y^{(i)} \\log\\left(1+e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} \\right) + (1 - y^{(i)}) \\log\\left(e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} + 1\\right) \\right].\n\\end{align*}\n\\]\nLet’s compute the partial derivative of the loss with respect to \\(w_j\\). Applying the chain rule, we have \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial w_j} \\mathcal{L}(\\mathbf{w}) &= \\sum_{i=1}^n\n\\left[y^{(i)} \\frac1{1 + e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}} \\cdot e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} (-x_j^{(i)}) + (1 - y^{(i)}) \\frac1{e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} + 1} \\cdot e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} x_j^{(i)}\\right]\\\\\n\\end{align*}\n\\]\nObserve that \\(\\frac{e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}}{1 + e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}} = \\frac{1+e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}}{1 + e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}} - \\frac1{1 + e^{-\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}} = 1-\\sigma(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)\\), and \\(\\frac{e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle}}{e^{\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle} + 1} = \\sigma(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)\\). Then, we can rewrite the partial derivative as \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial w_j} \\mathcal{L}(\\mathbf{w}) &= \\sum_{i=1}^n\n\\left[-y^{(i)} (1 - \\sigma(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle)) x_j^{(i)} + (1 - y^{(i)}) \\sigma(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle) x_j^{(i)}\\right]\\\\\n&= \\sum_{i=1}^n\nx_j^{(i)} \\left[\\sigma(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle) - y^{(i)}\\right]\\\\\n&= \\mathbf{X}_j^\\top \\left(\\sigma(\\mathbf{X} \\mathbf{w}) - \\mathbf{y}\\right),\n\\end{align*}\n\\] where \\(\\sigma(\\cdot)\\) is applied element-wise to the vector \\(\\mathbf{X} \\mathbf{w}\\), and \\(\\mathbf{X}_j\\) is the \\(j\\)th column of the design matrix \\(\\mathbf{X}\\). Finally, we can write the gradient of the loss with respect to the weight vector \\(\\mathbf{w}\\) as \\[\n\\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}) =  \\mathbf{X}^\\top \\left(\\sigma(\\mathbf{X} \\mathbf{w}) - \\mathbf{y}\\right).\n\\]\nOur exact optimization approach would be to set the gradient to zero and solve for \\(\\mathbf{w}\\). Do you see why this doesn’t work with the non-linear sigmoid function?\nInstead of exact optimization, we will use gradient descent!\n\n\nNon-linear Transformations\nOften, our data is not linearly separable, i.e., we cannot draw a straight line to separate the two classes. In this case, we can use non-linear transformations to map the data to a higher-dimensional space, where it is linearly separable. One approach: As we saw for linear regression, we can add polynomial features to the data. In the image below, we add a new feature \\(x_1^2 + x_2^2\\) to the data, which allows us to separate the two classes with a linear decision boundary in the transformed feature space.\n\n\n\nIt is not a priori clear which non-linear transformation will work best for a given dataset. In several lectures, we will explore how to use kernel methods to implicitly map the data to a higher-dimensional space, which captures many of the non-linear transformations we might want to use.\n\n\nMeasuring Error in Binary Classification\nThe simplest way to measure the error of a classification model is to compute the error rate, which is the fraction of examples that are misclassified. For example, if we have \\(n\\) examples and our model misclassifies \\(k\\) of them, the error rate is \\(\\frac{k}{n}\\).\nHowever, the error rate does not take into account which points are misclassified. We will often break down the accuracy of a classification model into four categories:\n- True Positives (TP): The model correctly predicts a positive class.\n- True Negatives (TN): The model correctly predicts a negative class.\n- False Positives (FP): The model incorrectly predicts a positive class when the true class is negative.\n- False Negatives (FN): The model incorrectly predicts a negative class when the true class is positive.\n\n\n\nThe raw counts of these four categories can be summarized in a confusion matrix. The confusion matrix is a square matrix with dimensions equal to the number of classes, where the rows represent the true classes and the columns represent the predicted classes.\nBut, these raw counts themselves are not very informative.\nWe often report the True Positive Rate (TPR), also known as recall, which is the fraction of true positives out of all actual positives: \\(\\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}.\\) The TPR measures how well the model identifies positive examples (higher is better).\nWe also report the False Positive Rate (FPR), which is the fraction of false positives out of all actual negatives: \\(\\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}.\\) The FPR measures how often the model incorrectly identifies negative examples as positive (lower is better).\nFinally, we can report the Precision, which is the fraction of true positives out of all predicted positives: \\(\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}.\\) Precision measures how well the model identifies positive examples among all predicted positives (higher is better).\nIf we have a model that does not achieve the desired TPR or FPR, we have a hidden lever we can pull: the threshold for predicting a positive class. By default, we predict a positive class if the predicted probability is greater than \\(\\frac12\\). But, we can change this threshold to an arbitrary value \\(\\tau \\in [0, 1]\\). Increasing the threshold decreases the number of positive predictions, which decreases both TPR and FPR: The TPR decreases because we are less likely to predict a positive class, so we will miss more true positives. The FPR decreases because we are less likely to predict a positive class, so we will make fewer false positive predictions.\n\n\n\nWe can visualize the trade-off between TPR and FPR by plotting the Receiver Operating Characteristic (ROC) curve. The ROC curve is a plot of the TPR against the FPR for different threshold values \\(\\tau\\). Because a higher TPR is better and a lower FPR is better, we want the ROC curve to be as close to the top-left corner as possible. The area under the ROC curve (AUC) is a single number that summarizes the performance of the model across all threshold values. A model with an AUC of 1 is perfect, while a model with an AUC of 0.5 is no better than random guessing.\n\n\n\n\n\nMultiple Classes\nIn many settings, we are interested in classifying data into more than two classes. For example, we might want to classify images into different categories, such as cats, dogs, and birds. In this case, we need to extend our binary classification model to handle multiple classes. Our supervised learning setup remains the same, where we have labelled data \\((\\mathbf{x}^{(1)}, y^{(1)}), \\ldots, (\\mathbf{x}^{(n)}, y^{(n)})\\), but now \\(y^{(i)} \\in \\{1, 2, \\ldots, k\\}\\), for \\(k\\) the number of classes.\nWe can still use the same ideas that we used for binary logistic regression, but we need to extend the output of the model to predict a probability distribution over each of the \\(k\\) classes. Instead of a single output, which we can interpret as the probability of the positive class, we will have a vector of outputs \\(\\mathbf{f}(\\mathbf{x}^{(i)}) \\in \\mathbb{R}^k\\), where \\(k\\) is the number of classes.\nTo ensure this vector is a valid probability distribution, we can use the softmax function, defined as \\[\n\\begin{align*}\n\\text{softmax}(\\mathbf{z}) &= \\begin{bmatrix}\n\\frac{e^{z_1}}{\\sum_{j=1}^k e^{z_j}} \\\\\n\\frac{e^{z_2}}{\\sum_{j=1}^k e^{z_j}} \\\\\n\\vdots \\\\\n\\frac{e^{z_k}}{\\sum_{j=1}^k e^{z_j}} \\\\\n\\end{bmatrix}\n\\end{align*}\n\\] Softmax applies the exponential function to each element of the vector, and then normalizes the resulting vector so that the sum of the resulting vector is 1. This ensures that the output is a valid probability distribution, where each probability is between 0 and 1 and the sum of all elements is 1.\nThe loss function for multi-class classification is the cross-entropy loss, which is a generalization of the binary cross-entropy loss: \\[\n\\begin{align*}\n\\mathcal{L}(\\mathbf{w}) &= - \\sum_{i=1}^n \\sum_{j=1}^k \\mathbb{1}[y^{(i)}=j] \\log\\left(f_j(\\mathbf{x}^{(i)})\\right),\n\\end{align*}\n\\] where \\(f_j(\\mathbf{x}^{(i)})\\) is the \\(j\\)th element of the softmax output vector \\(\\mathbf{f}(\\mathbf{x}^{(i)})\\), and \\(\\mathbb{1}[y^{(i)}=j]\\) is an indicator function that is 1 if \\(y^{(i)} = j\\) and 0 otherwise."
  },
  {
    "objectID": "notes/code_mnist.html",
    "href": "notes/code_mnist.html",
    "title": "",
    "section": "",
    "text": "import os\nimport random\nimport torch\nimport torchvision\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\n\n# -----------------------------\n# 1) Load MNIST (training split)\n# -----------------------------\ntransform = transforms.ToTensor()  # float tensor in [0,1], shape [1, 28, 28]\ntrain_ds = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n\n# ------------------------------------------------\n# 2) Build a (random) subset of the training data\n# ------------------------------------------------\nsubset_size = 3000  # adjust as desired\nsubset_indices = set(random.sample(range(len(train_ds)), subset_size))\n\nsubset_imgs = []\nsubset_labels = []\nfor idx in subset_indices:\n    img, lbl = train_ds[idx]\n    subset_imgs.append(img)   # [1, 28, 28]\n    subset_labels.append(lbl)\n\nsubset_imgs = torch.stack(subset_imgs, dim=0)             # [N, 1, 28, 28]\nsubset_labels = torch.tensor(subset_labels, dtype=torch.long)  # [N]\nsubset_flat = subset_imgs.view(subset_imgs.size(0), -1)   # [N, 784]\n\n# ------------------------------------------------------\n# 3) Select \"key\" examples (2, 7, 9) NOT in the subset\n# ------------------------------------------------------\ndef find_example_not_in_indices(ds, digit, excluded_indices):\n    for i in range(len(ds)):\n        if i in excluded_indices:\n            continue\n        img, lbl = ds[i]\n        if lbl == digit:\n            return img, lbl, i\n    raise ValueError(f\"Digit {digit} not found outside the excluded indices.\")\n\nkey_digits = [2, 7, 9]\nkeys = []  # list of (key_img, key_lbl, key_idx)\nfor d in key_digits:\n    kimg, klbl, kidx = find_example_not_in_indices(train_ds, d, subset_indices)\n    keys.append((kimg, klbl, kidx))  # each kimg shape [1,28,28]\n\n# ---------------------------------------------------------------\n# 4) For each key, find the 5 nearest neighbors in the subset\n#    by L2 distance in pixel space (subset is the only pool)\n# ---------------------------------------------------------------\ndef topk_neighbors_l2(key_img, pool_flat, k=5):\n    key_vec = key_img.view(1, -1)          # [1, 784]\n    dists = torch.cdist(key_vec, pool_flat, p=2).squeeze(0)  # [N]\n    vals, idxs = torch.topk(dists, k, largest=False)\n    return idxs, vals\n\nk = 5\nneighbors_per_key = []  # (key_img, key_lbl, neighbor_imgs[k], neighbor_lbls[k], dists[k])\nfor (kimg, klbl, _) in keys:\n    idxs, dists = topk_neighbors_l2(kimg, subset_flat, k=k)\n    nb_imgs = subset_imgs[idxs]    # [k, 1, 28, 28]\n    nb_lbls = subset_labels[idxs]  # [k]\n    neighbors_per_key.append((kimg, klbl, nb_imgs, nb_lbls, dists))\n\n# ---------------------------------------------------------------\n# 5) Plot: one row per key (2, 7, 9). Each row shows:\n#    [key] | [NN1] [NN2] [NN3] [NN4] [NN5]\n# ---------------------------------------------------------------\nrows = len(key_digits)\ncols = 1 + k\nfig, axes = plt.subplots(rows, cols, figsize=(2.2*cols, 2.2*rows))\nif rows == 1:\n    axes = axes[None, :]  # ensure 2D indexing if only one row\n\nfor r, (kimg, klbl, nb_imgs, nb_lbls, dists) in enumerate(neighbors_per_key):\n    # First column: the key\n    ax = axes[r, 0]\n    ax.imshow(kimg.squeeze(0), cmap=\"gray\")\n    ax.axis(\"off\")\n\n    # Next columns: neighbors\n    for c in range(k):\n        ax = axes[r, 1 + c]\n        ax.imshow(nb_imgs[c].squeeze(0), cmap=\"gray\")\n        ax.axis(\"off\")\n\n    # Draw a vertical line separating key and neighbors\n    # We add it to the whole row figure, not just one subplot\n    fig.add_artist(plt.Line2D(\n        [1.025/cols, 1.025/cols],     # x-position in figure coordinates\n        [r/rows, (r+1)/rows], # from top of this row to bottom\n        transform=fig.transFigure,\n        color=\"blue\", linewidth=5\n    ))\n\n# Optional row labels (left side)\nfor r in range(rows):\n    axes[r, 0].set_ylabel(f\"Digit {key_digits[r]}\", rotation=90,\n                          fontsize=12, labelpad=10)\n\nplt.tight_layout()\n\nimport os\nos.makedirs(\"images\", exist_ok=True)\nplt.savefig(\"images/kernel_mnist_knn.svg\", dpi=300)\n\n\n\n\n\n\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\n\n# -----------------------------\n# 1) Data\n# -----------------------------\ntransform = transforms.ToTensor()  # [1,28,28] in [0,1]\ntrain_ds = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\ntrain_loader = DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=0, pin_memory=True)\n\n# -----------------------------\n# 2) Model: multinomial logistic regression (softmax)\n# -----------------------------\nclass SoftmaxLogReg(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(784, 10, bias=True)\n    def forward(self, x):\n        x = x.view(x.size(0), -1)  # flatten to [B,784]\n        return self.linear(x)      # logits [B,10]\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(0)\nmodel = SoftmaxLogReg().to(device)\n\n# -----------------------------\n# 3) Train\n# -----------------------------\ncriterion = nn.CrossEntropyLoss()\nopt = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n\nepochs = 3  # increase to 5–10 for crisper weights\nmodel.train()\nfor _ in range(epochs):\n    for x, y in train_loader:\n        x, y = x.to(device), y.to(device)\n        opt.zero_grad()\n        logits = model(x)\n        loss = criterion(logits, y)\n        loss.backward()\n        opt.step()\n\n# -----------------------------\n# 4) Visualize class weights\n# -----------------------------\nwith torch.no_grad():\n    W = model.linear.weight.detach().cpu().view(10, 28, 28)  # [10,28,28]\n    vmax = W.abs().max().item()  # shared symmetric scale\n\nfig, axes = plt.subplots(2, 5, figsize=(10, 4))\naxes = axes.reshape(2, 5)\n\nfor d in range(10):\n    r, c = divmod(d, 5)\n    ax = axes[r, c]\n    im = ax.imshow(W[d].numpy(), cmap=\"seismic\", vmin=-vmax, vmax=vmax)\n    ax.set_title(f\"Digit {d}\", fontsize=11)\n    ax.axis(\"off\")\n\nplt.tight_layout()\n\n# Optional: one shared colorbar\ncbar = fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.75, pad=0.02)\ncbar.set_label(\"\", rotation=90)\nplt.savefig('images/kernel_mnist_weights.svg', dpi=300)"
  },
  {
    "objectID": "notes/SupportVectorMachines.html",
    "href": "notes/SupportVectorMachines.html",
    "title": "Support Vector Machines and Constrained Optimization",
    "section": "",
    "text": "We have so far explored the Naive Bayes classifier and logistic regression for solving classification tasks. In this section, we will learn about another tool called support vector machines (SVMs). While the final algorithm will be similar to logistic regression, SVMs approach the problem from a different perspective. As we’ll soon discover, both logistic regression and SVMs have their own strengths and weaknesses. Plus, we’ll get to see some fun math along the way!\n\nSupport Vector Machines\nConsider a binary classification problem with data points \\(\\mathbf{x}^{(i)}\\) and labels \\(y^{(i)} \\in \\{-1, 1\\}\\). (We previously used \\(y^{(i)} \\in \\{0, 1\\}\\), but this is just a relabeling for convenience.) We want to find a hyperplane that separates the two classes. For the majority of our discussion, we will assume that the data is linearly separable, meaning there exists a hyperplane that can perfectly separate the two classes.\n\n\n\nWhen there is such a separating hyperplane, there are often infinitely many that we can choose from. The question at the heart of support vector machines is: which hyperplane should we choose?\n\n\n\nAs we’ve seen in the past, it is not too difficult to find a model that perfectly fits our training data; the real challenge is to find a model that generalizes well to unseen data. In the context of classification, we may expect that a model that separates the two classes with the largest ‘margin of error’ will generalize better than one that is very close to the data points. With this in mind, Vladimir Vapnik and Alexey Chervonenkis proposed the idea of support vector machines, which aim to find the hyperplane that maximizes the margin between the two classes.\n\n\n\nLet’s consider a particular hyperplane, defined by the normal vector \\(\\mathbf{w}\\) and bias \\(b\\). The equation of the hyperplane is given by: \\[\n\\begin{align}\n\\langle \\mathbf{w}, \\mathbf{x} \\rangle - b = 0.\n\\end{align}\n\\]\nWe will define \\(\\mathbf{w}\\) and \\(b\\) so that \\(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle - b \\geq 1\\) for all points \\(\\mathbf{x}^{(i)}\\) in the positive class (\\(y^{(i)} = 1\\)) and \\(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle - b \\leq -1\\) for all points in the negative class (\\(y^{(i)} = -1\\)). As we can see in the figure above, such a hyperplane will always exist if the data is linearly separable. How can we find the hyperplane that maximizes the margin?\n\n\nComputing the Margin\nFor a given \\(\\mathbf{w}\\) and \\(b\\), we can see that the margin is the distance between the hyperplane \\(\\langle \\mathbf{w}, \\mathbf{x} \\rangle - b = 1\\) and the hyperplane \\(\\langle \\mathbf{w}, \\mathbf{x} \\rangle - b = -1\\). Let’s calculate this distance.\n\n\n\nConsider a point \\(\\mathbf{z}_1\\) on the hyperplane \\(\\langle \\mathbf{w}, \\mathbf{x} \\rangle - b = 1\\). Let \\(\\mathbf{z}_2\\) be the point on the hyperplane \\(\\langle \\mathbf{w}, \\mathbf{x} \\rangle - b = -1\\) that is closest to \\(\\mathbf{z}_1\\). Because \\(\\mathbf{z}_1\\) and \\(\\mathbf{z}_2\\) are the closest points to each other on the two hyperplanes, the line connecting them is perpendicular to both hyperplanes. (Otherwise, we could move along the hyperplane \\(\\langle \\mathbf{w}, \\mathbf{x} \\rangle - b = -1\\) to find a point \\(\\mathbf{z}_2'\\) that is closer to \\(\\mathbf{z}_1\\) than \\(\\mathbf{z}_2\\), contradicting our assumption that \\(\\mathbf{z}_2\\) is the closest point.) Formally, we can write: \\[\n\\mathbf{z}_1 - \\mathbf{z}_2 = \\lambda \\bar{\\mathbf{w}}\n\\] for some scaling \\(\\lambda \\in \\mathbb{R}\\), where \\(\\bar{\\mathbf{w}}\\) is the unit normal vector \\(\\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|_2}\\). We are interested in the length of this vector \\(\\lambda\\). By our observation above, we can write \\(\\mathbf{z}_1 =  \\lambda \\bar{\\mathbf{w}} + \\mathbf{z}_2\\). Then, plugging in \\(\\mathbf{z}_1\\) into the hyperplane equation \\(\\langle \\mathbf{w}, \\mathbf{z}_1 \\rangle - b = 1\\), we have: \\[\n\\begin{align}\n1 &= \\langle \\mathbf{w}, \\lambda \\bar{\\mathbf{w}} + \\mathbf{z}_2 \\rangle - b\n\\\\ &= \\langle \\mathbf{w}, \\lambda \\bar{\\mathbf{w}} \\rangle + \\langle \\mathbf{w}, \\mathbf{z}_2 \\rangle - b\n\\\\ &= \\lambda \\langle \\mathbf{w}, \\bar{\\mathbf{w}} \\rangle - 1\n\\\\ &= \\frac{\\lambda}{\\| \\mathbf{w} \\|_2} \\| \\mathbf{w} \\|_2^2 - 1.\n\\end{align}\n\\] Rearranging, we have that \\(\\lambda = \\frac2{\\| \\mathbf{w} \\|_2}\\). For a given \\(\\mathbf{w}\\) and \\(b\\), we now know how to compute the margin. Let’s now find the \\(\\mathbf{w}\\) and \\(b\\) with the largest margin.\n\n\nConstrained Optimization\nWe can formalize our goal as a constrained optimization problem: We want to find the hyperplane that maximizes the margin, subject to the constraints that all points in the positive class are on one side of the hyperplane and all points in the negative class are on the other side.\nObserve that maximizing the margin is equivalent to minimizing the inverse of the margin, which, by our calculation above, is equivalent to minimizing \\(\\frac12 \\| \\mathbf{w} \\|_2\\). Further, notice that we can simplify our constraints: \\(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle - b \\geq 1 \\text{ for } y^{(i)} = 1\\) and \\(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle - b \\leq -1 \\text{ for } y^{(i)} = -1\\) is equivalent to \\(y^{(i)}(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle - b) \\geq 1\\) for all \\(i\\). (This is why we relabelled the classes so that \\(y^{(i)} \\in \\{-1, 1\\}\\).) We can now write our optimization problem as follows: \\[\n\\begin{align}\n\\min_{\\mathbf{w}, b} \\frac12 \\| \\mathbf{w} \\|_2^2 \\textnormal{ such that }\ny^{(i)}(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle - b) \\geq 1 \\text{ } \\forall i.\n\\end{align}\n\\]\nThe points \\(\\mathbf{x}^{(i)}\\) that lie on the hyperplane i.e., those for which \\(y^{(i)}(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle - b) = 1\\) are called the support vectors. Let \\(\\mathcal{S}\\) be the set of indices of the support vectors.\n\n\n\nThe problem above is known as an example of a constrained optimization problem. More specifically, it is a quadratic programming problem: we have a quadratic objective function (the term \\(\\frac12 \\| \\mathbf{w} \\|_2^2\\) is quadratic in \\(\\mathbf{w}\\)) and linear constraints (the constraints are linear in \\(\\mathbf{w}\\) and \\(b\\)). Constrained optimization problems in general, and quadratic programming in particular, are a rich area of study in optimization, and there are many techniques to solve them. However, the time complexity of these techniques will depend on the number of variables in \\(\\mathbf{w}\\). For data in high dimensions (e.g., the transformed data after we add features to make the classes linearly separable), the time complexity can be quite high. In the next section, we will see how to find the dual of this problem, which can allow us to solve it more efficiently.\nBefore we do, let’s see why the hyperplane is called a support vector machine.\nClaim: The optimal hyperplane \\(\\mathbf{w}^\\star\\) is a linear combination of the support vectors: \\[\n\\mathbf{w}^\\star = \\sum_{i=1}^n \\beta_i  y^{(i)} \\mathbf{x}^{(i)}\n\\] for some coefficients \\(\\beta_i \\in \\mathbb{R}\\).\n\n\n\nProof\n\nSuppose for contradiction that \\(\\mathbf{w}^\\star\\) is not a linear combination of the support vectors. That is, \\(\\mathbf{w}^\\star = \\sum_{i=1}^n \\beta_i  y^{(i)} \\mathbf{x}^{(i)} + \\mathbf{v}\\) for some vector \\(\\mathbf{v}\\) that is orthogonal to all support vectors. We will construct a new hyperplane \\(\\mathbf{w}' = \\mathbf{w}^\\star - \\epsilon \\mathbf{v}\\) for some small \\(\\epsilon &gt; 0\\), so that each point is still correctly classified, and the margin is larger than that of \\(\\mathbf{w}^\\star\\).\nLet’s check that the new hyperplane \\(\\mathbf{w}'\\) still correctly classifies all points. First, for any support vector \\(\\mathbf{x}^{(i)}\\), we have \\[\n\\langle \\mathbf{w}', \\mathbf{x}^{(i)} \\rangle = \\langle \\mathbf{w}^\\star - \\epsilon \\mathbf{v}, \\mathbf{x}^{(i)} \\rangle = \\langle \\mathbf{w}^\\star, \\mathbf{x}^{(i)} \\rangle\n\\] since \\(\\mathbf{v}\\) is orthogonal to all support vectors. By assumption, \\(\\mathbf{w}^\\star\\) satisfies \\(y^{(i)}(\\langle \\mathbf{w}^\\star, \\mathbf{x}^{(i)} \\rangle - b) = 1\\). Second, for any non-support vector \\(\\mathbf{x}^{(j)}\\), we have \\[\n\\langle \\mathbf{w}', \\mathbf{x}^{(j)} \\rangle = \\langle \\mathbf{w}^\\star - \\epsilon \\mathbf{v}, \\mathbf{x}^{(j)} \\rangle = \\langle \\mathbf{w}^\\star, \\mathbf{x}^{(j)} \\rangle - \\epsilon \\langle \\mathbf{v}, \\mathbf{x}^{(j)} \\rangle.\n\\] Since \\(\\mathbf{x}^{(j)}\\) is not a support vector, we have \\(y^{(j)} (\\langle \\mathbf{v}, \\mathbf{x}^{(j)} \\rangle - b) &gt; 1\\). Because this inequality is strict, we can choose \\(\\epsilon\\) small enough so that \\(y^{(j)}(\\langle \\mathbf{w}', \\mathbf{x}^{(j)} \\rangle - b) = y^{(j)}(\\langle \\mathbf{w}^\\star, \\mathbf{x}^{(j)} \\rangle - b - \\epsilon \\langle \\mathbf{v}, \\mathbf{x}^{(j)} \\rangle) &gt; 1\\).\nNext, let’s check that the margin of \\(\\mathbf{w}'\\) is larger than that of \\(\\mathbf{w}^\\star\\). Since \\(\\mathbf{v}\\) is orthogonal to all support vectors, we can write: \\[\n\\begin{align}\n\\| \\mathbf{w}' \\|_2^2\n&= \\| \\sum_{i=1}^n \\beta_i y^{(i)} \\mathbf{x}^{(i)} - (1-\\epsilon) \\mathbf{v} \\|_2^2 \\\\\n&= \\| \\sum_{i=1}^n \\beta_i y^{(i)} \\mathbf{x}^{(i)} \\|_2^2 + (1-\\epsilon)^2 \\| \\mathbf{v} \\|_2^2 \\\\\n&&lt; \\| \\sum_{i=1}^n \\beta_i y^{(i)} \\mathbf{x}^{(i)} \\|_2^2 + \\| \\mathbf{v} \\|_2^2 = \\| \\mathbf{w}^\\star \\|_2^2.\n\\end{align}\n\\] Since the length of the normal vector \\(\\mathbf{w}\\) is inversely proportional to the margin, we have that the margin of \\(\\mathbf{w}'\\) is larger than that of \\(\\mathbf{w}^\\star\\), a contradiction!\n\n\n\n\nThe Dual Problem\nWe have derived a quadratic programming problem, which we’ll call the primal problem:\n\\[\n\\begin{align}\n\\min_{\\mathbf{w}, b} \\frac12 \\| \\mathbf{w} \\|_2^2 \\textnormal{ such that }\ny^{(i)}(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle - b) \\geq 1 \\text{ } \\forall i.\n\\end{align}\n\\]\nThe challenge in directly solving the primal is that the number of variables in \\(\\mathbf{w}\\) is equal to the number of features, which can be very large especially if we apply feature transformations to the data. Fortunately, we can turn the primal into a dual problem, that, roughly speaking, converts each constraint into a variable, and each variable into a constraint. The dual problem will have a number of variables equal to the number of data points, which can be much smaller than the number of features, especially if we run a convex hull algorithm to reduce to only the “external” points that could be support vectors.\nLet’s begin with the Lagrangian of the primal problem: \\[\n\\begin{align}\n\\min_{\\mathbf{w}, b} \\max_{\\boldsymbol{\\alpha}} \\frac12 \\| \\mathbf{w} \\|_2^2 - \\sum_{i=1}^n \\alpha_i \\left( y^{(i)}(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle - b) - 1 \\right)\n= \\min_{\\mathbf{w}, b} \\max_{\\boldsymbol{\\alpha}} \\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\alpha}),\n\\end{align}\n\\] where \\(\\boldsymbol{\\alpha} \\in \\mathbb{R}^{|\\mathcal{S}|}_+\\) is a vector of non-negative real numbers. Let’s see why the Lagrangian is equivalent to the primal problem: For a particular \\(\\mathbf{w}\\) and \\(b\\), the constraints in the primal problem must be satisfied by the Lagrangian, otherwise there is some \\(i\\) such that \\(y^{(i)}(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle - b) &lt; 1\\) and we can arbitrarily increase the Lagrangian objective by increasing \\(\\alpha_i\\). Among these \\(\\mathbf{w}\\) and \\(b\\) that satisfy the constraints, the Lagrangian objective is minimized when \\(\\frac12 \\|\\mathbf{w}\\|_2^2\\) is as small as possible, which is equivalent to minimizing the objective of the primal problem.\nThe Lagrangian is useful because we can more easily reason about the optimal \\(\\mathbf{w}\\) and \\(b\\). Notice that the Lagrangian is a convex function in \\(\\mathbf{w}\\) and \\(b\\), and a concave function in \\(\\boldsymbol{\\alpha}\\). Further, as long as the data is linearly separable, we can always come up with a \\(\\mathbf{w}\\) and \\(b\\) such that the constraints are satisfied, namely with \\(\\boldsymbol{\\alpha} = 0\\). Together, by Slater’s condition, we can apply the minimax theorem to exchange the order of the minimization and maximization. That is, \\[\n\\min_{\\mathbf{w}, b} \\max_{\\boldsymbol{\\alpha}} \\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\alpha})\n= \\max_{\\boldsymbol{\\alpha}} \\min_{\\mathbf{w}, b} \\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\alpha}).\n\\]\nLet’s fix a \\(\\boldsymbol{\\alpha}\\) and minimize the Lagrangian with respect to \\(\\mathbf{w}\\) and \\(b\\). Taking the gradient with respect to \\(\\mathbf{w}\\) and \\(b\\), and setting it to zero, we have: \\[\n\\begin{align}\n\\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}^*, b^*, \\boldsymbol{\\alpha})\n&= \\mathbf{w}^* - \\sum_{i=1}^n \\alpha_i y^{(i)} \\mathbf{x}^{(i)} = 0\n\\\\\\nabla_{b} \\mathcal{L}(\\mathbf{w}^*, b^*, \\boldsymbol{\\alpha})\n&= \\sum_{i=1}^n \\alpha_i y^{(i)} = 0.\n\\end{align}\n\\] We can plug these two equations into the Lagrangian to obtain \\[\n\\begin{align}\n\\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\alpha})\n&=\n\\frac12 \\left\\| \\sum_{i=1}^n \\alpha_i y^{(i)} \\mathbf{x}^{(i)} \\right\\|^2\n- \\sum_{i=1}^n \\alpha_i y^{(i)} {\\mathbf{x}^{(i)}}^\\top \\sum_{j=1}^n \\alpha_j y^{(j)} \\mathbf{x}^{(j)}\n+ b \\sum_{i=1}^n \\alpha_i y^{(i)}\n+ \\sum_{i=1}^n \\alpha_i\n\\\\&=\n- \\frac12 \\sum_{i,j=1}^n \\alpha_i \\alpha_j y^{(i)} y^{(j)} \\langle \\mathbf{x}^{(i)}, \\mathbf{x}^{(j)} \\rangle\n+ \\sum_{i=1}^n \\alpha_i.\n\\end{align}\n\\] Finally, the dual problem is given by \\[\n\\begin{align}\n\\max_{\\boldsymbol{\\alpha}} \\sum_{i=1}^n \\alpha_i - \\frac12 \\sum_{i,j=1}^n \\alpha_i \\alpha_j y^{(i)} y^{(j)} \\langle \\mathbf{x}^{(i)}, \\mathbf{x}^{(j)} \\rangle\n\\text{ such that } \\alpha_i \\geq 0 \\text{ and } \\sum_{i=1}^n \\alpha_i y^{(i)} = 0.\n\\end{align}\n\\] We call the \\(\\alpha_i\\)’s the dual variables. The points with \\(\\alpha_i &gt; 0\\) are exactly the support vectors, and the optimal hyperplane can be computed as \\(\\mathbf{w}^* = \\sum_{i \\in \\mathcal{S}}^n \\alpha_i y^{(i)} \\mathbf{x}^{(i)}\\). To get the optimal bias \\(b^*\\), we can use any support vector \\(\\mathbf{x}^{(i)}\\) and compute \\(b^* = \\langle \\mathbf{w}^*, \\mathbf{x}^{(i)} \\rangle - y^{(i)}\\) since we know that the support vector lies on the hyperplane.\n\n\nSoft Margin\nWe have so far assumed that the data is linearly separable. However, in practice, we may have noisy data or outliers that make it impossible to find a hyperplane that perfectly separates the two classes. To address this, we can modify our optimization problem to allow for some misclassification. We can do this by introducing slack variables \\(\\xi_i \\geq 0\\) for each data point, which allow us to ‘relax’ the constraints. The modified optimization problem is given by:\n\\[\n\\begin{align}\n\\min_{\\mathbf{w}, b} \\| \\mathbf{w} \\|^2 + C \\sum_{i=1}^n \\xi_i\n\\text{ such that }\ny^{(i)}(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle - b) \\geq 1 - \\xi_i \\text{ and } \\xi_i \\geq 0 \\text{ for all } i.\n\\end{align}\n\\]\n\n\n\nThe optimal choice of each slack variable satisfies \\(\\xi_i = \\max(0, 1 - y^{(i)}(\\langle \\mathbf{w}, \\mathbf{x}^{(i)} \\rangle - b))\\). The parameter \\(C &gt; 0\\) controls the trade-off between maximizing the margin and minimizing the misclassification. Notice that this is equivalent to minimizing the hinge loss, with a \\(\\ell_2\\) regularization term."
  },
  {
    "objectID": "notes/Bandits.html",
    "href": "notes/Bandits.html",
    "title": "Multi-Armed Bandits",
    "section": "",
    "text": "The fundamental tension in reinforcement learning (and perhaps in life more generally) is between exploration and exploitation. On one hand, we want to exploit what we know to maximize our reward. On the other, we want to explore new actions to discover their potential rewards. Today, we will explore this tension in the context of a simplified reinforcement learning model: multi-armed bandits.\nIn a multi-armed bandit problem, we have a set of \\(k\\) actions (or “arms”) to choose from, each with an unknown probability distribution of rewards. At each time step \\(t \\in \\{1,\\ldots,T\\}\\), we choose one arm to pull \\(a^{(t)} \\in \\{1, \\ldots, k\\}\\). We then receive a reward \\(r_{a^{(t)}} \\in [-1,1]\\) drawn from chosen arm \\(a^{(t)}\\). Define the expected reward of arm \\(a\\) as \\(\\mu_a = \\mathbb{E}[r_a]\\).\n\n\n\nOur goal is to minimize the average regret, compared to the optimal arm: \\[\nR_T = \\max_{a \\in \\{1,\\ldots, k\\}} \\mu_a - \\frac1{T} \\sum_{t=1}^{T} \\mu_{a^{(t)}},\n\\] The challenge is balancing exploration (trying out different arms to learn their rewards) with exploitation (choosing the arm that currently seems best). Our solution will be “optimism in the face of uncertainty”; that is, giving each arm a reasonable benefit of the doubt when picking the best option.\n\nUpper Confidence Bound (UCB)\nSince we initially have no information about the arms, we use the first \\(k\\) time steps to sample each arm once. From then on, we will maintain an estimate \\(\\tilde{\\mu}_a^{(t)}\\) of the expected reward for each arm based on the reward we observed from all previous time steps \\(1,\\ldots, t-1\\).\nWe will also maintain a confidence bound \\(\\epsilon_a^{(t)}\\) for each arm, which quantifies the uncertainty of our estimate. Formally, we expect \\(\\mu_a - \\tilde{\\mu}_a^{(t)} \\leq \\epsilon_a^{(t)}\\) with high probability. At each time step \\(t\\), we will choose the arm with the highest upper confidence bound: \\[\na^{(t)} = \\arg \\max_{a} \\tilde{\\mu}_a^{(t)} + \\epsilon_a^{(t)}.\n\\]\nWe will choose the confidence intervals so that the probability that our estimate deviates from the true mean by more than the confidence bound at any point in the algorithm is at most a user-defined parameter \\(\\delta\\).\nThe following technical lemma will help us analyze the algorithm.\nLemma For all time steps \\(t \\in \\{k+1, \\ldots, T\\}\\) and arms \\(a \\in \\{1, \\ldots, k\\}\\), with probability \\(1-\\delta\\), we have \\[\n|\\mu_a - \\tilde{\\mu}_a^{(t)}| \\leq \\epsilon_a^{(t)} = \\sqrt{\\frac{2\\log(\\frac{2T k}{\\delta})}{n_a^{(t)}}}\n\\] where \\(n_a^{(t)}\\) is the number of times arm \\(a\\) has been pulled up to time \\(t\\) i.e., \\(n_a^{(t)} = \\sum_{s=1}^{t-1} \\mathbb{1}(a^{(s)} = a)\\).\n\n\n\nProof of Lemma\n\nWe will apply Hoeffding’s inequality to bound the estimation error.\nConsider \\(n\\) independent random variables \\(X_1, X_2, \\ldots, X_n\\) that such that \\(a \\leq X_i \\leq b\\) for all \\(i\\). Let \\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\\) be the sample mean, and \\(\\mu = \\mathbb{E}[\\bar{X}]\\) be the expected value of the sample mean. Recall Hoeffding’s inequality: For any \\(\\epsilon &gt; 0\\), \\[\n\\Pr\\left(|\\mathbb{E}[\\bar{X}] - \\bar{X}| \\geq \\epsilon\\right) \\leq 2 \\exp\\left(-\\frac{2n\\epsilon^2}{(b-a)^2}\\right).\n\\] In our setting, the rewards are bounded between \\(-1\\) and \\(1\\), so we have \\(a = -1\\) and \\(b = 1\\). Setting the failure probability to \\(\\frac{\\delta}{T k}\\) and solving for \\(\\epsilon\\), we get \\[\n\\begin{align}\n2\\exp\\left(-\\frac{2 {n} \\epsilon^2}{4}\\right)\n&= \\frac{\\delta}{T k}\n\\\\ \\frac{{n} \\epsilon^2}{2}\n&= \\log\\left(\\frac{2T k}{\\delta}\\right)\n\\\\ \\epsilon\n&= \\sqrt{\\frac{2 \\log\\left(\\frac{2T k}{\\delta}\\right)}{n}}.\n\\end{align}\n\\]\nApplying Hoeffding’s inequality, and this choice of \\(\\epsilon\\), to our reward estimation problem for a particular arm \\(a\\) and time step \\(t\\), we get \\[\n\\Pr\\left(|\\mu_a - \\tilde{\\mu}_a^{(t)}| \\geq \\epsilon_a^{(t)}\\right) \\leq \\frac{\\delta}{T k}.\n\\]\nWe want this inequality to hold simultaneously for all arms and all time steps. Union bounding over all \\(Tk\\) events yields\n\\[\n\\Pr \\left(\n  \\bigcup_{t=1}^{T} \\bigcup_{a=1}^{k} \\left\\{ |\\mu_a - \\tilde{\\mu}_a^{(t)}| \\geq \\epsilon_a^{(t)} \\right\\}\n\\right)\n\\leq \\sum_{t=1}^T \\sum_{a=1}^k \\frac{\\delta}{T k}\n= \\delta.\n\\]\n\n\nWith the lemma in hand, we are now ready to prove the regret guarantee of the UCB algorithm. Let \\(a^*\\) be the optimal arm, i.e., the arm with the highest expected reward \\(\\mu_{a^*} = \\max_{a} \\mu_a\\).\nRegret Theorem: The average regret of the UCB algorithm satisfies: \\[\n\\mu_{a^*} - \\frac1{T} \\sum_{t=1}^T \\mu_{a^{(t)}}\n= O\\left(\\frac{k}{T} + \\sqrt{\\log\\left(\\frac{T k}{\\delta}\\right)} \\frac{\\sqrt{k}}{\\sqrt{T}}\\right)\n= \\tilde{O}\\left(\\sqrt{\\frac{k}{T}}\\right).\n\\]\nNote that the logarithmic term is always small; for example, \\(\\log_{10}(\\# \\text{atoms in the universe}) \\approx \\log_{10}(10^{80}) = 80\\).\nSince the multi-armed bandit problem is only interesting for \\(T \\gg k\\), the average regret is dominated by the second term, which decays as \\(\\tilde{O}(\\sqrt{k/T})\\) where the \\(\\tilde{O}(\\cdot)\\) hides log factors. In words, increasing the number of steps by a factor of 100 decreases the average regret by a factor of 10. Conversely, increasing the number of arms by a factor of 100 only increases the average regret by a factor of 10.\nUp to logarithmic factors, the UCB algorithm gives the best possible guarantee: That is, there are instances where the average regret grows at a rate of \\(\\Omega(\\sqrt{k/T})\\) for all possible strategies (see e.g., Chapter 2 in Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems).\n\n\n\nProof of Regret Theorem\n\nWe will first bound the difference between the reward of the optimal action \\(a^*\\) and the reward of the action chosen by the algorithm \\(a^{(t)}\\) at time \\(t\\). By the lemma and the greedy choice of \\(a^{(t)}\\), we have: \\[\n\\mu_{a^*}\n\\leq\n\\tilde{\\mu}_{a^*}^{(t)} + \\epsilon_{a^*}^{(t)}\n\\leq \\tilde{\\mu}_{a^{(t)}}^{(t)} + \\epsilon_{a^{(t)}}^{(t)}\n\\leq \\mu_{a^{(t)}} + 2 \\epsilon_{a^{(t)}}^{(t)}.\n\\] Subtracting \\(\\mu_{a^{(t)}}\\) from both sides, we get \\[\n\\mu_{a^*} - \\mu_{a^{(t)}}\n\\leq 2 \\epsilon_{a^{(t)}}^{(t)}\n= 2 \\sqrt{\\frac{2\\log(\\frac{2T k}{\\delta})}{n_a^{(t)}}}.\n\\]\nSumming over all but the first \\(k\\) time steps, \\[\n\\sum_{t=k+1}^T \\mu_{a^*} - \\mu_{a^{(t)}}\n\\leq \\sum_{t=k+1}^T 2 \\sqrt{\\frac{2\\log(\\frac{2T k}{\\delta})}{n_a^{(t)}}}\n= 2 \\sqrt{2\\log\\left(\\frac{2T k}{\\delta}\\right)} \\sum_{t=k+1}^T \\frac{1}{\\sqrt{n_a^{(t)}}}.\n\\] We can write the inner summation as \\[\n\\sum_{t=k+1}^T \\frac{1}{\\sqrt{n_a^{(t)}}}\n= \\sum_{a=1}^k \\sum_{i=1}^{n_a^{(T)}} \\frac{1}{\\sqrt{i}}\n\\leq \\sum_{a=1}^k 2 \\sqrt{n_a^{(T)}}\n\\leq 2 \\sqrt{T k},\n\\] where the first inequality follows by upper bounding the sum with an integral, and the second inequality follows from Cauchy-Schwarz. Do you see how?\nThen, the average regret is\n\\[\n\\frac1{T} \\sum_{t=1}^T \\mu_{a^*} - \\mu_{a^{(t)}}\n\\leq 2 \\frac{k}{T} +\\frac1{T} \\sum_{t=k+1}^T \\mu_{a^*} - \\mu_{a^{(t)}}\n= 2 \\frac{k}{T} + 4 \\sqrt{2\\log\\left(\\frac{2T k}{\\delta}\\right)} \\frac{\\sqrt{k}}{\\sqrt{T}}.\n\\]"
  },
  {
    "objectID": "notes/NeuralNetworks.html",
    "href": "notes/NeuralNetworks.html",
    "title": "Neural Networks",
    "section": "",
    "text": "Previously, we have built machine learned models in two steps: First, we chose a set of features to represent the data, possibly transforming the data in the process. Second, we optimized a model on the (transformed) features. Neural networks allow us to learn both the feature transformations and the model simultaneously.\n\nNeural Network Architectures\nConsider training data that is not linearly separable, i.e., we cannot draw a straight line to separate the classes. We saw before how we could represent the data in a higher-dimensional space, where it is linearly separable. The alternative we’ll explore today is to divide the region with multiple linear boundaries.\n\n\n\nWe can learn four linear boundaries, each of bounds the region in a different direction.\n\n\n\nWhen we pass the input through the learned lines, we will get real numbers representing the distance from the line e.g., \\(\\langle \\mathbf{x}, \\mathbf{w}^{(i)} \\rangle\\). Let’s apply a non-linear activation function to these distances, which will distinguish whether the distance is positive or negative e.g., \\(\\sigma(\\langle \\mathbf{x}, \\mathbf{w}^{(i)} \\rangle)\\), where \\(\\sigma(z) = \\mathbf{1}[z \\geq 0]\\). For example, a point on the “green” side of the first line, “blue” side of the second line, “blue” side of the third line, and “green” side of the fourth line would be represented as \\([1, 0, 0, 1]\\). Finally, we can combine these values to make a prediction.\n\n\n\nIn this way, we can learn a non-linear functions by combining multiple linear models with non-linear activations. By updating the weights of the linear models and the weights of the final combination, we can learn a complex function that fits the data well.\nNeural networks are remarkably flexible, with many different architectural options:\n\nThe activation functions which process the outputs of the linear models. Options include the ReLU function \\(\\sigma(z) = \\max(0, z)\\), the sigmoid function \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\), the hyperbolic tangent function \\(\\sigma(z) = \\tanh(z)\\), and more.\nThe number of layers in the network, i.e., how many times we apply the linear models and activation functions.\nThe number of neurons in each layer, i.e., how many linear models we apply in each layer.\nThe loss function to measure the error between the predicted and true labels. We can use the same loss functions we have seen before: cross-entropy for classification and squared error for regression.\n\nTensorflow offers an excellent visualization of common neural network architectures, which you can play around with here.\nBeyond these options, there are several types of neural network architectures that connect neurons in different ways: Convolutional layers are particularly useful for locally correlated data, such as images, where we can learn filters that capture local patterns. Transformers are particularly useful for sequential data, such as text, and form the backbone of modern foundation models. We will explore these architectures next week!\nWhen we train a neural network, we apply gradient descent to minimize the loss function, just like we have done for linear regression and logistic regression. But we need to be careful about how we compute the gradients, since the neural network is a composition of multiple functions, and there can be many parameters.\n\n\nBackpropagation\nThe key step in training a neural network is to compute the gradients of the loss function with respect each of the parameters in the network. Backpropagation allows us to compute these gradients efficiently. The basic idea is to modularly apply the chain rule from calculus to compute the gradients of each layer, starting from the output layer and working backwards to the input layer.\nBefore we dive into the details, let’s review the chain rule. For a scalar function \\(f: \\mathbb{R} \\to \\mathbb{R}\\), we write the derivative as \\[\n\\frac{df}{dx} = \\lim_{t \\to 0} \\frac{f(x + t) - f(x)}{t}.\n\\]\nFor a multivariate function \\(f: \\mathbb{R}^m \\to \\mathbb{R}\\), we write the partial derivative as \\[\n\\frac{\\partial f}{\\partial x_i} = \\lim_{t \\to 0} \\frac{f(x_1, \\ldots, x_i + t, \\ldots, x_d) - f(x_1, \\ldots, x_i, \\ldots, x_d)}{t}.\n\\]\nConsider two functions \\(f: \\mathbb{R} \\to \\mathbb{R}\\) be a function, and \\(y: \\mathbb{R} \\to \\mathbb{R}\\). The chain rule tells us that the derivative of the composition \\(f(y(x))\\) is given by \\(\\frac{df}{dx} = \\frac{df}{dy} \\cdot \\frac{dy}{dx}\\). To see this, we can write \\[\n\\frac{df}{dx}\n= \\lim_{t \\to 0} \\frac{f(y(x + t)) - f(y(x))}{t}\n= \\lim_{t \\to 0} \\frac{f(y(x + t)) - f(y(x))}{y(x + t) - y(x)} \\frac{y(x + t) - y(x)}{t}.\n= \\frac{df}{dy} \\cdot \\frac{dy}{dx},\n\\] where the last equality follows as long as \\(\\lim_{t \\to 0} y(x + t) - y(x) = 0\\).\nLet \\(f: \\mathbb{R}^m \\to \\mathbb{R}\\) be a function, and \\(y_i: \\mathbb{R} \\to \\mathbb{R}\\) be a sequence of \\(m\\) functions. The multivariate chain rule tells us that the derivative of the composition \\(f(y_1(x), \\ldots, y_m(x))\\) is given by \\[\n\\frac{df}{dx} = \\left(\n  \\frac{df}{dy_1} \\cdot \\frac{dy_1}{dx} + \\ldots + \\frac{df}{dy_m} \\cdot \\frac{dy_m}{dx}\n\\right).\n\\]\nBackpropagation is simply an application of the multivariate chain rule, and is the natural analog to the forward pass in a neural network.\n\n\n\nDuring the forward pass, we successively compute the outputs, and feed them into the next layer. Consider one of the functions in the neural network: The function \\(v : \\mathbb{R}^{d_\\ell} \\to \\mathbb{R}\\) maps the input(s) parameters \\(u_1, \\ldots, u_d\\) to an output, which itself is passed to another function. (In general, \\(d\\) could be 1, e.g., if the \\(v\\) were an activation function.) When we compute \\(v(u_1, \\ldots, u_d)\\), we crucially only need to know the values of parameters \\(u_1, \\ldots, u_d\\), rather than any of the parameters that we used to compute \\(u_1, \\ldots, u_d\\).\nThe time complexity of the forward pass is linear in the number of times a parameter is used in a function, which itself is linear in the number of parameters.\n\n\n\nDuring the backward pass, we compute the derivative of the loss function with respect to the parameters in one layer, then feed these derivatives into the prior layer. Suppose the parameter \\(u\\) is used in \\(n\\) different functions \\(v_1, \\ldots, v_n\\). By the multivariate chain rule, the derivative of the loss function with respect to \\(u\\) is given by \\[\n\\frac{\\partial \\mathcal{L}}{\\partial u} = \\frac{\\partial \\mathcal{L}}{\\partial v_1} \\cdot \\frac{\\partial v_1}{\\partial u} + \\ldots + \\frac{\\partial \\mathcal{L}}{\\partial v_n} \\cdot \\frac{\\partial v_n}{\\partial u}.\n\\] When we compute this partial derivative, we crucially only need to know the partial derivatives \\(\\frac{\\partial \\mathcal{L}}{\\partial v_1}, \\ldots, \\frac{\\partial \\mathcal{L}}{\\partial v_n}\\), and the partial derivatives \\(\\frac{\\partial v_1}{\\partial u}, \\ldots, \\frac{\\partial v_n}{\\partial u}\\). By the time we reach \\(u\\) in the backward pass, the first set of partial derivatives are already known from the prior layer, and the second set of partial derivatives only depend on how \\(u\\) is used in the functions \\(v_1, \\ldots, v_n\\).\nLike the forward pass, the time complexity of the backward pass is linear in the number of times a parameter appears in a function, which itself is linear in the number of parameters.\n\n\nLinear Algebraic View of Backpropagation\nNeural networks have been studied since the the 1950s, and the backpropagation algorithm was first proposed in the 1980s. So why did it take so long for neural networks to become popular? The answer is that neural networks require many parameters to be effective, and for a long time, we did not have the computational resources to train them at sufficient scale. A breakthrough in the 2010s made neural networks practical: Originally intended for matrix multiplication in graphics, the development of Graphical Processing Units (GPUs) allowed us to efficiently implement backpropagation.\nConsider two adjacent layers of neurons: \\(\\mathbf{u} \\in \\mathbb{R}^d\\) and \\(\\mathbf{v} \\in \\mathbb{R}^n\\) in a fully connected neural network. Suppose the weight matrix between the two layers is \\(\\mathbf{W} \\in \\mathbb{R}^{n \\times d}\\). (We will ignore the activation function because it can easily be applied in parallel to each neuron.) During the forward pass, we have already computed the neuron values \\(\\mathbf{u}\\). With this information, we can compute the values in the second layer as \\[\n\\mathbf{v} = \\mathbf{W} \\mathbf{u}.\n\\] In particular, the \\(i\\)th value in the second layer is given by \\[\nv_i = \\sum_{j=1}^d [\\mathbf{W}]_{i,j} u_j.\n\\] Because the function is linear, observe that the derivatives of the second layer with respect to the first layer are simply given by \\[\n\\frac{\\partial v_i}{\\partial u_j} = [\\mathbf{W}]_{i,j}.\n\\]\nDuring the backward pass, we have already computed the gradients of the loss function with respect to the second layer \\(\\nabla_{\\mathbf{v}} \\mathcal{L}\\). With this information, we can compute the gradients of the loss function with respect to the first layer as \\[\n\\nabla_{\\mathbf{u}} \\mathcal{L} = \\mathbf{W}^\\top \\nabla_{\\mathbf{v}} \\mathcal{L}.\n\\] In particular, the gradient of the loss function with respect to the \\(j\\)th neuron in the first layer is given by \\[\n\\frac{\\partial \\mathcal{L}}{\\partial u_j}\n= \\sum_{i=1}^n \\frac{\\partial \\mathcal{L}}{\\partial v_i}\n\\frac{\\partial v_i}{\\partial u_j}\n= \\sum_{i=1}^n [\\mathbf{W}^\\top]_{j,i} \\frac{\\partial \\mathcal{L}}{\\partial v_i}.\n\\]\nNote: Instead of the gradients with respect to the neurons, we are actually interested in the gradients of the loss function with respect to the parameters, because we are updating the parameters during training. But, we need the neuron gradients to compute the parameter gradients: Once we have the neuron gradients, we compute the parameter gradients with another application of the chain rule.\nThe power of the linear algebraic view is that we can use hardware specialized for matrix multiplication to efficiently compute the gradients!\nNeural networks are a powerful tool for learning complex functions, but notoriously difficult to understand because of their many parameters and nonlinearities. The big surprise of deep learning is that, even though they are not convex functions, gradient descent works remarkably well to train them. We don’t have good intuition for what happens in high-dimensional space (the loss function depends on the number of parameters, which can be very large), but it seems like there’s almost always a parameter update that at least slightly improves the loss function so we rarely get stuck in a bad local minima."
  }
]